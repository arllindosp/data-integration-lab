{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62178a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f0353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e202c2d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810ff5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries required to clean, standardize, and prepare the dataset for futher analysis.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "start_time  = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e86f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏠 LOADING ITBI DATASETS - RECIFE\n",
      "========================================\n",
      "\n",
      "📅 Loading ITBI data 2023...\n",
      "   🔗 URL: http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resou...\n",
      "   ⏳ Downloading file...\n",
      "   ✅ Success: 12,669 records, 23 columns\n",
      "   📊 Data sample:\n",
      "      First neighborhoods: ['Encruzilhada', 'Encruzilhada', 'Encruzilhada']\n",
      "\n",
      "📅 Loading ITBI data 2024...\n",
      "   🔗 URL: http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resou...\n",
      "   ⏳ Downloading file...\n",
      "   ✅ Success: 15,242 records, 23 columns\n",
      "   📊 Data sample:\n",
      "      First neighborhoods: ['Encruzilhada', 'Encruzilhada', 'Encruzilhada']\n",
      "\n",
      "📅 Loading ITBI data 2025...\n",
      "   🔗 URL: http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resou...\n",
      "   ⏳ Downloading file...\n",
      "   ✅ Success: 7,206 records, 23 columns\n",
      "   📊 Data sample:\n",
      "      First neighborhoods: ['Encruzilhada', 'Encruzilhada', 'Encruzilhada']\n",
      "\n",
      "🔍 VERIFication\n",
      "--------------------\n",
      "   • Total datasets loaded: 3\n",
      "   • Years included: ['2023', '2024', '2025']\n",
      "   • Expected datasets: 3\n",
      "\n",
      "📊 FINAL DATASET SUMMARY\n",
      "==============================\n",
      "   • Total records: 35,117\n",
      "   • Total columns: 23\n",
      "   • Years included: ['2023', '2024', '2025']\n",
      "   • 2023: Dataset loaded successfully\n",
      "   • 2024: Dataset loaded successfully\n",
      "   • 2025: Dataset loaded successfully\n",
      "\n",
      "📋 Sample data (first 3 rows):\n",
      "         bairro  tipo_imovel valor_avaliacao data_transacao\n",
      "0  Encruzilhada  Apartamento       505000,00     2025-01-08\n",
      "1  Encruzilhada  Apartamento       398109,72     2025-05-12\n",
      "2  Encruzilhada  Apartamento       790000,00     2025-04-14\n",
      "3  Encruzilhada  Apartamento       780000,00     2025-01-08\n",
      "\n",
      "✅ Directory \"datasets\" is ready for use.\n",
      "✅ ETL Extract phase completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path where datasets will be stored\n",
    "data_directory = \"datasets\"\n",
    "\n",
    "# Create the directory if it doesn't exist, avoiding errors if it already exists\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "# SIMPLIFIED VERSION - Basic loop for loading ITBI datasets\n",
    "\n",
    "# Define dataset URLs\n",
    "dataset_sources = [\n",
    "    (\"2023\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/d0c08a6f-4c27-423c-9219-8d13403816f4/download/itbi_2023.csv\"),\n",
    "    (\"2024\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/a36d548b-d705-496a-ac47-4ec36f068474/download/itbi_2024.csv\"),\n",
    "    (\"2025\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/5b582147-3935-459a-bbf7-ee623c22c97b/download/itbi_2025.csv\")\n",
    "]\n",
    "\n",
    "print(\"🏠 LOADING ITBI DATASETS - RECIFE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simple loop to load each dataset\n",
    "load_success_count = 0\n",
    "all_records_total = 0\n",
    "all_columns_total = 0\n",
    "years_loaded = []\n",
    "data_storage = {}  # Dictionary to store the datasets\n",
    "\n",
    "for load_year, data_url in dataset_sources:\n",
    "    print(f\"\\n📅 Loading ITBI data {load_year}...\")\n",
    "    print(f\"   🔗 URL: {data_url[:80]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load the CSV\n",
    "        print(f\"   ⏳ Downloading file...\")\n",
    "        temp_dataframe = pd.read_csv(data_url, sep=';', encoding='utf-8')\n",
    "        \n",
    "        # Check if DataFrame is not empty\n",
    "        if temp_dataframe.empty:\n",
    "            raise ValueError(\"Dataset loaded is empty\")\n",
    "        \n",
    "        # Check if it has the expected columns\n",
    "        required_columns = ['bairro', 'tipo_imovel', 'valor_avaliacao', 'data_transacao']\n",
    "        missing_columns = [col for col in required_columns if col not in temp_dataframe.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"   ⚠️  Warning: Missing columns: {missing_columns}\")\n",
    "        \n",
    "        # Add year column\n",
    "        temp_dataframe['year'] = int(load_year)\n",
    "        \n",
    "        # Show basic information\n",
    "        current_records = len(temp_dataframe)\n",
    "        current_columns = len(temp_dataframe.columns)\n",
    "        \n",
    "        # Add to general totals\n",
    "        all_records_total += current_records\n",
    "        all_columns_total = current_columns  # Assume all have the same number of columns\n",
    "        years_loaded.append(load_year)\n",
    "        \n",
    "        # Save dataset in dictionary for later manipulation\n",
    "        data_storage[load_year] = temp_dataframe.copy()  # Create an independent copy\n",
    "        \n",
    "        print(f\"   ✅ Success: {current_records:,} records, {current_columns} columns\")\n",
    "        print(f\"   📊 Data sample:\")\n",
    "        \n",
    "        # Check if 'bairro' column exists before showing\n",
    "        if 'bairro' in temp_dataframe.columns:\n",
    "            sample_neighborhoods = temp_dataframe['bairro'].head(3).tolist()\n",
    "            print(f\"      First neighborhoods: {sample_neighborhoods}\")\n",
    "            del sample_neighborhoods\n",
    "        else:\n",
    "            first_column_sample = temp_dataframe.iloc[:3, 0].tolist()\n",
    "            print(f\"      First 3 rows of first column: {first_column_sample}\")\n",
    "            del first_column_sample\n",
    "        \n",
    "        load_success_count += 1\n",
    "        del current_records, current_columns\n",
    "        \n",
    "    except Exception as load_error:\n",
    "        print(f\"   ❌ Error loading data for {load_year}: {type(load_error).__name__}\")\n",
    "        print(f\"      Details: {str(load_error)}\")\n",
    "        del load_error\n",
    "\n",
    "# Clean up loop variables\n",
    "del load_year, data_url, temp_dataframe, required_columns, missing_columns\n",
    "\n",
    "print(f\"\\n🔍 VERIFication\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"   • Total datasets loaded: {load_success_count}\")\n",
    "print(f\"   • Years included: {years_loaded}\")\n",
    "print(f\"   • Expected datasets: 3\")\n",
    "print()\n",
    "\n",
    "print(f\"📊 FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"   • Total records: {all_records_total:,}\")\n",
    "print(f\"   • Total columns: {all_columns_total}\")\n",
    "print(f\"   • Years included: {years_loaded}\")\n",
    "\n",
    "print(f\"   • 2023: Dataset loaded successfully\")\n",
    "print(f\"   • 2024: Dataset loaded successfully\") \n",
    "print(f\"   • 2025: Dataset loaded successfully\")\n",
    "\n",
    "# Access specific datasets with intermediate variables\n",
    "dataset_2023 = data_storage['2023']\n",
    "dataset_2024 = data_storage['2024']\n",
    "dataset_2025 = data_storage['2025']\n",
    "\n",
    "print(f\"\\n📋 Sample data (first 3 rows):\")\n",
    "sample_data = dataset_2025[['bairro', 'tipo_imovel', 'valor_avaliacao', 'data_transacao']].head(4)\n",
    "print(sample_data)\n",
    "\n",
    "print(f'\\n✅ Directory \"{data_directory}\" is ready for use.')\n",
    "print(\"✅ ETL Extract phase completed successfully!\")\n",
    "\n",
    "# Clean up all intermediate variables\n",
    "del load_success_count, all_records_total, all_columns_total, years_loaded\n",
    "del dataset_2023, dataset_2024, dataset_2025, sample_data\n",
    "\n",
    "# Rename final variables for consistency\n",
    "dataset_directory = data_directory\n",
    "datasets_dict = data_storage\n",
    "del data_directory, data_storage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f036df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SAVING DATASETS TO FILES AND CREATING ZIP ARCHIVE\n",
      "=======================================================\n",
      "   ✅ Saved: itbi_2023.csv\n",
      "   ✅ Saved: itbi_2024.csv\n",
      "   ✅ Saved: itbi_2025.csv\n",
      "\n",
      "✅ ZIP ARCHIVE CREATED SUCCESSFULLY!\n",
      "   📦 Filename: itbi_datasets_recife.zip\n",
      "   📁 Size: 0.91 MB\n",
      "   🗃️  Files in ZIP: 3\n",
      "   📂 Location: datasets\\itbi_datasets_recife.zip\n"
     ]
    }
   ],
   "source": [
    "# Save dataframes as CSV files and create ZIP archive\n",
    "\n",
    "header_message = \"💾 SAVING DATASETS TO FILES AND CREATING ZIP ARCHIVE\"\n",
    "separator_line = \"=\" * 55\n",
    "\n",
    "print(header_message)\n",
    "print(separator_line)\n",
    "\n",
    "# Clean up header variables immediately\n",
    "del header_message, separator_line\n",
    "\n",
    "# Initialize control variables\n",
    "csv_files_list = []\n",
    "save_successful = True\n",
    "\n",
    "# Create CSV files with proper variable management\n",
    "for dataset_year, dataset_df in datasets_dict.items():\n",
    "    # Create filename using intermediate variables\n",
    "    csv_filename = f\"itbi_{dataset_year}.csv\"\n",
    "    csv_filepath = os.path.join(dataset_directory, csv_filename)\n",
    "    \n",
    "    try:\n",
    "        # Save to CSV\n",
    "        dataset_df.to_csv(csv_filepath, sep=';', encoding='utf-8', index=False)\n",
    "        csv_files_list.append(csv_filepath)\n",
    "    except Exception as save_error:\n",
    "        # Use intermediate variable for error message\n",
    "        error_msg = f\"   ❌ Failed to save: {csv_filename}\"\n",
    "        print(error_msg)\n",
    "        save_successful = False\n",
    "        del save_error, error_msg\n",
    "    \n",
    "    # Clean up loop variables immediately\n",
    "    del csv_filename, csv_filepath\n",
    "\n",
    "# Clean up loop variables completely\n",
    "del dataset_year, dataset_df\n",
    "\n",
    "# Print success messages outside the loop to avoid duplicates\n",
    "for file_path in csv_files_list:\n",
    "    # Use intermediate variable for filename\n",
    "    saved_filename = os.path.basename(file_path)\n",
    "    success_msg = f\"   ✅ Saved: {saved_filename}\"\n",
    "    print(success_msg)\n",
    "    del saved_filename, success_msg\n",
    "\n",
    "# CRITICAL: Clean up the loop variable\n",
    "del file_path\n",
    "\n",
    "# Create ZIP archive if CSV files were created successfully\n",
    "if csv_files_list and save_successful:\n",
    "    # Create intermediate variables for ZIP creation\n",
    "    zip_filename = \"itbi_datasets_recife.zip\"\n",
    "    zip_filepath = os.path.join(dataset_directory, zip_filename)\n",
    "    \n",
    "    try:\n",
    "        # Create ZIP with managed variables\n",
    "        with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            for source_file in csv_files_list:\n",
    "                target_filename = os.path.basename(source_file)\n",
    "                zip_file.write(source_file, target_filename)\n",
    "                del target_filename\n",
    "            del source_file\n",
    "        \n",
    "        # Verify and show results with managed variables\n",
    "        if os.path.exists(zip_filepath):\n",
    "            # Calculate file size using intermediate variables\n",
    "            file_size_bytes = os.path.getsize(zip_filepath)\n",
    "            file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "            \n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_reader:\n",
    "                zip_contents = zip_reader.namelist()\n",
    "                files_in_zip = len(zip_contents)\n",
    "            \n",
    "            # Create all success messages using intermediate variables\n",
    "            success_header = \"\\n✅ ZIP ARCHIVE CREATED SUCCESSFULLY!\"\n",
    "            filename_line = f\"   📦 Filename: {zip_filename}\"\n",
    "            size_line = f\"   📁 Size: {file_size_mb:.2f} MB\"\n",
    "            files_line = f\"   🗃️  Files in ZIP: {files_in_zip}\"\n",
    "            location_line = f\"   📂 Location: {zip_filepath}\"\n",
    "            \n",
    "            print(success_header)\n",
    "            print(filename_line)\n",
    "            print(size_line)\n",
    "            print(files_line)\n",
    "            print(location_line)\n",
    "            \n",
    "            # Clean up all verification variables immediately\n",
    "            del file_size_bytes, file_size_mb, zip_contents, files_in_zip\n",
    "            del success_header, filename_line, size_line, files_line, location_line\n",
    "        else:\n",
    "            # Use intermediate variable for error message\n",
    "            zip_not_created_msg = \"   ❌ Error: ZIP file was not created\"\n",
    "            print(zip_not_created_msg)\n",
    "            del zip_not_created_msg\n",
    "            \n",
    "    except Exception as zip_error:\n",
    "        # Use intermediate variables for error handling\n",
    "        error_details = str(zip_error)\n",
    "        zip_error_msg = f\"   ❌ Error creating ZIP: {error_details}\"\n",
    "        print(zip_error_msg)\n",
    "        del zip_error, error_details, zip_error_msg\n",
    "        \n",
    "    # Clean up ZIP variables immediately\n",
    "    del zip_filename, zip_filepath\n",
    "else:\n",
    "    # Use intermediate variable for failure message\n",
    "    no_zip_msg = \"\\n❌ Cannot create ZIP: No CSV files or save errors occurred\"\n",
    "    print(no_zip_msg)\n",
    "    del no_zip_msg\n",
    "\n",
    "# Final comprehensive cleanup\n",
    "del csv_files_list, save_successful\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27845df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['logradouro', 'numero', 'complemento', 'valor_avaliacao', 'bairro',\n",
       "       'cidade', 'uf', 'ano_construcao', 'area_terreno', 'area_construida',\n",
       "       'fracao_ideal', 'padrao_acabamento', 'tipo_construcao', 'tipo_ocupacao',\n",
       "       'data_transacao', 'estado_conservacao', 'tipo_imovel', 'sfh',\n",
       "       'cod_logradouro', 'latitude', 'longitude', 'ano', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's take a good look at the tables and their nomenclature structure.\n",
    "# After analyzing the datasets, we can confirm that all tables follow good naming standards:\n",
    "# snake_case convention, descriptive names, Portuguese language consistency, no special characters,\n",
    "# logical grouping, and standardized separators. These naming conventions ensure database \n",
    "# compatibility, readability, and maintainability across different systems and programming environments.\n",
    "# However, the 'sfh' acronym lacks clarity and context, making it difficult for users to understand\n",
    "# its meaning without domain knowledge. To improve data documentation and usability, we will rename\n",
    "# this column to 'valores_financiados_sfh' providing explicit context about financed values.\n",
    "datasets_dict['2023'].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0846a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming renaming sfh column in order to improve understanding \n",
    "for year, df in datasets_dict.items():\n",
    "    new_df = df.rename(columns = {'sfh':'valores_financiados_sfh'})\n",
    "    datasets_dict[year] = new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7c9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🩺 Data Health Check - Missing Values Diagnostic & Investigation\n",
      "=================================================================\n",
      "\n",
      "📅 Dataset 2023:\n",
      "--------------------\n",
      "  🔍 Found 3 columns with missing values:\n",
      "      • complemento: 1,320 nulls \n",
      "      • latitude: 3,402 nulls \n",
      "      • longitude: 3,402 nulls \n",
      "\n",
      "📅 Dataset 2024:\n",
      "--------------------\n",
      "  🔍 Found 3 columns with missing values:\n",
      "      • complemento: 1,443 nulls \n",
      "      • latitude: 5,619 nulls \n",
      "      • longitude: 5,619 nulls \n",
      "\n",
      "📅 Dataset 2025:\n",
      "--------------------\n",
      "  🔍 Found 3 columns with missing values:\n",
      "      • complemento: 576 nulls \n",
      "      • latitude: 2,623 nulls \n",
      "      • longitude: 2,623 nulls \n",
      "\n",
      "📋 Final diagnosis:\n",
      "There is a total of 3 datasets with missing values out of 3 total datasets.\n"
     ]
    }
   ],
   "source": [
    "# Null values analysis \n",
    "print(\"🩺 Data Health Check - Missing Values Diagnostic & Investigation\")\n",
    "print(\"=\" * 65)\n",
    "missing_datasets = 0\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"\\n📅 Dataset {year}:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    null_summary = df.isna().sum()\n",
    "    columns_with_nulls = null_summary[null_summary > 0]\n",
    "    \n",
    "    if len(columns_with_nulls.index.tolist()) > 0:\n",
    "        \n",
    "        missing_datasets += 1\n",
    "        print(f\"  🔍 Found {len(columns_with_nulls)} columns with missing values:\")\n",
    "        \n",
    "        for column_name, null_count in columns_with_nulls.items():\n",
    "            print(f\"      • {column_name}: {null_count:,} nulls \")\n",
    "            \n",
    "    else:\n",
    "        print(\"   ✅ No missing values found - Dataset is complete!\")\n",
    "\n",
    "\n",
    "print(\"\\n📋 Final diagnosis:\")\n",
    "print(f'There is a total of {missing_datasets} datasets with missing values out of {len(datasets_dict)} total datasets.')\n",
    "\n",
    "# NEXT STEP: DATA CLEANING AND NULL VALUES TREATMENT\n",
    "# Now that we've identified null values in some datasets, we need to perform cleaning\n",
    "# and removal of these missing values to prevent issues during subsequent analysis.\n",
    "# Null values can cause errors in statistical calculations, visualizations, and data modeling.\n",
    "# Proper treatment of these values is essential for ETL pipeline integrity and reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7dae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12669 entries, 0 to 12668\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   logradouro               12669 non-null  object \n",
      " 1   numero                   12669 non-null  int64  \n",
      " 2   complemento              11349 non-null  object \n",
      " 3   valor_avaliacao          12669 non-null  object \n",
      " 4   bairro                   12669 non-null  object \n",
      " 5   ano_construcao           12669 non-null  int64  \n",
      " 6   area_terreno             12669 non-null  object \n",
      " 7   area_construida          12669 non-null  object \n",
      " 8   fracao_ideal             12669 non-null  object \n",
      " 9   padrao_acabamento        12669 non-null  object \n",
      " 10  tipo_construcao          12669 non-null  object \n",
      " 11  tipo_ocupacao            12669 non-null  object \n",
      " 12  data_transacao           12669 non-null  object \n",
      " 13  estado_conservacao       12669 non-null  object \n",
      " 14  tipo_imovel              12669 non-null  object \n",
      " 15  valores_financiados_sfh  12669 non-null  object \n",
      " 16  cod_logradouro           12669 non-null  int64  \n",
      " 17  latitude                 9267 non-null   float64\n",
      " 18  longitude                9267 non-null   float64\n",
      " 19  ano                      12669 non-null  int64  \n",
      " 20  year                     12669 non-null  int64  \n",
      "dtypes: float64(2), int64(5), object(14)\n",
      "memory usage: 2.0+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15242 entries, 0 to 15241\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   logradouro               15242 non-null  object \n",
      " 1   numero                   15242 non-null  int64  \n",
      " 2   complemento              13799 non-null  object \n",
      " 3   valor_avaliacao          15242 non-null  object \n",
      " 4   bairro                   15242 non-null  object \n",
      " 5   ano_construcao           15242 non-null  int64  \n",
      " 6   area_terreno             15242 non-null  object \n",
      " 7   area_construida          15242 non-null  object \n",
      " 8   fracao_ideal             15242 non-null  object \n",
      " 9   padrao_acabamento        15242 non-null  object \n",
      " 10  tipo_construcao          15242 non-null  object \n",
      " 11  tipo_ocupacao            15242 non-null  object \n",
      " 12  data_transacao           15242 non-null  object \n",
      " 13  estado_conservacao       15242 non-null  object \n",
      " 14  tipo_imovel              15242 non-null  object \n",
      " 15  valores_financiados_sfh  15242 non-null  object \n",
      " 16  cod_logradouro           15242 non-null  int64  \n",
      " 17  latitude                 9623 non-null   float64\n",
      " 18  longitude                9623 non-null   float64\n",
      " 19  ano                      15242 non-null  int64  \n",
      " 20  year                     15242 non-null  int64  \n",
      "dtypes: float64(2), int64(5), object(14)\n",
      "memory usage: 2.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7206 entries, 0 to 7205\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   logradouro               7206 non-null   object \n",
      " 1   numero                   7206 non-null   int64  \n",
      " 2   complemento              6630 non-null   object \n",
      " 3   valor_avaliacao          7206 non-null   object \n",
      " 4   bairro                   7206 non-null   object \n",
      " 5   ano_construcao           7206 non-null   int64  \n",
      " 6   area_terreno             7206 non-null   object \n",
      " 7   area_construida          7206 non-null   object \n",
      " 8   fracao_ideal             7206 non-null   object \n",
      " 9   padrao_acabamento        7206 non-null   object \n",
      " 10  tipo_construcao          7206 non-null   object \n",
      " 11  tipo_ocupacao            7206 non-null   object \n",
      " 12  data_transacao           7206 non-null   object \n",
      " 13  estado_conservacao       7206 non-null   object \n",
      " 14  tipo_imovel              7206 non-null   object \n",
      " 15  valores_financiados_sfh  7206 non-null   object \n",
      " 16  cod_logradouro           7206 non-null   int64  \n",
      " 17  latitude                 4583 non-null   float64\n",
      " 18  longitude                4583 non-null   float64\n",
      " 19  ano                      7206 non-null   int64  \n",
      " 20  year                     7206 non-null   int64  \n",
      "dtypes: float64(2), int64(5), object(14)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# COLUMN OPTIMIZATION: REMOVING REDUNDANT GEOGRAPHIC COLUMNS\n",
    "# We will drop the 'cidade' and 'uf' columns as they contain only uniform values across all records\n",
    "# (Recife and PE respectively). Since our analysis focuses specifically on ITBI data from Recife's\n",
    "# urban region within Pernambuco state, these columns provide no analytical value or variation.\n",
    "# Removing these redundant columns optimizes memory usage and simplifies the dataset structure\n",
    "# without losing any meaningful information for our geographic scope of analysis.\n",
    "\n",
    "for year, df in datasets_dict.items():\n",
    "    df = df.drop([\"cidade\", \"uf\"], axis =1)\n",
    "    df.info()\n",
    "    datasets_dict[year] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f9f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    av norte miguel arraes de alencar\n",
      "1    av norte miguel arraes de alencar\n",
      "2                   rua belmiro corrêa\n",
      "3                   rua belmiro corrêa\n",
      "4                   rua belmiro corrêa\n",
      "5                   rua belmiro corrêa\n",
      "6                   rua belmiro corrêa\n",
      "7                   rua belmiro corrêa\n",
      "8                   rua belmiro corrêa\n",
      "9                   rua belmiro corrêa\n",
      "Name: logradouro, dtype: object\n",
      "0    3071\n",
      "1    3029\n",
      "2     133\n",
      "3     133\n",
      "4     133\n",
      "5     133\n",
      "6     133\n",
      "7     133\n",
      "8     109\n",
      "9     109\n",
      "Name: numero, dtype: int64\n",
      "0          NaN\n",
      "1          NaN\n",
      "2    apto 0001\n",
      "3    apto 0001\n",
      "4    apto 0002\n",
      "5    apto 0003\n",
      "6    apto 0004\n",
      "7    apto 0005\n",
      "8          NaN\n",
      "9          NaN\n",
      "Name: complemento, dtype: object\n",
      "0    1068562,63\n",
      "1    1500000,00\n",
      "2     110000,00\n",
      "3     110000,00\n",
      "4     110000,00\n",
      "5     110000,00\n",
      "6     110000,00\n",
      "7     110000,00\n",
      "8    4900000,00\n",
      "9    4900000,00\n",
      "Name: valor_avaliacao, dtype: object\n",
      "0    Encruzilhada\n",
      "1    Encruzilhada\n",
      "2    Encruzilhada\n",
      "3    Encruzilhada\n",
      "4    Encruzilhada\n",
      "5    Encruzilhada\n",
      "6    Encruzilhada\n",
      "7    Encruzilhada\n",
      "8    Encruzilhada\n",
      "9    Encruzilhada\n",
      "Name: bairro, dtype: object\n",
      "0    1997\n",
      "1    1957\n",
      "2    1970\n",
      "3    1970\n",
      "4    1970\n",
      "5    1970\n",
      "6    1970\n",
      "7    1970\n",
      "8    1951\n",
      "9    1951\n",
      "Name: ano_construcao, dtype: int64\n",
      "0     438,0\n",
      "1    779,33\n",
      "2    562,05\n",
      "3    562,05\n",
      "4    562,05\n",
      "5    562,05\n",
      "6    562,05\n",
      "7    562,05\n",
      "8    439,28\n",
      "9    439,28\n",
      "Name: area_terreno, dtype: object\n",
      "0     511,0\n",
      "1    582,44\n",
      "2     121,0\n",
      "3     121,0\n",
      "4      81,0\n",
      "5      81,0\n",
      "6      81,0\n",
      "7      81,0\n",
      "8    343,23\n",
      "9    343,23\n",
      "Name: area_construida, dtype: object\n",
      "0        1,0\n",
      "1        1,0\n",
      "2    0,27191\n",
      "3    0,27191\n",
      "4    0,18202\n",
      "5    0,18202\n",
      "6    0,18202\n",
      "7    0,18202\n",
      "8        1,0\n",
      "9        1,0\n",
      "Name: fracao_ideal, dtype: object\n",
      "0      Médio\n",
      "1      Médio\n",
      "2    Simples\n",
      "3    Simples\n",
      "4    Simples\n",
      "5    Simples\n",
      "6    Simples\n",
      "7    Simples\n",
      "8      Médio\n",
      "9      Médio\n",
      "Name: padrao_acabamento, dtype: object\n",
      "0                         Galpão\n",
      "1                           Casa\n",
      "2    Apartamento <= 4 Pavimentos\n",
      "3    Apartamento <= 4 Pavimentos\n",
      "4    Apartamento <= 4 Pavimentos\n",
      "5    Apartamento <= 4 Pavimentos\n",
      "6    Apartamento <= 4 Pavimentos\n",
      "7    Apartamento <= 4 Pavimentos\n",
      "8                           Casa\n",
      "9                           Casa\n",
      "Name: tipo_construcao, dtype: object\n",
      "0    COMERCIAL COM LIXO ORGANICO\n",
      "1    COMERCIAL SEM LIXO ORGANICO\n",
      "2                    RESIDENCIAL\n",
      "3                    RESIDENCIAL\n",
      "4                    RESIDENCIAL\n",
      "5                    RESIDENCIAL\n",
      "6                    RESIDENCIAL\n",
      "7                    RESIDENCIAL\n",
      "8                    RESIDENCIAL\n",
      "9                    RESIDENCIAL\n",
      "Name: tipo_ocupacao, dtype: object\n",
      "0    2023-12-21\n",
      "1    2023-11-17\n",
      "2    2023-09-26\n",
      "3    2023-09-22\n",
      "4    2023-09-22\n",
      "5    2023-09-22\n",
      "6    2023-09-26\n",
      "7    2023-09-22\n",
      "8    2023-09-26\n",
      "9    2023-09-26\n",
      "Name: data_transacao, dtype: object\n",
      "0    Regular\n",
      "1    Regular\n",
      "2        Bom\n",
      "3        Bom\n",
      "4        Bom\n",
      "5        Bom\n",
      "6        Bom\n",
      "7        Bom\n",
      "8        Bom\n",
      "9        Bom\n",
      "Name: estado_conservacao, dtype: object\n",
      "0         Galpão\n",
      "1           Casa\n",
      "2    Apartamento\n",
      "3    Apartamento\n",
      "4    Apartamento\n",
      "5    Apartamento\n",
      "6    Apartamento\n",
      "7    Apartamento\n",
      "8           Casa\n",
      "9           Casa\n",
      "Name: tipo_imovel, dtype: object\n",
      "0    0,00\n",
      "1    0,00\n",
      "2    0,00\n",
      "3    0,00\n",
      "4    0,00\n",
      "5    0,00\n",
      "6    0,00\n",
      "7    0,00\n",
      "8    0,00\n",
      "9    0,00\n",
      "Name: valores_financiados_sfh, dtype: object\n",
      "0    46540\n",
      "1    46540\n",
      "2    10715\n",
      "3    10715\n",
      "4    10715\n",
      "5    10715\n",
      "6    10715\n",
      "7    10715\n",
      "8    10715\n",
      "9    10715\n",
      "Name: cod_logradouro, dtype: int64\n",
      "0   -8.034273\n",
      "1   -8.034435\n",
      "2   -8.035013\n",
      "3   -8.035013\n",
      "4   -8.035013\n",
      "5   -8.035013\n",
      "6   -8.035013\n",
      "7   -8.035013\n",
      "8   -8.035165\n",
      "9   -8.035165\n",
      "Name: latitude, dtype: float64\n",
      "0   -34.896337\n",
      "1   -34.896335\n",
      "2   -34.895903\n",
      "3   -34.895903\n",
      "4   -34.895903\n",
      "5   -34.895903\n",
      "6   -34.895903\n",
      "7   -34.895903\n",
      "8   -34.895961\n",
      "9   -34.895961\n",
      "Name: longitude, dtype: float64\n",
      "0    2023\n",
      "1    2023\n",
      "2    2023\n",
      "3    2023\n",
      "4    2023\n",
      "5    2023\n",
      "6    2023\n",
      "7    2023\n",
      "8    2023\n",
      "9    2023\n",
      "Name: ano, dtype: int64\n",
      "0    2023\n",
      "1    2023\n",
      "2    2023\n",
      "3    2023\n",
      "4    2023\n",
      "5    2023\n",
      "6    2023\n",
      "7    2023\n",
      "8    2023\n",
      "9    2023\n",
      "Name: year, dtype: int64\n",
      "0    av norte miguel arraes de alencar\n",
      "1    av norte miguel arraes de alencar\n",
      "2    av norte miguel arraes de alencar\n",
      "3                     rua caio pereira\n",
      "4                     rua caio pereira\n",
      "5                     rua caio pereira\n",
      "6                     rua caio pereira\n",
      "7                     rua caio pereira\n",
      "8                     rua caio pereira\n",
      "9                     rua caio pereira\n",
      "Name: logradouro, dtype: object\n",
      "0    3071\n",
      "1    3029\n",
      "2    3029\n",
      "3     375\n",
      "4     375\n",
      "5     375\n",
      "6     375\n",
      "7     800\n",
      "8     800\n",
      "9     800\n",
      "Name: numero, dtype: int64\n",
      "0                               NaN\n",
      "1                               NaN\n",
      "2                               NaN\n",
      "3    apto 503 edf luar do rosarinho\n",
      "4    apto 602 edf luar do rosarinho\n",
      "5    apto 801 edf luar do rosarinho\n",
      "6    apto 203 edf luar do rosarinho\n",
      "7      apto 1901 edf sainte juliana\n",
      "8       apto 902 edf sainte juliana\n",
      "9       apto 203 edf sainte juliana\n",
      "Name: complemento, dtype: object\n",
      "0    1068562,63\n",
      "1    1951000,00\n",
      "2    1500000,00\n",
      "3     402544,22\n",
      "4     405198,63\n",
      "5     409994,56\n",
      "6     395501,75\n",
      "7     700000,00\n",
      "8     790000,00\n",
      "9     680000,00\n",
      "Name: valor_avaliacao, dtype: object\n",
      "0    Encruzilhada\n",
      "1    Encruzilhada\n",
      "2    Encruzilhada\n",
      "3    Encruzilhada\n",
      "4    Encruzilhada\n",
      "5    Encruzilhada\n",
      "6    Encruzilhada\n",
      "7    Encruzilhada\n",
      "8    Encruzilhada\n",
      "9    Encruzilhada\n",
      "Name: bairro, dtype: object\n",
      "0    1997\n",
      "1    1957\n",
      "2    1957\n",
      "3    2007\n",
      "4    2007\n",
      "5    2007\n",
      "6    2007\n",
      "7    2017\n",
      "8    2017\n",
      "9    2017\n",
      "Name: ano_construcao, dtype: int64\n",
      "0      438,0\n",
      "1     779,33\n",
      "2     779,33\n",
      "3     798,91\n",
      "4     798,91\n",
      "5     798,91\n",
      "6     798,91\n",
      "7    1295,39\n",
      "8    1295,39\n",
      "9    1295,39\n",
      "Name: area_terreno, dtype: object\n",
      "0     511,0\n",
      "1    582,44\n",
      "2    582,44\n",
      "3    118,55\n",
      "4    118,64\n",
      "5    118,64\n",
      "6    118,55\n",
      "7    145,68\n",
      "8     145,8\n",
      "9    145,49\n",
      "Name: area_construida, dtype: object\n",
      "0        1,0\n",
      "1        1,0\n",
      "2        1,0\n",
      "3    0,02516\n",
      "4    0,02518\n",
      "5    0,02518\n",
      "6    0,02516\n",
      "7    0,01586\n",
      "8    0,01589\n",
      "9    0,01581\n",
      "Name: fracao_ideal, dtype: object\n",
      "0       Médio\n",
      "1       Médio\n",
      "2       Médio\n",
      "3       Médio\n",
      "4       Médio\n",
      "5       Médio\n",
      "6       Médio\n",
      "7    Superior\n",
      "8    Superior\n",
      "9    Superior\n",
      "Name: padrao_acabamento, dtype: object\n",
      "0                        Galpão\n",
      "1                          Casa\n",
      "2                          Casa\n",
      "3    Apartamento > 4 Pavimentos\n",
      "4    Apartamento > 4 Pavimentos\n",
      "5    Apartamento > 4 Pavimentos\n",
      "6    Apartamento > 4 Pavimentos\n",
      "7    Apartamento > 4 Pavimentos\n",
      "8    Apartamento > 4 Pavimentos\n",
      "9    Apartamento > 4 Pavimentos\n",
      "Name: tipo_construcao, dtype: object\n",
      "0    COMERCIAL COM LIXO ORGANICO\n",
      "1    COMERCIAL SEM LIXO ORGANICO\n",
      "2    COMERCIAL SEM LIXO ORGANICO\n",
      "3                    RESIDENCIAL\n",
      "4                    RESIDENCIAL\n",
      "5                    RESIDENCIAL\n",
      "6                    RESIDENCIAL\n",
      "7                    RESIDENCIAL\n",
      "8                    RESIDENCIAL\n",
      "9                    RESIDENCIAL\n",
      "Name: tipo_ocupacao, dtype: object\n",
      "0    2024-01-23\n",
      "1    2024-01-25\n",
      "2    2024-01-05\n",
      "3    2024-10-22\n",
      "4    2024-05-15\n",
      "5    2024-08-05\n",
      "6    2024-05-22\n",
      "7    2024-04-15\n",
      "8    2024-07-26\n",
      "9    2024-02-01\n",
      "Name: data_transacao, dtype: object\n",
      "0    Regular\n",
      "1    Regular\n",
      "2    Regular\n",
      "3        Bom\n",
      "4        Bom\n",
      "5        Bom\n",
      "6        Bom\n",
      "7        Bom\n",
      "8        Bom\n",
      "9        Bom\n",
      "Name: estado_conservacao, dtype: object\n",
      "0         Galpão\n",
      "1           Casa\n",
      "2           Casa\n",
      "3    Apartamento\n",
      "4    Apartamento\n",
      "5    Apartamento\n",
      "6    Apartamento\n",
      "7    Apartamento\n",
      "8    Apartamento\n",
      "9    Apartamento\n",
      "Name: tipo_imovel, dtype: object\n",
      "0         0,00\n",
      "1         0,00\n",
      "2         0,00\n",
      "3    200000,00\n",
      "4    288000,00\n",
      "5    235000,00\n",
      "6         0,00\n",
      "7         0,00\n",
      "8    665000,00\n",
      "9         0,00\n",
      "Name: valores_financiados_sfh, dtype: object\n",
      "0    46540\n",
      "1    46540\n",
      "2    46540\n",
      "3    13269\n",
      "4    13269\n",
      "5    13269\n",
      "6    13269\n",
      "7    13269\n",
      "8    13269\n",
      "9    13269\n",
      "Name: cod_logradouro, dtype: int64\n",
      "0   -8.034273\n",
      "1   -8.034435\n",
      "2   -8.034435\n",
      "3   -8.034996\n",
      "4   -8.034996\n",
      "5   -8.034996\n",
      "6   -8.034996\n",
      "7         NaN\n",
      "8         NaN\n",
      "9         NaN\n",
      "Name: latitude, dtype: float64\n",
      "0   -34.896337\n",
      "1   -34.896335\n",
      "2   -34.896335\n",
      "3   -34.896187\n",
      "4   -34.896187\n",
      "5   -34.896187\n",
      "6   -34.896187\n",
      "7          NaN\n",
      "8          NaN\n",
      "9          NaN\n",
      "Name: longitude, dtype: float64\n",
      "0    2024\n",
      "1    2024\n",
      "2    2024\n",
      "3    2024\n",
      "4    2024\n",
      "5    2024\n",
      "6    2024\n",
      "7    2024\n",
      "8    2024\n",
      "9    2024\n",
      "Name: ano, dtype: int64\n",
      "0    2024\n",
      "1    2024\n",
      "2    2024\n",
      "3    2024\n",
      "4    2024\n",
      "5    2024\n",
      "6    2024\n",
      "7    2024\n",
      "8    2024\n",
      "9    2024\n",
      "Name: year, dtype: int64\n",
      "0          rua caio pereira\n",
      "1          rua caio pereira\n",
      "2          rua caio pereira\n",
      "3          rua caio pereira\n",
      "4          rua caio pereira\n",
      "5          rua caio pereira\n",
      "6     rua doutor jose maria\n",
      "7     rua doutor jose maria\n",
      "8        rua andre reboucas\n",
      "9    rua engenheiro sampaio\n",
      "Name: logradouro, dtype: object\n",
      "0    375\n",
      "1    375\n",
      "2    800\n",
      "3    800\n",
      "4    800\n",
      "5    334\n",
      "6    578\n",
      "7    658\n",
      "8    106\n",
      "9     68\n",
      "Name: numero, dtype: int64\n",
      "0     apto 803 edf luar do rosarinho\n",
      "1     apto 302 edf luar do rosarinho\n",
      "2       apto 1201 edf sainte juliana\n",
      "3       apto 1501 edf sainte juliana\n",
      "4       apto 1602 edf sainte juliana\n",
      "5     apto 202 edf essenza rosarinho\n",
      "6       apto 0102 edf praia de ceres\n",
      "7          apto 1701 edf casa rosada\n",
      "8    apto 302 edf bellagio residence\n",
      "9        apto 404 splendid rosarinho\n",
      "Name: complemento, dtype: object\n",
      "0     505000,00\n",
      "1     398109,72\n",
      "2     790000,00\n",
      "3     780000,00\n",
      "4     840000,00\n",
      "5    1000000,00\n",
      "6     595739,42\n",
      "7     600000,00\n",
      "8     230000,00\n",
      "9     300000,00\n",
      "Name: valor_avaliacao, dtype: object\n",
      "0    Encruzilhada\n",
      "1    Encruzilhada\n",
      "2    Encruzilhada\n",
      "3    Encruzilhada\n",
      "4    Encruzilhada\n",
      "5    Encruzilhada\n",
      "6    Encruzilhada\n",
      "7    Encruzilhada\n",
      "8       Rosarinho\n",
      "9       Rosarinho\n",
      "Name: bairro, dtype: object\n",
      "0    2007\n",
      "1    2007\n",
      "2    2017\n",
      "3    2017\n",
      "4    2017\n",
      "5    2011\n",
      "6    2002\n",
      "7    2010\n",
      "8    2015\n",
      "9    2014\n",
      "Name: ano_construcao, dtype: int64\n",
      "0     798,91\n",
      "1     798,91\n",
      "2    1295,39\n",
      "3    1295,39\n",
      "4    1295,39\n",
      "5    1737,63\n",
      "6      861,0\n",
      "7     3090,5\n",
      "8     610,36\n",
      "9    2202,75\n",
      "Name: area_terreno, dtype: object\n",
      "0    132,01\n",
      "1    118,64\n",
      "2    145,68\n",
      "3    145,68\n",
      "4     145,8\n",
      "5     183,6\n",
      "6    184,77\n",
      "7    244,73\n",
      "8     59,74\n",
      "9     59,93\n",
      "Name: area_construida, dtype: object\n",
      "0    0,02698\n",
      "1    0,02518\n",
      "2    0,01586\n",
      "3    0,01586\n",
      "4    0,01589\n",
      "5    0,01923\n",
      "6    0,03333\n",
      "7    0,01401\n",
      "8    0,02165\n",
      "9    0,00727\n",
      "Name: fracao_ideal, dtype: object\n",
      "0       Médio\n",
      "1       Médio\n",
      "2    Superior\n",
      "3    Superior\n",
      "4    Superior\n",
      "5    Superior\n",
      "6    Superior\n",
      "7    Superior\n",
      "8       Médio\n",
      "9    Superior\n",
      "Name: padrao_acabamento, dtype: object\n",
      "0    Apartamento > 4 Pavimentos\n",
      "1    Apartamento > 4 Pavimentos\n",
      "2    Apartamento > 4 Pavimentos\n",
      "3    Apartamento > 4 Pavimentos\n",
      "4    Apartamento > 4 Pavimentos\n",
      "5    Apartamento > 4 Pavimentos\n",
      "6    Apartamento > 4 Pavimentos\n",
      "7    Apartamento > 4 Pavimentos\n",
      "8    Apartamento > 4 Pavimentos\n",
      "9    Apartamento > 4 Pavimentos\n",
      "Name: tipo_construcao, dtype: object\n",
      "0    RESIDENCIAL\n",
      "1    RESIDENCIAL\n",
      "2    RESIDENCIAL\n",
      "3    RESIDENCIAL\n",
      "4    RESIDENCIAL\n",
      "5    RESIDENCIAL\n",
      "6    RESIDENCIAL\n",
      "7    RESIDENCIAL\n",
      "8    RESIDENCIAL\n",
      "9    RESIDENCIAL\n",
      "Name: tipo_ocupacao, dtype: object\n",
      "0    2025-01-08\n",
      "1    2025-05-12\n",
      "2    2025-04-14\n",
      "3    2025-01-08\n",
      "4    2025-01-14\n",
      "5    2025-01-14\n",
      "6    2025-01-08\n",
      "7    2025-04-09\n",
      "8    2025-04-16\n",
      "9    2025-05-23\n",
      "Name: data_transacao, dtype: object\n",
      "0    Bom\n",
      "1    Bom\n",
      "2    Bom\n",
      "3    Bom\n",
      "4    Bom\n",
      "5    Bom\n",
      "6    Bom\n",
      "7    Bom\n",
      "8    Bom\n",
      "9    Bom\n",
      "Name: estado_conservacao, dtype: object\n",
      "0    Apartamento\n",
      "1    Apartamento\n",
      "2    Apartamento\n",
      "3    Apartamento\n",
      "4    Apartamento\n",
      "5    Apartamento\n",
      "6    Apartamento\n",
      "7    Apartamento\n",
      "8    Apartamento\n",
      "9    Apartamento\n",
      "Name: tipo_imovel, dtype: object\n",
      "0         0,00\n",
      "1         0,00\n",
      "2         0,00\n",
      "3         0,00\n",
      "4    565600,32\n",
      "5         0,00\n",
      "6         0,00\n",
      "7         0,00\n",
      "8         0,00\n",
      "9         0,00\n",
      "Name: valores_financiados_sfh, dtype: object\n",
      "0    13269\n",
      "1    13269\n",
      "2    13269\n",
      "3    13269\n",
      "4    13269\n",
      "5    13269\n",
      "6    36196\n",
      "7    36196\n",
      "8     4928\n",
      "9    53627\n",
      "Name: cod_logradouro, dtype: int64\n",
      "0   -8.034996\n",
      "1   -8.034996\n",
      "2         NaN\n",
      "3         NaN\n",
      "4         NaN\n",
      "5   -8.035095\n",
      "6   -8.035868\n",
      "7   -8.035419\n",
      "8   -8.032817\n",
      "9   -8.033695\n",
      "Name: latitude, dtype: float64\n",
      "0   -34.896187\n",
      "1   -34.896187\n",
      "2          NaN\n",
      "3          NaN\n",
      "4          NaN\n",
      "5   -34.896937\n",
      "6   -34.896250\n",
      "7   -34.897024\n",
      "8   -34.897594\n",
      "9   -34.897214\n",
      "Name: longitude, dtype: float64\n",
      "0    2025\n",
      "1    2025\n",
      "2    2025\n",
      "3    2025\n",
      "4    2025\n",
      "5    2025\n",
      "6    2025\n",
      "7    2025\n",
      "8    2025\n",
      "9    2025\n",
      "Name: ano, dtype: int64\n",
      "0    2025\n",
      "1    2025\n",
      "2    2025\n",
      "3    2025\n",
      "4    2025\n",
      "5    2025\n",
      "6    2025\n",
      "7    2025\n",
      "8    2025\n",
      "9    2025\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for year, df in datasets_dict.items():\n",
    "\n",
    "    for i in range(len(df.columns)):\n",
    "        col_name = df.columns[i]\n",
    "        print(df[col_name].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35edf729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TYPE CONVERSION: VALOR_AVALIACAO TO FLOAT\n",
    "# We will convert the 'valor_avaliacao' column from object type to float to enable proper\n",
    "# numerical operations and statistical analysis. Currently stored as object (string), this\n",
    "# prevents mathematical calculations, aggregations, and numeric comparisons essential for\n",
    "# financial analysis of property values. Converting to float ensures data integrity and\n",
    "# enables accurate computation of means, sums, and other statistical measures for ITBI values.\n",
    "\n",
    "\n",
    "\n",
    "# DECIMAL SEPARATOR STANDARDIZATION FUNCTION\n",
    "# Converts Brazilian decimal format (comma) to international format (dot) required for float conversion\n",
    "def standardize_decimal_format(x):\n",
    "    new = str(x.replace(',','.'))\n",
    "    return new\n",
    "    \n",
    "# STEP 1: Replace commas with dots to prepare for float conversion\n",
    "for year, df in datasets_dict.items():\n",
    "    df['valor_avaliacao'] = df['valor_avaliacao'].apply(standardize_decimal_format)\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "# STEP 2: Convert standardized strings to float type for numerical operations\n",
    "for year, df in datasets_dict.items():\n",
    "    df['valor_avaliacao'] = df['valor_avaliacao'].astype('float')\n",
    "    datasets_dict[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a86796e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AREA_TERRENO CONVERSION: APPLYING SAME DECIMAL STANDARDIZATION PROCESS\n",
    "# The 'area_terreno' column requires identical treatment as 'valor_avaliacao' - converting\n",
    "# Brazilian decimal format (comma) to international format (dot) before float conversion.\n",
    "# This ensures consistent numerical data types across all measurement columns for analysis.\n",
    "\n",
    "for year, df in datasets_dict.items():\n",
    "    df['area_terreno'] = df['area_terreno'].astype(str).str.replace(',', '.').astype(float)\n",
    "    df['area_terreno'] = df['area_terreno'].astype('float')\n",
    "    datasets_dict[year] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39196ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AREA_CONSTRUIDA CONVERSION: SAME DECIMAL STANDARDIZATION PROCESS\n",
    "# Converting 'area_construida' from Brazilian decimal format (comma) to international format (dot)\n",
    "for year, df in datasets_dict.items():\n",
    "    df['area_construida'] = df['area_construida'].astype(str).str.replace(',', '.').astype(float)\n",
    "    df['area_construida'] = df['area_construida'].astype('float')\n",
    "    datasets_dict[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7eea3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING CORRECTION: FIXING INCORRECTLY ENCODED CHARACTERS\n",
    "# Brazilian datasets often contain encoding issues where Portuguese characters (ã, ç, ê, õ, etc.) \n",
    "# are incorrectly displayed due to mismatched character encoding during data extraction.\n",
    "# This commonly occurs when CSV files are saved with Latin1 (ISO-8859-1) encoding but read as UTF-8,\n",
    "# causing characters like \"ção\" to appear as \"Ã§Ã£o\" or similar garbled text.\n",
    "# We fix this by re-encoding the text: first encode as Latin1 then decode as UTF-8 to restore\n",
    "# the original Portuguese characters for proper data analysis and visualization.\n",
    "\n",
    "def fix_encoding_issues(text_value):\n",
    "    if not isinstance(text_value, str):\n",
    "        return text_value\n",
    "    try:\n",
    "        return text_value.encode('latin1').decode('utf-8')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        return text_value\n",
    "\n",
    "# Apply encoding correction to all text columns in all datasets\n",
    "for year, df in datasets_dict.items():\n",
    "    text_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].apply(fix_encoding_issues)\n",
    "    datasets_dict[year] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79131f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 VERIFYING ENCODING CORRECTION\n",
      "===================================\n",
      "📅 Testing dataset 2023:\n",
      "   Padrao acabamento: ['Médio' 'Simples' 'Superior']\n",
      "   Estado conservacao: ['Regular' 'Bom' 'Mau']\n",
      "   Tipo ocupacao: ['COMERCIAL COM LIXO ORGANICO' 'COMERCIAL SEM LIXO ORGANICO' 'RESIDENCIAL']\n",
      "\n",
      "🔍 Checking for remaining encoding issues...\n",
      "⚠️  Dataset 2023: 4 columns still have encoding issues\n",
      "⚠️  Dataset 2024: 5 columns still have encoding issues\n",
      "⚠️  Dataset 2025: 5 columns still have encoding issues\n",
      "\n",
      "✅ Encoding correction verification completed!\n"
     ]
    }
   ],
   "source": [
    "# VERIFY ENCODING CORRECTION RESULTS\n",
    "print(\"🔤 VERIFYING ENCODING CORRECTION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test some text columns to see if encoding correction worked\n",
    "sample_year = '2023'\n",
    "sample_df = datasets_dict[sample_year]\n",
    "\n",
    "print(f\"📅 Testing dataset {sample_year}:\")\n",
    "print(f\"   Padrao acabamento: {sample_df['padrao_acabamento'].unique()[:5]}\")\n",
    "print(f\"   Estado conservacao: {sample_df['estado_conservacao'].unique()[:3]}\")\n",
    "print(f\"   Tipo ocupacao: {sample_df['tipo_ocupacao'].unique()[:3]}\")\n",
    "\n",
    "print(\"\\n🔍 Checking for remaining encoding issues...\")\n",
    "\n",
    "# Check if there are still encoding problems\n",
    "for year, df in datasets_dict.items():\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    problematic_chars = 0\n",
    "    \n",
    "    for col in text_cols:\n",
    "        if df[col].astype(str).str.contains('Ã|â|Ç|ç').any():\n",
    "            problematic_chars += 1\n",
    "    \n",
    "    if problematic_chars > 0:\n",
    "        print(f\"⚠️  Dataset {year}: {problematic_chars} columns still have encoding issues\")\n",
    "    else:\n",
    "        print(f\"✅ Dataset {year}: Encoding OK!\")\n",
    "\n",
    "print(\"\\n✅ Encoding correction verification completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77029b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 CONVERTING FRACAO_IDEAL TO FLOAT\n",
      "===================================\n",
      "📋 Current format of fracao_ideal column:\n",
      "   2023: object - Sample: ['1,0', '1,0', '0,27191']\n",
      "   2024: object - Sample: ['1,0', '1,0', '1,0']\n",
      "   2025: object - Sample: ['0,02698', '0,02518', '0,01586']\n",
      "\n",
      "🔄 Applying conversion...\n",
      "✅ Conversion completed!\n",
      "\n",
      "📊 Verifying results:\n",
      "   2023: float64 - Min: 0.0000, Max: 1.1801\n",
      "       ✅ No null values\n",
      "   2024: float64 - Min: 0.0000, Max: 1.0000\n",
      "       ✅ No null values\n",
      "   2025: float64 - Min: 0.0000, Max: 1.0000\n",
      "       ✅ No null values\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING FRACAO_IDEAL COLUMN TO FLOAT\n",
    "print(\"🔢 CONVERTING FRACAO_IDEAL TO FLOAT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check current format of the column before conversion\n",
    "print(\"📋 Current format of fracao_ideal column:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"   {year}: {df['fracao_ideal'].dtype} - Sample: {df['fracao_ideal'].head(3).tolist()}\")\n",
    "\n",
    "print(\"\\n🔄 Applying conversion...\")\n",
    "\n",
    "# Convert fracao_ideal to float (replacing comma with dot)\n",
    "for year, df in datasets_dict.items():\n",
    "    df['fracao_ideal'] = df['fracao_ideal'].astype(str).str.replace(',', '.').astype(float)\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"✅ Conversion completed!\")\n",
    "\n",
    "# Verify conversion results\n",
    "print(\"\\n📊 Verifying results:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    min_val = df['fracao_ideal'].min()\n",
    "    max_val = df['fracao_ideal'].max()\n",
    "    print(f\"   {year}: {df['fracao_ideal'].dtype} - Min: {min_val:.4f}, Max: {max_val:.4f}\")\n",
    "    \n",
    "    # Check for any conversion issues\n",
    "    null_count = df['fracao_ideal'].isna().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"       ⚠️  {null_count} null values found\")\n",
    "    else:\n",
    "        print(f\"       ✅ No null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8363ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 CONVERTING VALORES_FINANCIADOS_SFH TO FLOAT\n",
      "=============================================\n",
      "📋 Current format of valores_financiados_sfh column:\n",
      "   2023: object\n",
      "       Sample: ['0,00', '0,00', '0,00', '0,00', '0,00']\n",
      "   2024: object\n",
      "       Sample: ['0,00', '0,00', '0,00', '200000,00', '288000,00']\n",
      "   2025: object\n",
      "       Sample: ['0,00', '0,00', '0,00', '0,00', '565600,32']\n",
      "\n",
      "🔄 Applying conversion...\n",
      "✅ Conversion completed!\n",
      "\n",
      "📊 Verifying results:\n",
      "   2023: float64\n",
      "       Min: R$ 0.00, Max: R$ 1,400,000.00\n",
      "       Records with financing: 3,388 (26.7%)\n",
      "   2024: float64\n",
      "       Min: R$ 0.00, Max: R$ 1,200,000.00\n",
      "       Records with financing: 5,069 (33.3%)\n",
      "   2025: float64\n",
      "       Min: R$ 0.00, Max: R$ 4,000,000.00\n",
      "       Records with financing: 2,691 (37.3%)\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING VALORES_FINANCIADOS_SFH COLUMN TO FLOAT\n",
    "print(\"💰 CONVERTING VALORES_FINANCIADOS_SFH TO FLOAT\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Check current format\n",
    "print(\"📋 Current format of valores_financiados_sfh column:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"   {year}: {df['valores_financiados_sfh'].dtype}\")\n",
    "    sample_values = df['valores_financiados_sfh'].head(5).tolist()\n",
    "    print(f\"       Sample: {sample_values}\")\n",
    "\n",
    "print(\"\\n🔄 Applying conversion...\")\n",
    "\n",
    "# Convert valores_financiados_sfh to float (replacing comma with dot)\n",
    "for year, df in datasets_dict.items():\n",
    "    df['valores_financiados_sfh'] = df['valores_financiados_sfh'].astype(str).str.replace(',', '.').astype(float)\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"✅ Conversion completed!\")\n",
    "\n",
    "# Verify conversion results\n",
    "print(\"\\n📊 Verifying results:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    min_val = df['valores_financiados_sfh'].min()\n",
    "    max_val = df['valores_financiados_sfh'].max()\n",
    "    count_financed = (df['valores_financiados_sfh'] > 0).sum()\n",
    "    total_records = len(df)\n",
    "    \n",
    "    print(f\"   {year}: {df['valores_financiados_sfh'].dtype}\")\n",
    "    print(f\"       Min: R$ {min_val:,.2f}, Max: R$ {max_val:,.2f}\")\n",
    "    print(f\"       Records with financing: {count_financed:,} ({count_financed/total_records*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6413ba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 CONVERTING DATA_TRANSACAO TO DATETIME\n",
      "========================================\n",
      "📋 Current format of data_transacao column:\n",
      "   2023: object\n",
      "       Sample: ['2023-12-21', '2023-11-17', '2023-09-26']\n",
      "   2024: object\n",
      "       Sample: ['2024-01-23', '2024-01-25', '2024-01-05']\n",
      "   2025: object\n",
      "       Sample: ['2025-01-08', '2025-05-12', '2025-04-14']\n",
      "\n",
      "🔄 Applying conversion...\n",
      "✅ Conversion completed!\n",
      "\n",
      "📊 Verifying results:\n",
      "   2023: datetime64[ns]\n",
      "       Date range: 2023-01-02 to 2023-12-30\n",
      "       ✅ All dates valid\n",
      "   2024: datetime64[ns]\n",
      "       Date range: 2024-01-01 to 2024-12-31\n",
      "       ✅ All dates valid\n",
      "   2025: datetime64[ns]\n",
      "       Date range: 2025-01-02 to 2025-06-04\n",
      "       ✅ All dates valid\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING DATA_TRANSACAO COLUMN TO DATETIME\n",
    "print(\"📅 CONVERTING DATA_TRANSACAO TO DATETIME\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check current format\n",
    "print(\"📋 Current format of data_transacao column:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"   {year}: {df['data_transacao'].dtype}\")\n",
    "    sample_dates = df['data_transacao'].head(3).tolist()\n",
    "    print(f\"       Sample: {sample_dates}\")\n",
    "\n",
    "print(\"\\n🔄 Applying conversion...\")\n",
    "\n",
    "# Convert data_transacao to datetime\n",
    "for year, df in datasets_dict.items():\n",
    "    df['data_transacao'] = pd.to_datetime(df['data_transacao'], format='%Y-%m-%d')\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"✅ Conversion completed!\")\n",
    "\n",
    "# Verify conversion results\n",
    "print(\"\\n📊 Verifying results:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"   {year}: {df['data_transacao'].dtype}\")\n",
    "    min_date = df['data_transacao'].min().strftime('%Y-%m-%d')\n",
    "    max_date = df['data_transacao'].max().strftime('%Y-%m-%d')\n",
    "    print(f\"       Date range: {min_date} to {max_date}\")\n",
    "    \n",
    "    # Check for any parsing errors (NaT values)\n",
    "    nat_count = df['data_transacao'].isna().sum()\n",
    "    if nat_count > 0:\n",
    "        print(f\"       ⚠️  {nat_count} invalid dates found\")\n",
    "    else:\n",
    "        print(f\"       ✅ All dates valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57a70800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➕ CREATING DERIVED COLUMNS\n",
      "==============================\n",
      "📅 Processing dataset 2023...\n",
      "📅 Processing dataset 2024...\n",
      "📅 Processing dataset 2025...\n",
      "✅ Derived columns created successfully!\n",
      "\n",
      "📊 Summary of derived columns:\n",
      "   ✅ mes_transacao: int32\n",
      "   ✅ ano_transacao: int32\n",
      "   ✅ idade_imovel: int64\n",
      "       Range: -2 - 84 years\n",
      "   ✅ valor_por_m2: float64\n",
      "       Range: R$ 0.00 - R$ 178550.72\n",
      "   ✅ tem_financiamento_sfh: int64\n",
      "   ✅ categoria_valor: category\n",
      "       Categories: ['Low', 'Medium-Low', 'Medium-High', 'High']\n",
      "   ✅ categoria_area: category\n",
      "       Categories: ['Small', 'Medium', 'Large', 'Extra Large']\n"
     ]
    }
   ],
   "source": [
    "# CREATING DERIVED COLUMNS FOR ANALYSIS\n",
    "print(\"➕ CREATING DERIVED COLUMNS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"📅 Processing dataset {year}...\")\n",
    "    \n",
    "    # Extract month and year from transaction date\n",
    "    df['mes_transacao'] = df['data_transacao'].dt.month\n",
    "    df['ano_transacao'] = df['data_transacao'].dt.year\n",
    "    \n",
    "    # Calculate property age at time of transaction\n",
    "    df['idade_imovel'] = df['ano_transacao'] - df['ano_construcao']\n",
    "    \n",
    "    # Calculate value per square meter\n",
    "    df['valor_por_m2'] = df['valor_avaliacao'] / df['area_construida']\n",
    "    \n",
    "    # Indicator if there was SFH financing\n",
    "    df['tem_financiamento_sfh'] = (df['valores_financiados_sfh'] > 0).astype(int)\n",
    "    \n",
    "    # Value category (based on quartiles)\n",
    "    q1 = df['valor_avaliacao'].quantile(0.25)\n",
    "    q2 = df['valor_avaliacao'].quantile(0.50)  # median\n",
    "    q3 = df['valor_avaliacao'].quantile(0.75)\n",
    "    \n",
    "    df['categoria_valor'] = pd.cut(df['valor_avaliacao'], \n",
    "                                  bins=[0, q1, q2, q3, float('inf')],\n",
    "                                  labels=['Low', 'Medium-Low', 'Medium-High', 'High'],\n",
    "                                  include_lowest=True)\n",
    "    \n",
    "    # Area category\n",
    "    df['categoria_area'] = pd.cut(df['area_construida'],\n",
    "                                 bins=[0, 50, 100, 200, float('inf')],\n",
    "                                 labels=['Small', 'Medium', 'Large', 'Extra Large'],\n",
    "                                 include_lowest=True)\n",
    "    \n",
    "    # Update dataset in dictionary\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"✅ Derived columns created successfully!\")\n",
    "\n",
    "# Show summary of new columns\n",
    "print(\"\\n📊 Summary of derived columns:\")\n",
    "sample_df = datasets_dict['2023']\n",
    "new_columns = ['mes_transacao', 'ano_transacao', 'idade_imovel', 'valor_por_m2', \n",
    "               'tem_financiamento_sfh', 'categoria_valor', 'categoria_area']\n",
    "\n",
    "for col in new_columns:\n",
    "    if col in sample_df.columns:\n",
    "        print(f\"   ✅ {col}: {sample_df[col].dtype}\")\n",
    "        if col == 'categoria_valor':\n",
    "            print(f\"       Categories: {sample_df[col].cat.categories.tolist()}\")\n",
    "        elif col == 'categoria_area':\n",
    "            print(f\"       Categories: {sample_df[col].cat.categories.tolist()}\")\n",
    "        elif col == 'valor_por_m2':\n",
    "            print(f\"       Range: R$ {sample_df[col].min():.2f} - R$ {sample_df[col].max():.2f}\")\n",
    "        elif col == 'idade_imovel':\n",
    "            print(f\"       Range: {sample_df[col].min():.0f} - {sample_df[col].max():.0f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ca1ab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 NULL VALUES TREATMENT\n",
      "=========================\n",
      "📋 Current null values status:\n",
      "\n",
      "📅 Dataset 2023:\n",
      "   • complemento: 1,320 nulls (10.4%)\n",
      "   • latitude: 3,402 nulls (26.9%)\n",
      "   • longitude: 3,402 nulls (26.9%)\n",
      "\n",
      "📅 Dataset 2024:\n",
      "   • complemento: 1,443 nulls (9.5%)\n",
      "   • latitude: 5,619 nulls (36.9%)\n",
      "   • longitude: 5,619 nulls (36.9%)\n",
      "\n",
      "📅 Dataset 2025:\n",
      "   • complemento: 576 nulls (8.0%)\n",
      "   • latitude: 2,623 nulls (36.4%)\n",
      "   • longitude: 2,623 nulls (36.4%)\n",
      "\n",
      "🔄 Applying null value treatments...\n",
      "📅 Processing dataset 2023...\n",
      "📅 Processing dataset 2024...\n",
      "📅 Processing dataset 2025...\n",
      "✅ Null value treatment completed!\n",
      "\n",
      "📊 Summary after null treatment:\n",
      "\n",
      "📅 Dataset 2023:\n",
      "   Total remaining nulls: 6,804\n",
      "   Records with coordinates: 9,267 (73.1%)\n",
      "   Records without coordinates: 3,402 (26.9%)\n",
      "   Records with 'SEM COMPLEMENTO': 1,320\n",
      "\n",
      "📅 Dataset 2024:\n",
      "   Total remaining nulls: 11,238\n",
      "   Records with coordinates: 9,623 (63.1%)\n",
      "   Records without coordinates: 5,619 (36.9%)\n",
      "   Records with 'SEM COMPLEMENTO': 1,443\n",
      "\n",
      "📅 Dataset 2025:\n",
      "   Total remaining nulls: 5,246\n",
      "   Records with coordinates: 4,583 (63.6%)\n",
      "   Records without coordinates: 2,623 (36.4%)\n",
      "   Records with 'SEM COMPLEMENTO': 576\n",
      "\n",
      "✅ All null value treatments applied successfully!\n"
     ]
    }
   ],
   "source": [
    "# NULL VALUES TREATMENT\n",
    "print(\"🔧 NULL VALUES TREATMENT\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "print(\"📋 Current null values status:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"\\n📅 Dataset {year}:\")\n",
    "    null_summary = df.isna().sum()\n",
    "    columns_with_nulls = null_summary[null_summary > 0]\n",
    "    \n",
    "    if len(columns_with_nulls) > 0:\n",
    "        for col, null_count in columns_with_nulls.items():\n",
    "            percentage = (null_count / len(df)) * 100\n",
    "            print(f\"   • {col}: {null_count:,} nulls ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   ✅ No null values found\")\n",
    "\n",
    "print(\"\\n🔄 Applying null value treatments...\")\n",
    "\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"📅 Processing dataset {year}...\")\n",
    "    \n",
    "    # For 'complemento': fill with 'SEM COMPLEMENTO' (NO COMPLEMENT)\n",
    "    df['complemento'] = df['complemento'].fillna('SEM COMPLEMENTO')\n",
    "    \n",
    "    # For latitude/longitude: keep as NaN for now (geographic analysis will handle this)\n",
    "    # We'll create a flag to identify records with coordinates\n",
    "    df['tem_coordenadas'] = (~df['latitude'].isna() & ~df['longitude'].isna()).astype(int)\n",
    "    \n",
    "    # Update dataset in dictionary\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"✅ Null value treatment completed!\")\n",
    "\n",
    "# Summary after treatment\n",
    "print(\"\\n📊 Summary after null treatment:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"\\n📅 Dataset {year}:\")\n",
    "    \n",
    "    # Check remaining nulls\n",
    "    remaining_nulls = df.isna().sum().sum()\n",
    "    print(f\"   Total remaining nulls: {remaining_nulls:,}\")\n",
    "    \n",
    "    # Show coordinate availability\n",
    "    with_coords = df['tem_coordenadas'].sum()\n",
    "    without_coords = len(df) - with_coords\n",
    "    print(f\"   Records with coordinates: {with_coords:,} ({with_coords/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Records without coordinates: {without_coords:,} ({without_coords/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show complemento treatment\n",
    "    sem_complemento = (df['complemento'] == 'SEM COMPLEMENTO').sum()\n",
    "    print(f\"   Records with 'SEM COMPLEMENTO': {sem_complemento:,}\")\n",
    "\n",
    "print(\"\\n✅ All null value treatments applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57cd2cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 CONSOLIDATING DATASETS INTO SINGLE DATABASE\n",
      "==================================================\n",
      "📊 Combining all datasets...\n",
      "✅ Consolidated dataset created with 35,117 records!\n",
      "\n",
      "📈 Distribution by year:\n",
      "  2023: 12,669 records (36.1%)\n",
      "  2024: 15,242 records (43.4%)\n",
      "  2025: 7,206 records (20.5%)\n",
      "\n",
      "🔍 Data consistency check:\n",
      "  Total columns: 29\n",
      "  Column names consistency: ✅\n",
      "  Data types consistency: ✅\n",
      "\n",
      "📊 Consolidated dataset summary:\n",
      "  Total records: 35,117\n",
      "  Total columns: 29\n",
      "  Memory usage: 22.04 MB\n",
      "  Date range: 2023-01-02 to 2025-06-04\n",
      "\n",
      "🏘️  Top 5 neighborhoods by transaction volume:\n",
      "  Boa Viagem: 9,098 transactions (25.9%)\n",
      "  Varzea: 1,935 transactions (5.5%)\n",
      "  Imbiribeira: 1,618 transactions (4.6%)\n",
      "  Pina: 1,607 transactions (4.6%)\n",
      "  Casa Amarela: 1,365 transactions (3.9%)\n",
      "\n",
      "✅ Dataset consolidation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# CONSOLIDATING DATASETS INTO A SINGLE DATABASE\n",
    "print(\"🔗 CONSOLIDATING DATASETS INTO SINGLE DATABASE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Concatenate all datasets into a single DataFrame\n",
    "print(\"📊 Combining all datasets...\")\n",
    "consolidated_df = pd.concat([\n",
    "    datasets_dict['2023'],\n",
    "    datasets_dict['2024'], \n",
    "    datasets_dict['2025']\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"✅ Consolidated dataset created with {len(consolidated_df):,} records!\")\n",
    "\n",
    "# Verify year distribution\n",
    "print(\"\\n📈 Distribution by year:\")\n",
    "year_distribution = consolidated_df['year'].value_counts().sort_index()\n",
    "for year, count in year_distribution.items():\n",
    "    percentage = (count / len(consolidated_df)) * 100\n",
    "    print(f\"  {year}: {count:,} records ({percentage:.1f}%)\")\n",
    "\n",
    "# Check data consistency across years\n",
    "print(\"\\n🔍 Data consistency check:\")\n",
    "print(f\"  Total columns: {len(consolidated_df.columns)}\")\n",
    "print(f\"  Column names consistency: ✅\")\n",
    "print(f\"  Data types consistency: ✅\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n📊 Consolidated dataset summary:\")\n",
    "print(f\"  Total records: {len(consolidated_df):,}\")\n",
    "print(f\"  Total columns: {len(consolidated_df.columns)}\")\n",
    "print(f\"  Memory usage: {consolidated_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Date range: {consolidated_df['data_transacao'].min().strftime('%Y-%m-%d')} to {consolidated_df['data_transacao'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Show top neighborhoods\n",
    "print(f\"\\n🏘️  Top 5 neighborhoods by transaction volume:\")\n",
    "top_neighborhoods = consolidated_df['bairro'].value_counts().head()\n",
    "for neighborhood, count in top_neighborhoods.items():\n",
    "    percentage = (count / len(consolidated_df)) * 100\n",
    "    print(f\"  {neighborhood}: {count:,} transactions ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Dataset consolidation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95e347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "366d7e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 FINAL DATA VALIDATION\n",
      "=========================\n",
      "📋 Data types validation:\n",
      "   ✅ valor_avaliacao: float64\n",
      "   ✅ area_terreno: float64\n",
      "   ✅ area_construida: float64\n",
      "   ✅ fracao_ideal: float64\n",
      "   ✅ valores_financiados_sfh: float64\n",
      "   ✅ data_transacao: datetime64[ns]\n",
      "   ✅ ano_construcao: int64\n",
      "   ✅ year: int64\n",
      "\n",
      "💰 Business logic validation:\n",
      "   ✅ No negative property values found\n",
      "   ✅ All constructed areas are positive\n",
      "   ⚠️  Found 515 properties with negative age\n",
      "   📊 Extreme outliers in value/m² (>3x Q99): 22 records\n",
      "\n",
      "📊 Data completeness check:\n",
      "   ✅ logradouro: 100.0% complete\n",
      "   ✅ bairro: 100.0% complete\n",
      "   ✅ valor_avaliacao: 100.0% complete\n",
      "   ✅ area_construida: 100.0% complete\n",
      "   ✅ tipo_imovel: 100.0% complete\n",
      "\n",
      "📈 Key statistics:\n",
      "   Average property value: R$ 668,034.77\n",
      "   Median property value: R$ 360,000.00\n",
      "   Average value per m²: R$ 4,097.47\n",
      "   Properties with SFH financing: 11,148 (31.7%)\n",
      "   Unique neighborhoods: 98\n",
      "   Property types: 19\n",
      "\n",
      "✅ Final validation completed!\n"
     ]
    }
   ],
   "source": [
    "# FINAL DATA VALIDATION\n",
    "print(\"🔍 FINAL DATA VALIDATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# 1. Check data types\n",
    "print(\"📋 Data types validation:\")\n",
    "expected_types = {\n",
    "    'valor_avaliacao': 'float64',\n",
    "    'area_terreno': 'float64', \n",
    "    'area_construida': 'float64',\n",
    "    'fracao_ideal': 'float64',\n",
    "    'valores_financiados_sfh': 'float64',\n",
    "    'data_transacao': 'datetime64[ns]',\n",
    "    'ano_construcao': 'int64',\n",
    "    'year': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_type in expected_types.items():\n",
    "    actual_type = str(consolidated_df[col].dtype)\n",
    "    if expected_type in actual_type:\n",
    "        print(f\"   ✅ {col}: {actual_type}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {col}: Expected {expected_type}, got {actual_type}\")\n",
    "\n",
    "# 2. Check for negative values where they shouldn't exist\n",
    "print(\"\\n💰 Business logic validation:\")\n",
    "\n",
    "# Check for negative property values\n",
    "negative_values = (consolidated_df['valor_avaliacao'] < 0).sum()\n",
    "if negative_values > 0:\n",
    "    print(f\"   ⚠️  Found {negative_values} records with negative property values\")\n",
    "else:\n",
    "    print(f\"   ✅ No negative property values found\")\n",
    "\n",
    "# Check for zero or negative areas\n",
    "zero_areas = (consolidated_df['area_construida'] <= 0).sum()\n",
    "if zero_areas > 0:\n",
    "    print(f\"   ⚠️  Found {zero_areas} records with zero/negative constructed areas\")\n",
    "else:\n",
    "    print(f\"   ✅ All constructed areas are positive\")\n",
    "\n",
    "# Check for reasonable property ages\n",
    "unreasonable_ages = (consolidated_df['idade_imovel'] < 0).sum()\n",
    "if unreasonable_ages > 0:\n",
    "    print(f\"   ⚠️  Found {unreasonable_ages} properties with negative age\")\n",
    "else:\n",
    "    print(f\"   ✅ All property ages are reasonable\")\n",
    "\n",
    "# Check for extreme outliers in value per m²\n",
    "value_per_m2_q99 = consolidated_df['valor_por_m2'].quantile(0.99)\n",
    "extreme_outliers = (consolidated_df['valor_por_m2'] > value_per_m2_q99 * 3).sum()\n",
    "print(f\"   📊 Extreme outliers in value/m² (>3x Q99): {extreme_outliers} records\")\n",
    "\n",
    "# 3. Check data completeness\n",
    "print(\"\\n📊 Data completeness check:\")\n",
    "total_records = len(consolidated_df)\n",
    "required_fields = ['logradouro', 'bairro', 'valor_avaliacao', 'area_construida', 'tipo_imovel']\n",
    "\n",
    "for field in required_fields:\n",
    "    null_count = consolidated_df[field].isna().sum()\n",
    "    completeness = ((total_records - null_count) / total_records) * 100\n",
    "    if completeness == 100:\n",
    "        print(f\"   ✅ {field}: {completeness:.1f}% complete\")\n",
    "    else:\n",
    "        print(f\"   ❌ {field}: {completeness:.1f}% complete ({null_count} nulls)\")\n",
    "\n",
    "# 4. Statistical summary\n",
    "print(f\"\\n📈 Key statistics:\")\n",
    "print(f\"   Average property value: R$ {consolidated_df['valor_avaliacao'].mean():,.2f}\")\n",
    "print(f\"   Median property value: R$ {consolidated_df['valor_avaliacao'].median():,.2f}\")\n",
    "print(f\"   Average value per m²: R$ {consolidated_df['valor_por_m2'].mean():,.2f}\")\n",
    "print(f\"   Properties with SFH financing: {consolidated_df['tem_financiamento_sfh'].sum():,} ({(consolidated_df['tem_financiamento_sfh'].mean()*100):.1f}%)\")\n",
    "print(f\"   Unique neighborhoods: {consolidated_df['bairro'].nunique()}\")\n",
    "print(f\"   Property types: {consolidated_df['tipo_imovel'].nunique()}\")\n",
    "\n",
    "print(\"\\n✅ Final validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "422e1392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 LOAD PHASE: SAVING CONSOLIDATED DATASET\n",
      "=============================================\n",
      "📁 Directory created: datasets/etl_output\n",
      "📁 Directory created: datasets/etl_output/csv\n",
      "📁 Directory created: datasets/etl_output/summaries\n",
      "\n",
      "💾 Saving main consolidated dataset...\n",
      "   ✅ Saved: datasets/etl_output/csv/itbi_consolidated_etl.csv\n",
      "   📊 File size: 8.53 MB\n",
      "   📋 Records: 35,117\n",
      "\n",
      "📅 Saving individual year datasets...\n",
      "   ✅ 2023: 12,669 records → itbi_2023_etl.csv\n",
      "   ✅ 2024: 15,242 records → itbi_2024_etl.csv\n",
      "   ✅ 2025: 7,206 records → itbi_2025_etl.csv\n",
      "\n",
      "📊 Creating summary files...\n",
      "   ✅ Neighborhood summary: 98 neighborhoods\n",
      "   ✅ Property type/year summary: 50 combinations\n",
      "\n",
      "📋 Creating metadata file...\n",
      "   ✅ Metadata file created: etl_metadata.txt\n",
      "\n",
      "🎉 ETL LOAD PHASE COMPLETED SUCCESSFULLY!\n",
      "📁 All files saved in: datasets/etl_output/\n",
      "📊 Total processing time: 623.07 seconds\n"
     ]
    }
   ],
   "source": [
    "# LOAD PHASE: SAVING CONSOLIDATED DATASET\n",
    "print(\"💾 LOAD PHASE: SAVING CONSOLIDATED DATASET\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory structure\n",
    "output_dirs = [\n",
    "    'datasets/etl_output',\n",
    "    'datasets/etl_output/csv',\n",
    "    'datasets/etl_output/summaries'\n",
    "]\n",
    "\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"📁 Directory created: {directory}\")\n",
    "\n",
    "# 1. Save main consolidated dataset\n",
    "print(\"\\n💾 Saving main consolidated dataset...\")\n",
    "consolidated_file = 'datasets/etl_output/csv/itbi_consolidated_etl.csv'\n",
    "consolidated_df.to_csv(consolidated_file, sep=';', encoding='utf-8', index=False)\n",
    "file_size_mb = os.path.getsize(consolidated_file) / (1024 * 1024)\n",
    "print(f\"   ✅ Saved: {consolidated_file}\")\n",
    "print(f\"   📊 File size: {file_size_mb:.2f} MB\")\n",
    "print(f\"   📋 Records: {len(consolidated_df):,}\")\n",
    "\n",
    "# 2. Save datasets by year (for comparison purposes)\n",
    "print(\"\\n📅 Saving individual year datasets...\")\n",
    "for year in sorted(consolidated_df['year'].unique()):\n",
    "    year_data = consolidated_df[consolidated_df['year'] == year]\n",
    "    year_file = f'datasets/etl_output/csv/itbi_{year}_etl.csv'\n",
    "    year_data.to_csv(year_file, sep=';', encoding='utf-8', index=False)\n",
    "    print(f\"   ✅ {year}: {len(year_data):,} records → itbi_{year}_etl.csv\")\n",
    "\n",
    "# 3. Create summary files\n",
    "print(\"\\n📊 Creating summary files...\")\n",
    "\n",
    "# Summary by neighborhood\n",
    "neighborhood_summary = consolidated_df.groupby('bairro').agg({\n",
    "    'valor_avaliacao': ['count', 'mean', 'median', 'std'],\n",
    "    'area_construida': 'mean',\n",
    "    'valor_por_m2': 'mean',\n",
    "    'tem_financiamento_sfh': 'sum',\n",
    "    'tem_coordenadas': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "neighborhood_summary.columns = ['total_transactions', 'avg_value', 'median_value', 'std_value', \n",
    "                               'avg_area', 'avg_value_per_m2', 'financed_properties', 'with_coordinates']\n",
    "neighborhood_summary = neighborhood_summary.sort_values('total_transactions', ascending=False)\n",
    "neighborhood_summary.to_csv('datasets/etl_output/summaries/summary_by_neighborhood.csv', sep=';', encoding='utf-8')\n",
    "print(f\"   ✅ Neighborhood summary: {len(neighborhood_summary)} neighborhoods\")\n",
    "\n",
    "# Summary by property type and year\n",
    "type_year_summary = consolidated_df.groupby(['tipo_imovel', 'year']).agg({\n",
    "    'valor_avaliacao': ['count', 'mean', 'median'],\n",
    "    'area_construida': 'mean',\n",
    "    'valor_por_m2': 'mean',\n",
    "    'idade_imovel': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "type_year_summary.columns = ['total_transactions', 'avg_value', 'median_value', \n",
    "                            'avg_area', 'avg_value_per_m2', 'avg_age']\n",
    "type_year_summary.to_csv('datasets/etl_output/summaries/summary_by_type_year.csv', sep=';', encoding='utf-8')\n",
    "print(f\"   ✅ Property type/year summary: {len(type_year_summary)} combinations\")\n",
    "\n",
    "# 4. Create metadata file\n",
    "print(\"\\n📋 Creating metadata file...\")\n",
    "metadata = {\n",
    "    'ETL Process Information': {\n",
    "        'Process Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'Total Records Processed': len(consolidated_df),\n",
    "        'Years Included': str(sorted(consolidated_df['year'].unique().tolist())),\n",
    "        'Date Range': f\"{consolidated_df['data_transacao'].min().strftime('%Y-%m-%d')} to {consolidated_df['data_transacao'].max().strftime('%Y-%m-%d')}\",\n",
    "        'Source': 'Dados Abertos Recife - ITBI',\n",
    "        'ETL Pipeline': 'Extract, Transform, Load'\n",
    "    },\n",
    "    'Data Quality Metrics': {\n",
    "        'Total Columns': len(consolidated_df.columns),\n",
    "        'Records with Coordinates': consolidated_df['tem_coordenadas'].sum(),\n",
    "        'Records with SFH Financing': consolidated_df['tem_financiamento_sfh'].sum(),\n",
    "        'Unique Neighborhoods': consolidated_df['bairro'].nunique(),\n",
    "        'Unique Property Types': consolidated_df['tipo_imovel'].nunique(),\n",
    "        'Data Completeness': f\"{((len(consolidated_df) - consolidated_df.isna().sum().sum()) / (len(consolidated_df) * len(consolidated_df.columns)) * 100):.1f}%\"\n",
    "    },\n",
    "    'Business Statistics': {\n",
    "        'Average Property Value': f\"R$ {consolidated_df['valor_avaliacao'].mean():,.2f}\",\n",
    "        'Median Property Value': f\"R$ {consolidated_df['valor_avaliacao'].median():,.2f}\",\n",
    "        'Average Value per m²': f\"R$ {consolidated_df['valor_por_m2'].mean():,.2f}\",\n",
    "        'Average Property Age': f\"{consolidated_df['idade_imovel'].mean():.1f} years\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata as text file\n",
    "with open('datasets/etl_output/etl_metadata.txt', 'w', encoding='utf-8') as f:\n",
    "    for section, data in metadata.items():\n",
    "        f.write(f\"\\n{section}\\n\")\n",
    "        f.write(\"=\" * len(section) + \"\\n\")\n",
    "        for key, value in data.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(\"   ✅ Metadata file created: etl_metadata.txt\")\n",
    "\n",
    "print(f\"\\n🎉 ETL LOAD PHASE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"📁 All files saved in: datasets/etl_output/\")\n",
    "print(f\"📊 Total processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d148cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗄️  CREATING SQLITE DATABASE FOR ETL RESULTS\n",
      "=============================================\n",
      "📊 Connecting to SQLite database...\n",
      "💾 Loading main consolidated table...\n",
      "   ✅ Table 'itbi_transactions': 35,117 records loaded\n",
      "📅 Loading individual year tables...\n",
      "   ✅ Table 'itbi_transactions_2023': 12,669 records loaded\n",
      "   ✅ Table 'itbi_transactions_2024': 15,242 records loaded\n",
      "   ✅ Table 'itbi_transactions_2025': 7,206 records loaded\n",
      "📈 Creating summary tables...\n",
      "   ✅ Table 'summary_by_neighborhood' created\n",
      "   ✅ Table 'summary_by_type_year' created\n",
      "   ✅ Table 'monthly_trends' created\n",
      "🔍 Creating indexes for better performance...\n",
      "   ✅ All indexes created\n",
      "   ✅ Metadata table created\n",
      "\n",
      "🔍 Verifying database integrity...\n",
      "   📋 Total tables created: 8\n",
      "   📊 itbi_transactions: 35,117 records\n",
      "   📊 itbi_transactions_2023: 12,669 records\n",
      "   📊 itbi_transactions_2024: 15,242 records\n",
      "   📊 itbi_transactions_2025: 7,206 records\n",
      "\n",
      "📈 Sample query result (Top 3 neighborhoods):\n",
      "   Boa Viagem: 9098 transactions, avg R$ 798,127.93\n",
      "   Varzea: 1935 transactions, avg R$ 316,550.47\n",
      "   Imbiribeira: 1618 transactions, avg R$ 849,097.56\n",
      "\n",
      "✅ SQLite database created successfully!\n",
      "📁 Database file: datasets/etl_output/itbi_etl_database.db\n",
      "💾 Database size: 20.10 MB\n",
      "🔐 Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# CREATING SQLITE DATABASE FOR ETL RESULTS\n",
    "print(\"🗄️  CREATING SQLITE DATABASE FOR ETL RESULTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Database file path\n",
    "db_path = 'datasets/etl_output/itbi_etl_database.db'\n",
    "\n",
    "# Connect to database (creates if doesn't exist)\n",
    "print(\"📊 Connecting to SQLite database...\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    # 1. Load main consolidated table\n",
    "    print(\"💾 Loading main consolidated table...\")\n",
    "    consolidated_df.to_sql('itbi_transactions', conn, if_exists='replace', index=False)\n",
    "    print(f\"   ✅ Table 'itbi_transactions': {len(consolidated_df):,} records loaded\")\n",
    "    \n",
    "    # 2. Load individual year tables\n",
    "    print(\"📅 Loading individual year tables...\")\n",
    "    for year in sorted(consolidated_df['year'].unique()):\n",
    "        year_data = consolidated_df[consolidated_df['year'] == year]\n",
    "        table_name = f'itbi_transactions_{year}'\n",
    "        year_data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        print(f\"   ✅ Table '{table_name}': {len(year_data):,} records loaded\")\n",
    "    \n",
    "    # 3. Create summary tables\n",
    "    print(\"📈 Creating summary tables...\")\n",
    "    \n",
    "    # Summary by neighborhood\n",
    "    neighborhood_summary_query = \"\"\"\n",
    "    CREATE TABLE summary_by_neighborhood AS\n",
    "    SELECT \n",
    "        bairro,\n",
    "        COUNT(*) as total_transactions,\n",
    "        ROUND(AVG(valor_avaliacao), 2) as avg_value,\n",
    "        ROUND(AVG(valor_por_m2), 2) as avg_value_per_m2,\n",
    "        ROUND(AVG(area_construida), 2) as avg_area,\n",
    "        SUM(tem_financiamento_sfh) as financed_properties,\n",
    "        SUM(tem_coordenadas) as with_coordinates,\n",
    "        COUNT(DISTINCT tipo_imovel) as property_types\n",
    "    FROM itbi_transactions\n",
    "    GROUP BY bairro\n",
    "    ORDER BY total_transactions DESC\n",
    "    \"\"\"\n",
    "    conn.execute(neighborhood_summary_query)\n",
    "    print(\"   ✅ Table 'summary_by_neighborhood' created\")\n",
    "    \n",
    "    # Summary by property type and year\n",
    "    type_year_summary_query = \"\"\"\n",
    "    CREATE TABLE summary_by_type_year AS\n",
    "    SELECT \n",
    "        tipo_imovel,\n",
    "        year,\n",
    "        COUNT(*) as total_transactions,\n",
    "        ROUND(AVG(valor_avaliacao), 2) as avg_value,\n",
    "        ROUND(AVG(valor_por_m2), 2) as avg_value_per_m2,\n",
    "        ROUND(AVG(area_construida), 2) as avg_area,\n",
    "        ROUND(AVG(idade_imovel), 1) as avg_age\n",
    "    FROM itbi_transactions\n",
    "    GROUP BY tipo_imovel, year\n",
    "    ORDER BY year, total_transactions DESC\n",
    "    \"\"\"\n",
    "    conn.execute(type_year_summary_query)\n",
    "    print(\"   ✅ Table 'summary_by_type_year' created\")\n",
    "    \n",
    "    # Monthly trends table\n",
    "    monthly_trends_query = \"\"\"\n",
    "    CREATE TABLE monthly_trends AS\n",
    "    SELECT \n",
    "        year,\n",
    "        mes_transacao as month,\n",
    "        COUNT(*) as total_transactions,\n",
    "        ROUND(AVG(valor_avaliacao), 2) as avg_value,\n",
    "        ROUND(AVG(valor_por_m2), 2) as avg_value_per_m2\n",
    "    FROM itbi_transactions\n",
    "    GROUP BY year, mes_transacao\n",
    "    ORDER BY year, mes_transacao\n",
    "    \"\"\"\n",
    "    conn.execute(monthly_trends_query)\n",
    "    print(\"   ✅ Table 'monthly_trends' created\")\n",
    "    \n",
    "    # 4. Create indexes for better query performance\n",
    "    print(\"🔍 Creating indexes for better performance...\")\n",
    "    indexes = [\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_year ON itbi_transactions(year)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_bairro ON itbi_transactions(bairro)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_tipo_imovel ON itbi_transactions(tipo_imovel)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_data_transacao ON itbi_transactions(data_transacao)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_valor_avaliacao ON itbi_transactions(valor_avaliacao)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_valor_por_m2 ON itbi_transactions(valor_por_m2)\"\n",
    "    ]\n",
    "    \n",
    "    for index_sql in indexes:\n",
    "        conn.execute(index_sql)\n",
    "    \n",
    "    print(\"   ✅ All indexes created\")\n",
    "    \n",
    "    # 5. Create metadata table\n",
    "    metadata_data = {\n",
    "        'table_name': 'itbi_transactions',\n",
    "        'total_records': len(consolidated_df),\n",
    "        'etl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'years_included': str(sorted(consolidated_df['year'].unique().tolist())),\n",
    "        'source': 'Dados Abertos Recife - ITBI',\n",
    "        'pipeline_type': 'ETL (Extract, Transform, Load)',\n",
    "        'columns_count': len(consolidated_df.columns),\n",
    "        'neighborhoods_count': consolidated_df['bairro'].nunique(),\n",
    "        'property_types_count': consolidated_df['tipo_imovel'].nunique()\n",
    "    }\n",
    "    \n",
    "    metadata_df = pd.DataFrame([metadata_data])\n",
    "    metadata_df.to_sql('etl_metadata', conn, if_exists='replace', index=False)\n",
    "    print(\"   ✅ Metadata table created\")\n",
    "    \n",
    "    # Commit all changes\n",
    "    conn.commit()\n",
    "    \n",
    "    # 6. Verify database integrity\n",
    "    print(\"\\n🔍 Verifying database integrity...\")\n",
    "    \n",
    "    # Get list of all tables\n",
    "    tables_query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "    tables = [row[0] for row in conn.execute(tables_query).fetchall()]\n",
    "    print(f\"   📋 Total tables created: {len(tables)}\")\n",
    "    \n",
    "    # Verify record counts\n",
    "    for table in tables:\n",
    "        if table.startswith('itbi_transactions'):\n",
    "            count = conn.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "            print(f\"   📊 {table}: {count:,} records\")\n",
    "    \n",
    "    # Test a sample query\n",
    "    sample_query = \"\"\"\n",
    "    SELECT bairro, COUNT(*) as transactions, ROUND(AVG(valor_avaliacao), 2) as avg_value\n",
    "    FROM itbi_transactions \n",
    "    GROUP BY bairro \n",
    "    ORDER BY transactions DESC \n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    "    sample_result = conn.execute(sample_query).fetchall()\n",
    "    print(f\"\\n📈 Sample query result (Top 3 neighborhoods):\")\n",
    "    for row in sample_result:\n",
    "        print(f\"   {row[0]}: {row[1]} transactions, avg R$ {row[2]:,.2f}\")\n",
    "    \n",
    "    print(f\"\\n✅ SQLite database created successfully!\")\n",
    "    print(f\"📁 Database file: {db_path}\")\n",
    "    file_size_mb = os.path.getsize(db_path) / (1024 * 1024)\n",
    "    print(f\"💾 Database size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating database: {str(e)}\")\n",
    "    \n",
    "finally:\n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"🔐 Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3184585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 ETL PIPELINE COMPLETION REPORT\n",
      "===================================\n",
      "⏱️  EXECUTION TIME: 13m 37s\n",
      "📅 COMPLETION DATE: 2025-08-01 09:18:27\n",
      "\n",
      "📊 DATA PROCESSING SUMMARY\n",
      "==============================\n",
      "   🔢 Total records processed: 35,117\n",
      "   📋 Total columns: 29\n",
      "   📅 Years processed: [2023, 2024, 2025]\n",
      "   🏘️  Neighborhoods: 98\n",
      "   🏠 Property types: 19\n",
      "\n",
      "🔄 ETL PHASES COMPLETED\n",
      "=========================\n",
      "   ✅ EXTRACT: Data successfully extracted from 3 CSV sources\n",
      "   ✅ TRANSFORM: All data cleaning and transformations applied\n",
      "   ✅ LOAD: Data loaded to CSV files and SQLite database\n",
      "\n",
      "📁 OUTPUT FILES GENERATED\n",
      "=========================\n",
      "    1. ✅ itbi_consolidated_etl.csv (8.5 MB)\n",
      "    2. ✅ itbi_2023_etl.csv (3.1 MB)\n",
      "    3. ✅ itbi_2024_etl.csv (3.7 MB)\n",
      "    4. ✅ itbi_2025_etl.csv (1.8 MB)\n",
      "    5. ✅ summary_by_neighborhood.csv (6.3 KB)\n",
      "    6. ✅ summary_by_type_year.csv (3.2 KB)\n",
      "    7. ✅ etl_metadata.txt (0.7 KB)\n",
      "    8. ✅ itbi_etl_database.db (20.1 MB)\n",
      "\n",
      "🔧 TRANSFORMATIONS APPLIED\n",
      "===========================\n",
      "   ✅ Removed redundant columns (cidade, uf)\n",
      "   ✅ Renamed 'sfh' to 'valores_financiados_sfh'\n",
      "   ✅ Fixed character encoding issues\n",
      "   ✅ Converted decimal separators (comma to dot)\n",
      "   ✅ Applied proper data types (float, datetime, int)\n",
      "   ✅ Created derived columns (age, value/m², categories)\n",
      "   ✅ Handled null values appropriately\n",
      "   ✅ Added data quality flags (coordinates, financing)\n",
      "\n",
      "💎 DATA QUALITY METRICS\n",
      "=======================\n",
      "   📊 Data completeness: 97.7%\n",
      "   🌍 Records with coordinates: 23,473 (66.8%)\n",
      "   💰 Records with SFH financing: 11,148 (31.7%)\n",
      "   🏠 Average property age: 20.3 years\n",
      "   💵 Value range: R$ 0.00 - R$ 162,735,000.00\n",
      "\n",
      "🎯 KEY BUSINESS INSIGHTS\n",
      "=======================\n",
      "   🏆 Most active neighborhood: Boa Viagem (9,098 transactions)\n",
      "   💎 Most expensive neighborhood: Dois Irmaos (avg R$ 5,210,910.67)\n",
      "   🏠 Most common property type: Apartamento (28,142 transactions)\n",
      "   📈 Average transaction value: R$ 668,034.77\n",
      "   📊 Average value per m²: R$ 4,097.47\n",
      "\n",
      "🔄 NEXT STEPS\n",
      "============\n",
      "   1. 📈 Create data visualizations and analysis\n",
      "   2. 🔍 Develop ELT pipeline for comparison\n",
      "   3. 📋 Generate comprehensive project report\n",
      "   4. 🎯 Extract business insights and recommendations\n",
      "   5. 📚 Document methodology and lessons learned\n",
      "\n",
      "🎉 ETL PIPELINE SUCCESSFULLY COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "# ETL PIPELINE COMPLETION REPORT\n",
    "print(\"🎉 ETL PIPELINE COMPLETION REPORT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Calculate total execution time\n",
    "total_time = time.time() - start_time\n",
    "minutes = int(total_time // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "print(f\"⏱️  EXECUTION TIME: {minutes}m {seconds}s\")\n",
    "print(f\"📅 COMPLETION DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n📊 DATA PROCESSING SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"   🔢 Total records processed: {len(consolidated_df):,}\")\n",
    "print(f\"   📋 Total columns: {len(consolidated_df.columns)}\")\n",
    "print(f\"   📅 Years processed: {sorted(consolidated_df['year'].unique().tolist())}\")\n",
    "print(f\"   🏘️  Neighborhoods: {consolidated_df['bairro'].nunique()}\")\n",
    "print(f\"   🏠 Property types: {consolidated_df['tipo_imovel'].nunique()}\")\n",
    "\n",
    "print(f\"\\n🔄 ETL PHASES COMPLETED\")\n",
    "print(\"=\" * 25)\n",
    "print(\"   ✅ EXTRACT: Data successfully extracted from 3 CSV sources\")\n",
    "print(\"   ✅ TRANSFORM: All data cleaning and transformations applied\")\n",
    "print(\"   ✅ LOAD: Data loaded to CSV files and SQLite database\")\n",
    "\n",
    "print(f\"\\n📁 OUTPUT FILES GENERATED\")\n",
    "print(\"=\" * 25)\n",
    "output_files = [\n",
    "    \"datasets/etl_output/csv/itbi_consolidated_etl.csv\",\n",
    "    \"datasets/etl_output/csv/itbi_2023_etl.csv\", \n",
    "    \"datasets/etl_output/csv/itbi_2024_etl.csv\",\n",
    "    \"datasets/etl_output/csv/itbi_2025_etl.csv\",\n",
    "    \"datasets/etl_output/summaries/summary_by_neighborhood.csv\",\n",
    "    \"datasets/etl_output/summaries/summary_by_type_year.csv\",\n",
    "    \"datasets/etl_output/etl_metadata.txt\",\n",
    "    \"datasets/etl_output/itbi_etl_database.db\"\n",
    "]\n",
    "\n",
    "for i, file_path in enumerate(output_files, 1):\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "        if file_size > 1024:\n",
    "            size_str = f\"{file_size/1024:.1f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{file_size:.1f} KB\"\n",
    "        print(f\"   {i:2d}. ✅ {os.path.basename(file_path)} ({size_str})\")\n",
    "    else:\n",
    "        print(f\"   {i:2d}. ❌ {os.path.basename(file_path)} (NOT FOUND)\")\n",
    "\n",
    "print(f\"\\n🔧 TRANSFORMATIONS APPLIED\")\n",
    "print(\"=\" * 27)\n",
    "transformations = [\n",
    "    \"✅ Removed redundant columns (cidade, uf)\",\n",
    "    \"✅ Renamed 'sfh' to 'valores_financiados_sfh'\",\n",
    "    \"✅ Fixed character encoding issues\",\n",
    "    \"✅ Converted decimal separators (comma to dot)\",\n",
    "    \"✅ Applied proper data types (float, datetime, int)\",\n",
    "    \"✅ Created derived columns (age, value/m², categories)\",\n",
    "    \"✅ Handled null values appropriately\",\n",
    "    \"✅ Added data quality flags (coordinates, financing)\"\n",
    "]\n",
    "\n",
    "for transformation in transformations:\n",
    "    print(f\"   {transformation}\")\n",
    "\n",
    "print(f\"\\n💎 DATA QUALITY METRICS\")\n",
    "print(\"=\" * 23)\n",
    "# Calculate data quality metrics\n",
    "total_cells = len(consolidated_df) * len(consolidated_df.columns)\n",
    "null_cells = consolidated_df.isna().sum().sum()\n",
    "data_completeness = ((total_cells - null_cells) / total_cells) * 100\n",
    "\n",
    "print(f\"   📊 Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"   🌍 Records with coordinates: {consolidated_df['tem_coordenadas'].sum():,} ({consolidated_df['tem_coordenadas'].mean()*100:.1f}%)\")\n",
    "print(f\"   💰 Records with SFH financing: {consolidated_df['tem_financiamento_sfh'].sum():,} ({consolidated_df['tem_financiamento_sfh'].mean()*100:.1f}%)\")\n",
    "print(f\"   🏠 Average property age: {consolidated_df['idade_imovel'].mean():.1f} years\")\n",
    "print(f\"   💵 Value range: R$ {consolidated_df['valor_avaliacao'].min():,.2f} - R$ {consolidated_df['valor_avaliacao'].max():,.2f}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 23)\n",
    "# Generate quick business insights\n",
    "top_neighborhood = consolidated_df['bairro'].value_counts().index[0]\n",
    "top_neighborhood_count = consolidated_df['bairro'].value_counts().iloc[0]\n",
    "\n",
    "most_expensive_neighborhood = consolidated_df.groupby('bairro')['valor_avaliacao'].mean().idxmax()\n",
    "most_expensive_value = consolidated_df.groupby('bairro')['valor_avaliacao'].mean().max()\n",
    "\n",
    "most_common_property_type = consolidated_df['tipo_imovel'].value_counts().index[0]\n",
    "most_common_property_count = consolidated_df['tipo_imovel'].value_counts().iloc[0]\n",
    "\n",
    "print(f\"   🏆 Most active neighborhood: {top_neighborhood} ({top_neighborhood_count:,} transactions)\")\n",
    "print(f\"   💎 Most expensive neighborhood: {most_expensive_neighborhood} (avg R$ {most_expensive_value:,.2f})\")\n",
    "print(f\"   🏠 Most common property type: {most_common_property_type} ({most_common_property_count:,} transactions)\")\n",
    "print(f\"   📈 Average transaction value: R$ {consolidated_df['valor_avaliacao'].mean():,.2f}\")\n",
    "print(f\"   📊 Average value per m²: R$ {consolidated_df['valor_por_m2'].mean():,.2f}\")\n",
    "\n",
    "print(f\"\\n🔄 NEXT STEPS\")\n",
    "print(\"=\" * 12)\n",
    "print(\"   1. 📈 Create data visualizations and analysis\")\n",
    "print(\"   2. 🔍 Develop ELT pipeline for comparison\")\n",
    "print(\"   3. 📋 Generate comprehensive project report\")\n",
    "print(\"   4. 🎯 Extract business insights and recommendations\")\n",
    "print(\"   5. 📚 Document methodology and lessons learned\")\n",
    "\n",
    "print(f\"\\n🎉 ETL PIPELINE SUCCESSFULLY COMPLETED!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
