{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62178a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f0353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e202c2d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810ff5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries required to clean, standardize, and prepare the dataset for futher analysis.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "start_time  = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e86f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè† LOADING ITBI DATASETS - RECIFE\n",
      "========================================\n",
      "\n",
      "üìÖ Loading ITBI data 2023...\n",
      "   üîó URL: http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resou...\n",
      "   ‚è≥ Downloading file...\n",
      "   ‚úÖ Success: 12,669 records, 23 columns\n",
      "   üìä Data sample:\n",
      "      First neighborhoods: ['Encruzilhada', 'Encruzilhada', 'Encruzilhada']\n",
      "\n",
      "üìÖ Loading ITBI data 2024...\n",
      "   üîó URL: http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resou...\n",
      "   ‚è≥ Downloading file...\n",
      "   ‚úÖ Success: 15,242 records, 23 columns\n",
      "   üìä Data sample:\n",
      "      First neighborhoods: ['Encruzilhada', 'Encruzilhada', 'Encruzilhada']\n",
      "\n",
      "üìÖ Loading ITBI data 2025...\n",
      "   üîó URL: http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resou...\n",
      "   ‚è≥ Downloading file...\n",
      "   ‚úÖ Success: 7,206 records, 23 columns\n",
      "   üìä Data sample:\n",
      "      First neighborhoods: ['Encruzilhada', 'Encruzilhada', 'Encruzilhada']\n",
      "\n",
      "üîç VERIFication\n",
      "--------------------\n",
      "   ‚Ä¢ Total datasets loaded: 3\n",
      "   ‚Ä¢ Years included: ['2023', '2024', '2025']\n",
      "   ‚Ä¢ Expected datasets: 3\n",
      "\n",
      "üìä FINAL DATASET SUMMARY\n",
      "==============================\n",
      "   ‚Ä¢ Total records: 35,117\n",
      "   ‚Ä¢ Total columns: 23\n",
      "   ‚Ä¢ Years included: ['2023', '2024', '2025']\n",
      "   ‚Ä¢ 2023: Dataset loaded successfully\n",
      "   ‚Ä¢ 2024: Dataset loaded successfully\n",
      "   ‚Ä¢ 2025: Dataset loaded successfully\n",
      "\n",
      "üìã Sample data (first 3 rows):\n",
      "         bairro  tipo_imovel valor_avaliacao data_transacao\n",
      "0  Encruzilhada  Apartamento       505000,00     2025-01-08\n",
      "1  Encruzilhada  Apartamento       398109,72     2025-05-12\n",
      "2  Encruzilhada  Apartamento       790000,00     2025-04-14\n",
      "3  Encruzilhada  Apartamento       780000,00     2025-01-08\n",
      "\n",
      "‚úÖ Directory \"datasets\" is ready for use.\n",
      "‚úÖ ETL Extract phase completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path where datasets will be stored\n",
    "data_directory = \"datasets\"\n",
    "\n",
    "# Create the directory if it doesn't exist, avoiding errors if it already exists\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "# SIMPLIFIED VERSION - Basic loop for loading ITBI datasets\n",
    "\n",
    "# Define dataset URLs\n",
    "dataset_sources = [\n",
    "    (\"2023\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/d0c08a6f-4c27-423c-9219-8d13403816f4/download/itbi_2023.csv\"),\n",
    "    (\"2024\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/a36d548b-d705-496a-ac47-4ec36f068474/download/itbi_2024.csv\"),\n",
    "    (\"2025\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/5b582147-3935-459a-bbf7-ee623c22c97b/download/itbi_2025.csv\")\n",
    "]\n",
    "\n",
    "print(\"üè† LOADING ITBI DATASETS - RECIFE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simple loop to load each dataset\n",
    "load_success_count = 0\n",
    "all_records_total = 0\n",
    "all_columns_total = 0\n",
    "years_loaded = []\n",
    "data_storage = {}  # Dictionary to store the datasets\n",
    "\n",
    "for load_year, data_url in dataset_sources:\n",
    "    print(f\"\\nüìÖ Loading ITBI data {load_year}...\")\n",
    "    print(f\"   üîó URL: {data_url[:80]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load the CSV\n",
    "        print(f\"   ‚è≥ Downloading file...\")\n",
    "        temp_dataframe = pd.read_csv(data_url, sep=';', encoding='utf-8')\n",
    "        \n",
    "        # Check if DataFrame is not empty\n",
    "        if temp_dataframe.empty:\n",
    "            raise ValueError(\"Dataset loaded is empty\")\n",
    "        \n",
    "        # Check if it has the expected columns\n",
    "        required_columns = ['bairro', 'tipo_imovel', 'valor_avaliacao', 'data_transacao']\n",
    "        missing_columns = [col for col in required_columns if col not in temp_dataframe.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"   ‚ö†Ô∏è  Warning: Missing columns: {missing_columns}\")\n",
    "        \n",
    "        # Add year column\n",
    "        temp_dataframe['year'] = int(load_year)\n",
    "        \n",
    "        # Show basic information\n",
    "        current_records = len(temp_dataframe)\n",
    "        current_columns = len(temp_dataframe.columns)\n",
    "        \n",
    "        # Add to general totals\n",
    "        all_records_total += current_records\n",
    "        all_columns_total = current_columns  # Assume all have the same number of columns\n",
    "        years_loaded.append(load_year)\n",
    "        \n",
    "        # Save dataset in dictionary for later manipulation\n",
    "        data_storage[load_year] = temp_dataframe.copy()  # Create an independent copy\n",
    "        \n",
    "        print(f\"   ‚úÖ Success: {current_records:,} records, {current_columns} columns\")\n",
    "        print(f\"   üìä Data sample:\")\n",
    "        \n",
    "        # Check if 'bairro' column exists before showing\n",
    "        if 'bairro' in temp_dataframe.columns:\n",
    "            sample_neighborhoods = temp_dataframe['bairro'].head(3).tolist()\n",
    "            print(f\"      First neighborhoods: {sample_neighborhoods}\")\n",
    "            del sample_neighborhoods\n",
    "        else:\n",
    "            first_column_sample = temp_dataframe.iloc[:3, 0].tolist()\n",
    "            print(f\"      First 3 rows of first column: {first_column_sample}\")\n",
    "            del first_column_sample\n",
    "        \n",
    "        load_success_count += 1\n",
    "        del current_records, current_columns\n",
    "        \n",
    "    except Exception as load_error:\n",
    "        print(f\"   ‚ùå Error loading data for {load_year}: {type(load_error).__name__}\")\n",
    "        print(f\"      Details: {str(load_error)}\")\n",
    "        del load_error\n",
    "\n",
    "# Clean up loop variables\n",
    "del load_year, data_url, temp_dataframe, required_columns, missing_columns\n",
    "\n",
    "print(f\"\\nüîç VERIFication\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"   ‚Ä¢ Total datasets loaded: {load_success_count}\")\n",
    "print(f\"   ‚Ä¢ Years included: {years_loaded}\")\n",
    "print(f\"   ‚Ä¢ Expected datasets: 3\")\n",
    "print()\n",
    "\n",
    "print(f\"üìä FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"   ‚Ä¢ Total records: {all_records_total:,}\")\n",
    "print(f\"   ‚Ä¢ Total columns: {all_columns_total}\")\n",
    "print(f\"   ‚Ä¢ Years included: {years_loaded}\")\n",
    "\n",
    "print(f\"   ‚Ä¢ 2023: Dataset loaded successfully\")\n",
    "print(f\"   ‚Ä¢ 2024: Dataset loaded successfully\") \n",
    "print(f\"   ‚Ä¢ 2025: Dataset loaded successfully\")\n",
    "\n",
    "# Access specific datasets with intermediate variables\n",
    "dataset_2023 = data_storage['2023']\n",
    "dataset_2024 = data_storage['2024']\n",
    "dataset_2025 = data_storage['2025']\n",
    "\n",
    "print(f\"\\nüìã Sample data (first 3 rows):\")\n",
    "sample_data = dataset_2025[['bairro', 'tipo_imovel', 'valor_avaliacao', 'data_transacao']].head(4)\n",
    "print(sample_data)\n",
    "\n",
    "print(f'\\n‚úÖ Directory \"{data_directory}\" is ready for use.')\n",
    "print(\"‚úÖ ETL Extract phase completed successfully!\")\n",
    "\n",
    "# Clean up all intermediate variables\n",
    "del load_success_count, all_records_total, all_columns_total, years_loaded\n",
    "del dataset_2023, dataset_2024, dataset_2025, sample_data\n",
    "\n",
    "# Rename final variables for consistency\n",
    "dataset_directory = data_directory\n",
    "datasets_dict = data_storage\n",
    "del data_directory, data_storage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f036df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVING DATASETS TO FILES AND CREATING ZIP ARCHIVE\n",
      "=======================================================\n",
      "   ‚úÖ Saved: itbi_2023.csv\n",
      "   ‚úÖ Saved: itbi_2024.csv\n",
      "   ‚úÖ Saved: itbi_2025.csv\n",
      "\n",
      "‚úÖ ZIP ARCHIVE CREATED SUCCESSFULLY!\n",
      "   üì¶ Filename: itbi_datasets_recife.zip\n",
      "   üìÅ Size: 0.91 MB\n",
      "   üóÉÔ∏è  Files in ZIP: 3\n",
      "   üìÇ Location: datasets\\itbi_datasets_recife.zip\n"
     ]
    }
   ],
   "source": [
    "# Save dataframes as CSV files and create ZIP archive\n",
    "\n",
    "header_message = \"üíæ SAVING DATASETS TO FILES AND CREATING ZIP ARCHIVE\"\n",
    "separator_line = \"=\" * 55\n",
    "\n",
    "print(header_message)\n",
    "print(separator_line)\n",
    "\n",
    "# Clean up header variables immediately\n",
    "del header_message, separator_line\n",
    "\n",
    "# Initialize control variables\n",
    "csv_files_list = []\n",
    "save_successful = True\n",
    "\n",
    "# Create CSV files with proper variable management\n",
    "for dataset_year, dataset_df in datasets_dict.items():\n",
    "    # Create filename using intermediate variables\n",
    "    csv_filename = f\"itbi_{dataset_year}.csv\"\n",
    "    csv_filepath = os.path.join(dataset_directory, csv_filename)\n",
    "    \n",
    "    try:\n",
    "        # Save to CSV\n",
    "        dataset_df.to_csv(csv_filepath, sep=';', encoding='utf-8', index=False)\n",
    "        csv_files_list.append(csv_filepath)\n",
    "    except Exception as save_error:\n",
    "        # Use intermediate variable for error message\n",
    "        error_msg = f\"   ‚ùå Failed to save: {csv_filename}\"\n",
    "        print(error_msg)\n",
    "        save_successful = False\n",
    "        del save_error, error_msg\n",
    "    \n",
    "    # Clean up loop variables immediately\n",
    "    del csv_filename, csv_filepath\n",
    "\n",
    "# Clean up loop variables completely\n",
    "del dataset_year, dataset_df\n",
    "\n",
    "# Print success messages outside the loop to avoid duplicates\n",
    "for file_path in csv_files_list:\n",
    "    # Use intermediate variable for filename\n",
    "    saved_filename = os.path.basename(file_path)\n",
    "    success_msg = f\"   ‚úÖ Saved: {saved_filename}\"\n",
    "    print(success_msg)\n",
    "    del saved_filename, success_msg\n",
    "\n",
    "# CRITICAL: Clean up the loop variable\n",
    "del file_path\n",
    "\n",
    "# Create ZIP archive if CSV files were created successfully\n",
    "if csv_files_list and save_successful:\n",
    "    # Create intermediate variables for ZIP creation\n",
    "    zip_filename = \"itbi_datasets_recife.zip\"\n",
    "    zip_filepath = os.path.join(dataset_directory, zip_filename)\n",
    "    \n",
    "    try:\n",
    "        # Create ZIP with managed variables\n",
    "        with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            for source_file in csv_files_list:\n",
    "                target_filename = os.path.basename(source_file)\n",
    "                zip_file.write(source_file, target_filename)\n",
    "                del target_filename\n",
    "            del source_file\n",
    "        \n",
    "        # Verify and show results with managed variables\n",
    "        if os.path.exists(zip_filepath):\n",
    "            # Calculate file size using intermediate variables\n",
    "            file_size_bytes = os.path.getsize(zip_filepath)\n",
    "            file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "            \n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_reader:\n",
    "                zip_contents = zip_reader.namelist()\n",
    "                files_in_zip = len(zip_contents)\n",
    "            \n",
    "            # Create all success messages using intermediate variables\n",
    "            success_header = \"\\n‚úÖ ZIP ARCHIVE CREATED SUCCESSFULLY!\"\n",
    "            filename_line = f\"   üì¶ Filename: {zip_filename}\"\n",
    "            size_line = f\"   üìÅ Size: {file_size_mb:.2f} MB\"\n",
    "            files_line = f\"   üóÉÔ∏è  Files in ZIP: {files_in_zip}\"\n",
    "            location_line = f\"   üìÇ Location: {zip_filepath}\"\n",
    "            \n",
    "            print(success_header)\n",
    "            print(filename_line)\n",
    "            print(size_line)\n",
    "            print(files_line)\n",
    "            print(location_line)\n",
    "            \n",
    "            # Clean up all verification variables immediately\n",
    "            del file_size_bytes, file_size_mb, zip_contents, files_in_zip\n",
    "            del success_header, filename_line, size_line, files_line, location_line\n",
    "        else:\n",
    "            # Use intermediate variable for error message\n",
    "            zip_not_created_msg = \"   ‚ùå Error: ZIP file was not created\"\n",
    "            print(zip_not_created_msg)\n",
    "            del zip_not_created_msg\n",
    "            \n",
    "    except Exception as zip_error:\n",
    "        # Use intermediate variables for error handling\n",
    "        error_details = str(zip_error)\n",
    "        zip_error_msg = f\"   ‚ùå Error creating ZIP: {error_details}\"\n",
    "        print(zip_error_msg)\n",
    "        del zip_error, error_details, zip_error_msg\n",
    "        \n",
    "    # Clean up ZIP variables immediately\n",
    "    del zip_filename, zip_filepath\n",
    "else:\n",
    "    # Use intermediate variable for failure message\n",
    "    no_zip_msg = \"\\n‚ùå Cannot create ZIP: No CSV files or save errors occurred\"\n",
    "    print(no_zip_msg)\n",
    "    del no_zip_msg\n",
    "\n",
    "# Final comprehensive cleanup\n",
    "del csv_files_list, save_successful\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27845df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['logradouro', 'numero', 'complemento', 'valor_avaliacao', 'bairro',\n",
       "       'cidade', 'uf', 'ano_construcao', 'area_terreno', 'area_construida',\n",
       "       'fracao_ideal', 'padrao_acabamento', 'tipo_construcao', 'tipo_ocupacao',\n",
       "       'data_transacao', 'estado_conservacao', 'tipo_imovel', 'sfh',\n",
       "       'cod_logradouro', 'latitude', 'longitude', 'ano', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's take a good look at the tables and their nomenclature structure.\n",
    "# After analyzing the datasets, we can confirm that all tables follow good naming standards:\n",
    "# snake_case convention, descriptive names, Portuguese language consistency, no special characters,\n",
    "# logical grouping, and standardized separators. These naming conventions ensure database \n",
    "# compatibility, readability, and maintainability across different systems and programming environments.\n",
    "# However, the 'sfh' acronym lacks clarity and context, making it difficult for users to understand\n",
    "# its meaning without domain knowledge. To improve data documentation and usability, we will rename\n",
    "# this column to 'valores_financiados_sfh' providing explicit context about financed values.\n",
    "datasets_dict['2023'].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0846a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming renaming sfh column in order to improve understanding \n",
    "for year, df in datasets_dict.items():\n",
    "    new_df = df.rename(columns = {'sfh':'valores_financiados_sfh'})\n",
    "    datasets_dict[year] = new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7c9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü©∫ Data Health Check - Missing Values Diagnostic & Investigation\n",
      "=================================================================\n",
      "\n",
      "üìÖ Dataset 2023:\n",
      "--------------------\n",
      "  üîç Found 3 columns with missing values:\n",
      "      ‚Ä¢ complemento: 1,320 nulls \n",
      "      ‚Ä¢ latitude: 3,402 nulls \n",
      "      ‚Ä¢ longitude: 3,402 nulls \n",
      "\n",
      "üìÖ Dataset 2024:\n",
      "--------------------\n",
      "  üîç Found 3 columns with missing values:\n",
      "      ‚Ä¢ complemento: 1,443 nulls \n",
      "      ‚Ä¢ latitude: 5,619 nulls \n",
      "      ‚Ä¢ longitude: 5,619 nulls \n",
      "\n",
      "üìÖ Dataset 2025:\n",
      "--------------------\n",
      "  üîç Found 3 columns with missing values:\n",
      "      ‚Ä¢ complemento: 576 nulls \n",
      "      ‚Ä¢ latitude: 2,623 nulls \n",
      "      ‚Ä¢ longitude: 2,623 nulls \n",
      "\n",
      "üìã Final diagnosis:\n",
      "There is a total of 3 datasets with missing values out of 3 total datasets.\n"
     ]
    }
   ],
   "source": [
    "# Null values analysis \n",
    "print(\"ü©∫ Data Health Check - Missing Values Diagnostic & Investigation\")\n",
    "print(\"=\" * 65)\n",
    "missing_datasets = 0\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"\\nüìÖ Dataset {year}:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    null_summary = df.isna().sum()\n",
    "    columns_with_nulls = null_summary[null_summary > 0]\n",
    "    \n",
    "    if len(columns_with_nulls.index.tolist()) > 0:\n",
    "        \n",
    "        missing_datasets += 1\n",
    "        print(f\"  üîç Found {len(columns_with_nulls)} columns with missing values:\")\n",
    "        \n",
    "        for column_name, null_count in columns_with_nulls.items():\n",
    "            print(f\"      ‚Ä¢ {column_name}: {null_count:,} nulls \")\n",
    "            \n",
    "    else:\n",
    "        print(\"   ‚úÖ No missing values found - Dataset is complete!\")\n",
    "\n",
    "\n",
    "print(\"\\nüìã Final diagnosis:\")\n",
    "print(f'There is a total of {missing_datasets} datasets with missing values out of {len(datasets_dict)} total datasets.')\n",
    "\n",
    "# NEXT STEP: DATA CLEANING AND NULL VALUES TREATMENT\n",
    "# Now that we've identified null values in some datasets, we need to perform cleaning\n",
    "# and removal of these missing values to prevent issues during subsequent analysis.\n",
    "# Null values can cause errors in statistical calculations, visualizations, and data modeling.\n",
    "# Proper treatment of these values is essential for ETL pipeline integrity and reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7dae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12669 entries, 0 to 12668\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   logradouro               12669 non-null  object \n",
      " 1   numero                   12669 non-null  int64  \n",
      " 2   complemento              11349 non-null  object \n",
      " 3   valor_avaliacao          12669 non-null  object \n",
      " 4   bairro                   12669 non-null  object \n",
      " 5   ano_construcao           12669 non-null  int64  \n",
      " 6   area_terreno             12669 non-null  object \n",
      " 7   area_construida          12669 non-null  object \n",
      " 8   fracao_ideal             12669 non-null  object \n",
      " 9   padrao_acabamento        12669 non-null  object \n",
      " 10  tipo_construcao          12669 non-null  object \n",
      " 11  tipo_ocupacao            12669 non-null  object \n",
      " 12  data_transacao           12669 non-null  object \n",
      " 13  estado_conservacao       12669 non-null  object \n",
      " 14  tipo_imovel              12669 non-null  object \n",
      " 15  valores_financiados_sfh  12669 non-null  object \n",
      " 16  cod_logradouro           12669 non-null  int64  \n",
      " 17  latitude                 9267 non-null   float64\n",
      " 18  longitude                9267 non-null   float64\n",
      " 19  ano                      12669 non-null  int64  \n",
      " 20  year                     12669 non-null  int64  \n",
      "dtypes: float64(2), int64(5), object(14)\n",
      "memory usage: 2.0+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15242 entries, 0 to 15241\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   logradouro               15242 non-null  object \n",
      " 1   numero                   15242 non-null  int64  \n",
      " 2   complemento              13799 non-null  object \n",
      " 3   valor_avaliacao          15242 non-null  object \n",
      " 4   bairro                   15242 non-null  object \n",
      " 5   ano_construcao           15242 non-null  int64  \n",
      " 6   area_terreno             15242 non-null  object \n",
      " 7   area_construida          15242 non-null  object \n",
      " 8   fracao_ideal             15242 non-null  object \n",
      " 9   padrao_acabamento        15242 non-null  object \n",
      " 10  tipo_construcao          15242 non-null  object \n",
      " 11  tipo_ocupacao            15242 non-null  object \n",
      " 12  data_transacao           15242 non-null  object \n",
      " 13  estado_conservacao       15242 non-null  object \n",
      " 14  tipo_imovel              15242 non-null  object \n",
      " 15  valores_financiados_sfh  15242 non-null  object \n",
      " 16  cod_logradouro           15242 non-null  int64  \n",
      " 17  latitude                 9623 non-null   float64\n",
      " 18  longitude                9623 non-null   float64\n",
      " 19  ano                      15242 non-null  int64  \n",
      " 20  year                     15242 non-null  int64  \n",
      "dtypes: float64(2), int64(5), object(14)\n",
      "memory usage: 2.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7206 entries, 0 to 7205\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   logradouro               7206 non-null   object \n",
      " 1   numero                   7206 non-null   int64  \n",
      " 2   complemento              6630 non-null   object \n",
      " 3   valor_avaliacao          7206 non-null   object \n",
      " 4   bairro                   7206 non-null   object \n",
      " 5   ano_construcao           7206 non-null   int64  \n",
      " 6   area_terreno             7206 non-null   object \n",
      " 7   area_construida          7206 non-null   object \n",
      " 8   fracao_ideal             7206 non-null   object \n",
      " 9   padrao_acabamento        7206 non-null   object \n",
      " 10  tipo_construcao          7206 non-null   object \n",
      " 11  tipo_ocupacao            7206 non-null   object \n",
      " 12  data_transacao           7206 non-null   object \n",
      " 13  estado_conservacao       7206 non-null   object \n",
      " 14  tipo_imovel              7206 non-null   object \n",
      " 15  valores_financiados_sfh  7206 non-null   object \n",
      " 16  cod_logradouro           7206 non-null   int64  \n",
      " 17  latitude                 4583 non-null   float64\n",
      " 18  longitude                4583 non-null   float64\n",
      " 19  ano                      7206 non-null   int64  \n",
      " 20  year                     7206 non-null   int64  \n",
      "dtypes: float64(2), int64(5), object(14)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# COLUMN OPTIMIZATION: REMOVING REDUNDANT GEOGRAPHIC COLUMNS\n",
    "# We will drop the 'cidade' and 'uf' columns as they contain only uniform values across all records\n",
    "# (Recife and PE respectively). Since our analysis focuses specifically on ITBI data from Recife's\n",
    "# urban region within Pernambuco state, these columns provide no analytical value or variation.\n",
    "# Removing these redundant columns optimizes memory usage and simplifies the dataset structure\n",
    "# without losing any meaningful information for our geographic scope of analysis.\n",
    "\n",
    "for year, df in datasets_dict.items():\n",
    "    df = df.drop([\"cidade\", \"uf\"], axis =1)\n",
    "    df.info()\n",
    "    datasets_dict[year] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f9f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    av norte miguel arraes de alencar\n",
      "1    av norte miguel arraes de alencar\n",
      "2                   rua belmiro corr√™a\n",
      "3                   rua belmiro corr√™a\n",
      "4                   rua belmiro corr√™a\n",
      "5                   rua belmiro corr√™a\n",
      "6                   rua belmiro corr√™a\n",
      "7                   rua belmiro corr√™a\n",
      "8                   rua belmiro corr√™a\n",
      "9                   rua belmiro corr√™a\n",
      "Name: logradouro, dtype: object\n",
      "0    3071\n",
      "1    3029\n",
      "2     133\n",
      "3     133\n",
      "4     133\n",
      "5     133\n",
      "6     133\n",
      "7     133\n",
      "8     109\n",
      "9     109\n",
      "Name: numero, dtype: int64\n",
      "0          NaN\n",
      "1          NaN\n",
      "2    apto 0001\n",
      "3    apto 0001\n",
      "4    apto 0002\n",
      "5    apto 0003\n",
      "6    apto 0004\n",
      "7    apto 0005\n",
      "8          NaN\n",
      "9          NaN\n",
      "Name: complemento, dtype: object\n",
      "0    1068562,63\n",
      "1    1500000,00\n",
      "2     110000,00\n",
      "3     110000,00\n",
      "4     110000,00\n",
      "5     110000,00\n",
      "6     110000,00\n",
      "7     110000,00\n",
      "8    4900000,00\n",
      "9    4900000,00\n",
      "Name: valor_avaliacao, dtype: object\n",
      "0    Encruzilhada\n",
      "1    Encruzilhada\n",
      "2    Encruzilhada\n",
      "3    Encruzilhada\n",
      "4    Encruzilhada\n",
      "5    Encruzilhada\n",
      "6    Encruzilhada\n",
      "7    Encruzilhada\n",
      "8    Encruzilhada\n",
      "9    Encruzilhada\n",
      "Name: bairro, dtype: object\n",
      "0    1997\n",
      "1    1957\n",
      "2    1970\n",
      "3    1970\n",
      "4    1970\n",
      "5    1970\n",
      "6    1970\n",
      "7    1970\n",
      "8    1951\n",
      "9    1951\n",
      "Name: ano_construcao, dtype: int64\n",
      "0     438,0\n",
      "1    779,33\n",
      "2    562,05\n",
      "3    562,05\n",
      "4    562,05\n",
      "5    562,05\n",
      "6    562,05\n",
      "7    562,05\n",
      "8    439,28\n",
      "9    439,28\n",
      "Name: area_terreno, dtype: object\n",
      "0     511,0\n",
      "1    582,44\n",
      "2     121,0\n",
      "3     121,0\n",
      "4      81,0\n",
      "5      81,0\n",
      "6      81,0\n",
      "7      81,0\n",
      "8    343,23\n",
      "9    343,23\n",
      "Name: area_construida, dtype: object\n",
      "0        1,0\n",
      "1        1,0\n",
      "2    0,27191\n",
      "3    0,27191\n",
      "4    0,18202\n",
      "5    0,18202\n",
      "6    0,18202\n",
      "7    0,18202\n",
      "8        1,0\n",
      "9        1,0\n",
      "Name: fracao_ideal, dtype: object\n",
      "0      M√©dio\n",
      "1      M√©dio\n",
      "2    Simples\n",
      "3    Simples\n",
      "4    Simples\n",
      "5    Simples\n",
      "6    Simples\n",
      "7    Simples\n",
      "8      M√©dio\n",
      "9      M√©dio\n",
      "Name: padrao_acabamento, dtype: object\n",
      "0                         Galp√£o\n",
      "1                           Casa\n",
      "2    Apartamento <= 4 Pavimentos\n",
      "3    Apartamento <= 4 Pavimentos\n",
      "4    Apartamento <= 4 Pavimentos\n",
      "5    Apartamento <= 4 Pavimentos\n",
      "6    Apartamento <= 4 Pavimentos\n",
      "7    Apartamento <= 4 Pavimentos\n",
      "8                           Casa\n",
      "9                           Casa\n",
      "Name: tipo_construcao, dtype: object\n",
      "0    COMERCIAL COM LIXO ORGANICO\n",
      "1    COMERCIAL SEM LIXO ORGANICO\n",
      "2                    RESIDENCIAL\n",
      "3                    RESIDENCIAL\n",
      "4                    RESIDENCIAL\n",
      "5                    RESIDENCIAL\n",
      "6                    RESIDENCIAL\n",
      "7                    RESIDENCIAL\n",
      "8                    RESIDENCIAL\n",
      "9                    RESIDENCIAL\n",
      "Name: tipo_ocupacao, dtype: object\n",
      "0    2023-12-21\n",
      "1    2023-11-17\n",
      "2    2023-09-26\n",
      "3    2023-09-22\n",
      "4    2023-09-22\n",
      "5    2023-09-22\n",
      "6    2023-09-26\n",
      "7    2023-09-22\n",
      "8    2023-09-26\n",
      "9    2023-09-26\n",
      "Name: data_transacao, dtype: object\n",
      "0    Regular\n",
      "1    Regular\n",
      "2        Bom\n",
      "3        Bom\n",
      "4        Bom\n",
      "5        Bom\n",
      "6        Bom\n",
      "7        Bom\n",
      "8        Bom\n",
      "9        Bom\n",
      "Name: estado_conservacao, dtype: object\n",
      "0         Galp√£o\n",
      "1           Casa\n",
      "2    Apartamento\n",
      "3    Apartamento\n",
      "4    Apartamento\n",
      "5    Apartamento\n",
      "6    Apartamento\n",
      "7    Apartamento\n",
      "8           Casa\n",
      "9           Casa\n",
      "Name: tipo_imovel, dtype: object\n",
      "0    0,00\n",
      "1    0,00\n",
      "2    0,00\n",
      "3    0,00\n",
      "4    0,00\n",
      "5    0,00\n",
      "6    0,00\n",
      "7    0,00\n",
      "8    0,00\n",
      "9    0,00\n",
      "Name: valores_financiados_sfh, dtype: object\n",
      "0    46540\n",
      "1    46540\n",
      "2    10715\n",
      "3    10715\n",
      "4    10715\n",
      "5    10715\n",
      "6    10715\n",
      "7    10715\n",
      "8    10715\n",
      "9    10715\n",
      "Name: cod_logradouro, dtype: int64\n",
      "0   -8.034273\n",
      "1   -8.034435\n",
      "2   -8.035013\n",
      "3   -8.035013\n",
      "4   -8.035013\n",
      "5   -8.035013\n",
      "6   -8.035013\n",
      "7   -8.035013\n",
      "8   -8.035165\n",
      "9   -8.035165\n",
      "Name: latitude, dtype: float64\n",
      "0   -34.896337\n",
      "1   -34.896335\n",
      "2   -34.895903\n",
      "3   -34.895903\n",
      "4   -34.895903\n",
      "5   -34.895903\n",
      "6   -34.895903\n",
      "7   -34.895903\n",
      "8   -34.895961\n",
      "9   -34.895961\n",
      "Name: longitude, dtype: float64\n",
      "0    2023\n",
      "1    2023\n",
      "2    2023\n",
      "3    2023\n",
      "4    2023\n",
      "5    2023\n",
      "6    2023\n",
      "7    2023\n",
      "8    2023\n",
      "9    2023\n",
      "Name: ano, dtype: int64\n",
      "0    2023\n",
      "1    2023\n",
      "2    2023\n",
      "3    2023\n",
      "4    2023\n",
      "5    2023\n",
      "6    2023\n",
      "7    2023\n",
      "8    2023\n",
      "9    2023\n",
      "Name: year, dtype: int64\n",
      "0    av norte miguel arraes de alencar\n",
      "1    av norte miguel arraes de alencar\n",
      "2    av norte miguel arraes de alencar\n",
      "3                     rua caio pereira\n",
      "4                     rua caio pereira\n",
      "5                     rua caio pereira\n",
      "6                     rua caio pereira\n",
      "7                     rua caio pereira\n",
      "8                     rua caio pereira\n",
      "9                     rua caio pereira\n",
      "Name: logradouro, dtype: object\n",
      "0    3071\n",
      "1    3029\n",
      "2    3029\n",
      "3     375\n",
      "4     375\n",
      "5     375\n",
      "6     375\n",
      "7     800\n",
      "8     800\n",
      "9     800\n",
      "Name: numero, dtype: int64\n",
      "0                               NaN\n",
      "1                               NaN\n",
      "2                               NaN\n",
      "3    apto 503 edf luar do rosarinho\n",
      "4    apto 602 edf luar do rosarinho\n",
      "5    apto 801 edf luar do rosarinho\n",
      "6    apto 203 edf luar do rosarinho\n",
      "7      apto 1901 edf sainte juliana\n",
      "8       apto 902 edf sainte juliana\n",
      "9       apto 203 edf sainte juliana\n",
      "Name: complemento, dtype: object\n",
      "0    1068562,63\n",
      "1    1951000,00\n",
      "2    1500000,00\n",
      "3     402544,22\n",
      "4     405198,63\n",
      "5     409994,56\n",
      "6     395501,75\n",
      "7     700000,00\n",
      "8     790000,00\n",
      "9     680000,00\n",
      "Name: valor_avaliacao, dtype: object\n",
      "0    Encruzilhada\n",
      "1    Encruzilhada\n",
      "2    Encruzilhada\n",
      "3    Encruzilhada\n",
      "4    Encruzilhada\n",
      "5    Encruzilhada\n",
      "6    Encruzilhada\n",
      "7    Encruzilhada\n",
      "8    Encruzilhada\n",
      "9    Encruzilhada\n",
      "Name: bairro, dtype: object\n",
      "0    1997\n",
      "1    1957\n",
      "2    1957\n",
      "3    2007\n",
      "4    2007\n",
      "5    2007\n",
      "6    2007\n",
      "7    2017\n",
      "8    2017\n",
      "9    2017\n",
      "Name: ano_construcao, dtype: int64\n",
      "0      438,0\n",
      "1     779,33\n",
      "2     779,33\n",
      "3     798,91\n",
      "4     798,91\n",
      "5     798,91\n",
      "6     798,91\n",
      "7    1295,39\n",
      "8    1295,39\n",
      "9    1295,39\n",
      "Name: area_terreno, dtype: object\n",
      "0     511,0\n",
      "1    582,44\n",
      "2    582,44\n",
      "3    118,55\n",
      "4    118,64\n",
      "5    118,64\n",
      "6    118,55\n",
      "7    145,68\n",
      "8     145,8\n",
      "9    145,49\n",
      "Name: area_construida, dtype: object\n",
      "0        1,0\n",
      "1        1,0\n",
      "2        1,0\n",
      "3    0,02516\n",
      "4    0,02518\n",
      "5    0,02518\n",
      "6    0,02516\n",
      "7    0,01586\n",
      "8    0,01589\n",
      "9    0,01581\n",
      "Name: fracao_ideal, dtype: object\n",
      "0       M√©dio\n",
      "1       M√©dio\n",
      "2       M√©dio\n",
      "3       M√©dio\n",
      "4       M√©dio\n",
      "5       M√©dio\n",
      "6       M√©dio\n",
      "7    Superior\n",
      "8    Superior\n",
      "9    Superior\n",
      "Name: padrao_acabamento, dtype: object\n",
      "0                        Galp√£o\n",
      "1                          Casa\n",
      "2                          Casa\n",
      "3    Apartamento > 4 Pavimentos\n",
      "4    Apartamento > 4 Pavimentos\n",
      "5    Apartamento > 4 Pavimentos\n",
      "6    Apartamento > 4 Pavimentos\n",
      "7    Apartamento > 4 Pavimentos\n",
      "8    Apartamento > 4 Pavimentos\n",
      "9    Apartamento > 4 Pavimentos\n",
      "Name: tipo_construcao, dtype: object\n",
      "0    COMERCIAL COM LIXO ORGANICO\n",
      "1    COMERCIAL SEM LIXO ORGANICO\n",
      "2    COMERCIAL SEM LIXO ORGANICO\n",
      "3                    RESIDENCIAL\n",
      "4                    RESIDENCIAL\n",
      "5                    RESIDENCIAL\n",
      "6                    RESIDENCIAL\n",
      "7                    RESIDENCIAL\n",
      "8                    RESIDENCIAL\n",
      "9                    RESIDENCIAL\n",
      "Name: tipo_ocupacao, dtype: object\n",
      "0    2024-01-23\n",
      "1    2024-01-25\n",
      "2    2024-01-05\n",
      "3    2024-10-22\n",
      "4    2024-05-15\n",
      "5    2024-08-05\n",
      "6    2024-05-22\n",
      "7    2024-04-15\n",
      "8    2024-07-26\n",
      "9    2024-02-01\n",
      "Name: data_transacao, dtype: object\n",
      "0    Regular\n",
      "1    Regular\n",
      "2    Regular\n",
      "3        Bom\n",
      "4        Bom\n",
      "5        Bom\n",
      "6        Bom\n",
      "7        Bom\n",
      "8        Bom\n",
      "9        Bom\n",
      "Name: estado_conservacao, dtype: object\n",
      "0         Galp√£o\n",
      "1           Casa\n",
      "2           Casa\n",
      "3    Apartamento\n",
      "4    Apartamento\n",
      "5    Apartamento\n",
      "6    Apartamento\n",
      "7    Apartamento\n",
      "8    Apartamento\n",
      "9    Apartamento\n",
      "Name: tipo_imovel, dtype: object\n",
      "0         0,00\n",
      "1         0,00\n",
      "2         0,00\n",
      "3    200000,00\n",
      "4    288000,00\n",
      "5    235000,00\n",
      "6         0,00\n",
      "7         0,00\n",
      "8    665000,00\n",
      "9         0,00\n",
      "Name: valores_financiados_sfh, dtype: object\n",
      "0    46540\n",
      "1    46540\n",
      "2    46540\n",
      "3    13269\n",
      "4    13269\n",
      "5    13269\n",
      "6    13269\n",
      "7    13269\n",
      "8    13269\n",
      "9    13269\n",
      "Name: cod_logradouro, dtype: int64\n",
      "0   -8.034273\n",
      "1   -8.034435\n",
      "2   -8.034435\n",
      "3   -8.034996\n",
      "4   -8.034996\n",
      "5   -8.034996\n",
      "6   -8.034996\n",
      "7         NaN\n",
      "8         NaN\n",
      "9         NaN\n",
      "Name: latitude, dtype: float64\n",
      "0   -34.896337\n",
      "1   -34.896335\n",
      "2   -34.896335\n",
      "3   -34.896187\n",
      "4   -34.896187\n",
      "5   -34.896187\n",
      "6   -34.896187\n",
      "7          NaN\n",
      "8          NaN\n",
      "9          NaN\n",
      "Name: longitude, dtype: float64\n",
      "0    2024\n",
      "1    2024\n",
      "2    2024\n",
      "3    2024\n",
      "4    2024\n",
      "5    2024\n",
      "6    2024\n",
      "7    2024\n",
      "8    2024\n",
      "9    2024\n",
      "Name: ano, dtype: int64\n",
      "0    2024\n",
      "1    2024\n",
      "2    2024\n",
      "3    2024\n",
      "4    2024\n",
      "5    2024\n",
      "6    2024\n",
      "7    2024\n",
      "8    2024\n",
      "9    2024\n",
      "Name: year, dtype: int64\n",
      "0          rua caio pereira\n",
      "1          rua caio pereira\n",
      "2          rua caio pereira\n",
      "3          rua caio pereira\n",
      "4          rua caio pereira\n",
      "5          rua caio pereira\n",
      "6     rua doutor jose maria\n",
      "7     rua doutor jose maria\n",
      "8        rua andre reboucas\n",
      "9    rua engenheiro sampaio\n",
      "Name: logradouro, dtype: object\n",
      "0    375\n",
      "1    375\n",
      "2    800\n",
      "3    800\n",
      "4    800\n",
      "5    334\n",
      "6    578\n",
      "7    658\n",
      "8    106\n",
      "9     68\n",
      "Name: numero, dtype: int64\n",
      "0     apto 803 edf luar do rosarinho\n",
      "1     apto 302 edf luar do rosarinho\n",
      "2       apto 1201 edf sainte juliana\n",
      "3       apto 1501 edf sainte juliana\n",
      "4       apto 1602 edf sainte juliana\n",
      "5     apto 202 edf essenza rosarinho\n",
      "6       apto 0102 edf praia de ceres\n",
      "7          apto 1701 edf casa rosada\n",
      "8    apto 302 edf bellagio residence\n",
      "9        apto 404 splendid rosarinho\n",
      "Name: complemento, dtype: object\n",
      "0     505000,00\n",
      "1     398109,72\n",
      "2     790000,00\n",
      "3     780000,00\n",
      "4     840000,00\n",
      "5    1000000,00\n",
      "6     595739,42\n",
      "7     600000,00\n",
      "8     230000,00\n",
      "9     300000,00\n",
      "Name: valor_avaliacao, dtype: object\n",
      "0    Encruzilhada\n",
      "1    Encruzilhada\n",
      "2    Encruzilhada\n",
      "3    Encruzilhada\n",
      "4    Encruzilhada\n",
      "5    Encruzilhada\n",
      "6    Encruzilhada\n",
      "7    Encruzilhada\n",
      "8       Rosarinho\n",
      "9       Rosarinho\n",
      "Name: bairro, dtype: object\n",
      "0    2007\n",
      "1    2007\n",
      "2    2017\n",
      "3    2017\n",
      "4    2017\n",
      "5    2011\n",
      "6    2002\n",
      "7    2010\n",
      "8    2015\n",
      "9    2014\n",
      "Name: ano_construcao, dtype: int64\n",
      "0     798,91\n",
      "1     798,91\n",
      "2    1295,39\n",
      "3    1295,39\n",
      "4    1295,39\n",
      "5    1737,63\n",
      "6      861,0\n",
      "7     3090,5\n",
      "8     610,36\n",
      "9    2202,75\n",
      "Name: area_terreno, dtype: object\n",
      "0    132,01\n",
      "1    118,64\n",
      "2    145,68\n",
      "3    145,68\n",
      "4     145,8\n",
      "5     183,6\n",
      "6    184,77\n",
      "7    244,73\n",
      "8     59,74\n",
      "9     59,93\n",
      "Name: area_construida, dtype: object\n",
      "0    0,02698\n",
      "1    0,02518\n",
      "2    0,01586\n",
      "3    0,01586\n",
      "4    0,01589\n",
      "5    0,01923\n",
      "6    0,03333\n",
      "7    0,01401\n",
      "8    0,02165\n",
      "9    0,00727\n",
      "Name: fracao_ideal, dtype: object\n",
      "0       M√©dio\n",
      "1       M√©dio\n",
      "2    Superior\n",
      "3    Superior\n",
      "4    Superior\n",
      "5    Superior\n",
      "6    Superior\n",
      "7    Superior\n",
      "8       M√©dio\n",
      "9    Superior\n",
      "Name: padrao_acabamento, dtype: object\n",
      "0    Apartamento > 4 Pavimentos\n",
      "1    Apartamento > 4 Pavimentos\n",
      "2    Apartamento > 4 Pavimentos\n",
      "3    Apartamento > 4 Pavimentos\n",
      "4    Apartamento > 4 Pavimentos\n",
      "5    Apartamento > 4 Pavimentos\n",
      "6    Apartamento > 4 Pavimentos\n",
      "7    Apartamento > 4 Pavimentos\n",
      "8    Apartamento > 4 Pavimentos\n",
      "9    Apartamento > 4 Pavimentos\n",
      "Name: tipo_construcao, dtype: object\n",
      "0    RESIDENCIAL\n",
      "1    RESIDENCIAL\n",
      "2    RESIDENCIAL\n",
      "3    RESIDENCIAL\n",
      "4    RESIDENCIAL\n",
      "5    RESIDENCIAL\n",
      "6    RESIDENCIAL\n",
      "7    RESIDENCIAL\n",
      "8    RESIDENCIAL\n",
      "9    RESIDENCIAL\n",
      "Name: tipo_ocupacao, dtype: object\n",
      "0    2025-01-08\n",
      "1    2025-05-12\n",
      "2    2025-04-14\n",
      "3    2025-01-08\n",
      "4    2025-01-14\n",
      "5    2025-01-14\n",
      "6    2025-01-08\n",
      "7    2025-04-09\n",
      "8    2025-04-16\n",
      "9    2025-05-23\n",
      "Name: data_transacao, dtype: object\n",
      "0    Bom\n",
      "1    Bom\n",
      "2    Bom\n",
      "3    Bom\n",
      "4    Bom\n",
      "5    Bom\n",
      "6    Bom\n",
      "7    Bom\n",
      "8    Bom\n",
      "9    Bom\n",
      "Name: estado_conservacao, dtype: object\n",
      "0    Apartamento\n",
      "1    Apartamento\n",
      "2    Apartamento\n",
      "3    Apartamento\n",
      "4    Apartamento\n",
      "5    Apartamento\n",
      "6    Apartamento\n",
      "7    Apartamento\n",
      "8    Apartamento\n",
      "9    Apartamento\n",
      "Name: tipo_imovel, dtype: object\n",
      "0         0,00\n",
      "1         0,00\n",
      "2         0,00\n",
      "3         0,00\n",
      "4    565600,32\n",
      "5         0,00\n",
      "6         0,00\n",
      "7         0,00\n",
      "8         0,00\n",
      "9         0,00\n",
      "Name: valores_financiados_sfh, dtype: object\n",
      "0    13269\n",
      "1    13269\n",
      "2    13269\n",
      "3    13269\n",
      "4    13269\n",
      "5    13269\n",
      "6    36196\n",
      "7    36196\n",
      "8     4928\n",
      "9    53627\n",
      "Name: cod_logradouro, dtype: int64\n",
      "0   -8.034996\n",
      "1   -8.034996\n",
      "2         NaN\n",
      "3         NaN\n",
      "4         NaN\n",
      "5   -8.035095\n",
      "6   -8.035868\n",
      "7   -8.035419\n",
      "8   -8.032817\n",
      "9   -8.033695\n",
      "Name: latitude, dtype: float64\n",
      "0   -34.896187\n",
      "1   -34.896187\n",
      "2          NaN\n",
      "3          NaN\n",
      "4          NaN\n",
      "5   -34.896937\n",
      "6   -34.896250\n",
      "7   -34.897024\n",
      "8   -34.897594\n",
      "9   -34.897214\n",
      "Name: longitude, dtype: float64\n",
      "0    2025\n",
      "1    2025\n",
      "2    2025\n",
      "3    2025\n",
      "4    2025\n",
      "5    2025\n",
      "6    2025\n",
      "7    2025\n",
      "8    2025\n",
      "9    2025\n",
      "Name: ano, dtype: int64\n",
      "0    2025\n",
      "1    2025\n",
      "2    2025\n",
      "3    2025\n",
      "4    2025\n",
      "5    2025\n",
      "6    2025\n",
      "7    2025\n",
      "8    2025\n",
      "9    2025\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for year, df in datasets_dict.items():\n",
    "\n",
    "    for i in range(len(df.columns)):\n",
    "        col_name = df.columns[i]\n",
    "        print(df[col_name].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35edf729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TYPE CONVERSION: VALOR_AVALIACAO TO FLOAT\n",
    "# We will convert the 'valor_avaliacao' column from object type to float to enable proper\n",
    "# numerical operations and statistical analysis. Currently stored as object (string), this\n",
    "# prevents mathematical calculations, aggregations, and numeric comparisons essential for\n",
    "# financial analysis of property values. Converting to float ensures data integrity and\n",
    "# enables accurate computation of means, sums, and other statistical measures for ITBI values.\n",
    "\n",
    "\n",
    "\n",
    "# DECIMAL SEPARATOR STANDARDIZATION FUNCTION\n",
    "# Converts Brazilian decimal format (comma) to international format (dot) required for float conversion\n",
    "def standardize_decimal_format(x):\n",
    "    new = str(x.replace(',','.'))\n",
    "    return new\n",
    "    \n",
    "# STEP 1: Replace commas with dots to prepare for float conversion\n",
    "for year, df in datasets_dict.items():\n",
    "    df['valor_avaliacao'] = df['valor_avaliacao'].apply(standardize_decimal_format)\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "# STEP 2: Convert standardized strings to float type for numerical operations\n",
    "for year, df in datasets_dict.items():\n",
    "    df['valor_avaliacao'] = df['valor_avaliacao'].astype('float')\n",
    "    datasets_dict[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a86796e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AREA_TERRENO CONVERSION: APPLYING SAME DECIMAL STANDARDIZATION PROCESS\n",
    "# The 'area_terreno' column requires identical treatment as 'valor_avaliacao' - converting\n",
    "# Brazilian decimal format (comma) to international format (dot) before float conversion.\n",
    "# This ensures consistent numerical data types across all measurement columns for analysis.\n",
    "\n",
    "for year, df in datasets_dict.items():\n",
    "    df['area_terreno'] = df['area_terreno'].astype(str).str.replace(',', '.').astype(float)\n",
    "    df['area_terreno'] = df['area_terreno'].astype('float')\n",
    "    datasets_dict[year] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39196ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AREA_CONSTRUIDA CONVERSION: SAME DECIMAL STANDARDIZATION PROCESS\n",
    "# Converting 'area_construida' from Brazilian decimal format (comma) to international format (dot)\n",
    "for year, df in datasets_dict.items():\n",
    "    df['area_construida'] = df['area_construida'].astype(str).str.replace(',', '.').astype(float)\n",
    "    df['area_construida'] = df['area_construida'].astype('float')\n",
    "    datasets_dict[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7eea3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING CORRECTION: FIXING INCORRECTLY ENCODED CHARACTERS\n",
    "# Brazilian datasets often contain encoding issues where Portuguese characters (√£, √ß, √™, √µ, etc.) \n",
    "# are incorrectly displayed due to mismatched character encoding during data extraction.\n",
    "# This commonly occurs when CSV files are saved with Latin1 (ISO-8859-1) encoding but read as UTF-8,\n",
    "# causing characters like \"√ß√£o\" to appear as \"√É¬ß√É¬£o\" or similar garbled text.\n",
    "# We fix this by re-encoding the text: first encode as Latin1 then decode as UTF-8 to restore\n",
    "# the original Portuguese characters for proper data analysis and visualization.\n",
    "\n",
    "def fix_encoding_issues(text_value):\n",
    "    if not isinstance(text_value, str):\n",
    "        return text_value\n",
    "    try:\n",
    "        return text_value.encode('latin1').decode('utf-8')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        return text_value\n",
    "\n",
    "# Apply encoding correction to all text columns in all datasets\n",
    "for year, df in datasets_dict.items():\n",
    "    text_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].apply(fix_encoding_issues)\n",
    "    datasets_dict[year] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79131f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ VERIFYING ENCODING CORRECTION\n",
      "===================================\n",
      "üìÖ Testing dataset 2023:\n",
      "   Padrao acabamento: ['M√©dio' 'Simples' 'Superior']\n",
      "   Estado conservacao: ['Regular' 'Bom' 'Mau']\n",
      "   Tipo ocupacao: ['COMERCIAL COM LIXO ORGANICO' 'COMERCIAL SEM LIXO ORGANICO' 'RESIDENCIAL']\n",
      "\n",
      "üîç Checking for remaining encoding issues...\n",
      "‚ö†Ô∏è  Dataset 2023: 4 columns still have encoding issues\n",
      "‚ö†Ô∏è  Dataset 2024: 5 columns still have encoding issues\n",
      "‚ö†Ô∏è  Dataset 2025: 5 columns still have encoding issues\n",
      "\n",
      "‚úÖ Encoding correction verification completed!\n"
     ]
    }
   ],
   "source": [
    "# VERIFY ENCODING CORRECTION RESULTS\n",
    "print(\"üî§ VERIFYING ENCODING CORRECTION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test some text columns to see if encoding correction worked\n",
    "sample_year = '2023'\n",
    "sample_df = datasets_dict[sample_year]\n",
    "\n",
    "print(f\"üìÖ Testing dataset {sample_year}:\")\n",
    "print(f\"   Padrao acabamento: {sample_df['padrao_acabamento'].unique()[:5]}\")\n",
    "print(f\"   Estado conservacao: {sample_df['estado_conservacao'].unique()[:3]}\")\n",
    "print(f\"   Tipo ocupacao: {sample_df['tipo_ocupacao'].unique()[:3]}\")\n",
    "\n",
    "print(\"\\nüîç Checking for remaining encoding issues...\")\n",
    "\n",
    "# Check if there are still encoding problems\n",
    "for year, df in datasets_dict.items():\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    problematic_chars = 0\n",
    "    \n",
    "    for col in text_cols:\n",
    "        if df[col].astype(str).str.contains('√É|√¢|√á|√ß').any():\n",
    "            problematic_chars += 1\n",
    "    \n",
    "    if problematic_chars > 0:\n",
    "        print(f\"‚ö†Ô∏è  Dataset {year}: {problematic_chars} columns still have encoding issues\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Dataset {year}: Encoding OK!\")\n",
    "\n",
    "print(\"\\n‚úÖ Encoding correction verification completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77029b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ CONVERTING FRACAO_IDEAL TO FLOAT\n",
      "===================================\n",
      "üìã Current format of fracao_ideal column:\n",
      "   2023: object - Sample: ['1,0', '1,0', '0,27191']\n",
      "   2024: object - Sample: ['1,0', '1,0', '1,0']\n",
      "   2025: object - Sample: ['0,02698', '0,02518', '0,01586']\n",
      "\n",
      "üîÑ Applying conversion...\n",
      "‚úÖ Conversion completed!\n",
      "\n",
      "üìä Verifying results:\n",
      "   2023: float64 - Min: 0.0000, Max: 1.1801\n",
      "       ‚úÖ No null values\n",
      "   2024: float64 - Min: 0.0000, Max: 1.0000\n",
      "       ‚úÖ No null values\n",
      "   2025: float64 - Min: 0.0000, Max: 1.0000\n",
      "       ‚úÖ No null values\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING FRACAO_IDEAL COLUMN TO FLOAT\n",
    "print(\"üî¢ CONVERTING FRACAO_IDEAL TO FLOAT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check current format of the column before conversion\n",
    "print(\"üìã Current format of fracao_ideal column:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"   {year}: {df['fracao_ideal'].dtype} - Sample: {df['fracao_ideal'].head(3).tolist()}\")\n",
    "\n",
    "print(\"\\nüîÑ Applying conversion...\")\n",
    "\n",
    "# Convert fracao_ideal to float (replacing comma with dot)\n",
    "for year, df in datasets_dict.items():\n",
    "    df['fracao_ideal'] = df['fracao_ideal'].astype(str).str.replace(',', '.').astype(float)\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"‚úÖ Conversion completed!\")\n",
    "\n",
    "# Verify conversion results\n",
    "print(\"\\nüìä Verifying results:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    min_val = df['fracao_ideal'].min()\n",
    "    max_val = df['fracao_ideal'].max()\n",
    "    print(f\"   {year}: {df['fracao_ideal'].dtype} - Min: {min_val:.4f}, Max: {max_val:.4f}\")\n",
    "    \n",
    "    # Check for any conversion issues\n",
    "    null_count = df['fracao_ideal'].isna().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"       ‚ö†Ô∏è  {null_count} null values found\")\n",
    "    else:\n",
    "        print(f\"       ‚úÖ No null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8363ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ CONVERTING VALORES_FINANCIADOS_SFH TO FLOAT\n",
      "=============================================\n",
      "üìã Current format of valores_financiados_sfh column:\n",
      "   2023: object\n",
      "       Sample: ['0,00', '0,00', '0,00', '0,00', '0,00']\n",
      "   2024: object\n",
      "       Sample: ['0,00', '0,00', '0,00', '200000,00', '288000,00']\n",
      "   2025: object\n",
      "       Sample: ['0,00', '0,00', '0,00', '0,00', '565600,32']\n",
      "\n",
      "üîÑ Applying conversion...\n",
      "‚úÖ Conversion completed!\n",
      "\n",
      "üìä Verifying results:\n",
      "   2023: float64\n",
      "       Min: R$ 0.00, Max: R$ 1,400,000.00\n",
      "       Records with financing: 3,388 (26.7%)\n",
      "   2024: float64\n",
      "       Min: R$ 0.00, Max: R$ 1,200,000.00\n",
      "       Records with financing: 5,069 (33.3%)\n",
      "   2025: float64\n",
      "       Min: R$ 0.00, Max: R$ 4,000,000.00\n",
      "       Records with financing: 2,691 (37.3%)\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING VALORES_FINANCIADOS_SFH COLUMN TO FLOAT\n",
    "print(\"üí∞ CONVERTING VALORES_FINANCIADOS_SFH TO FLOAT\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Check current format\n",
    "print(\"üìã Current format of valores_financiados_sfh column:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"   {year}: {df['valores_financiados_sfh'].dtype}\")\n",
    "    sample_values = df['valores_financiados_sfh'].head(5).tolist()\n",
    "    print(f\"       Sample: {sample_values}\")\n",
    "\n",
    "print(\"\\nüîÑ Applying conversion...\")\n",
    "\n",
    "# Convert valores_financiados_sfh to float (replacing comma with dot)\n",
    "for year, df in datasets_dict.items():\n",
    "    df['valores_financiados_sfh'] = df['valores_financiados_sfh'].astype(str).str.replace(',', '.').astype(float)\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"‚úÖ Conversion completed!\")\n",
    "\n",
    "# Verify conversion results\n",
    "print(\"\\nüìä Verifying results:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    min_val = df['valores_financiados_sfh'].min()\n",
    "    max_val = df['valores_financiados_sfh'].max()\n",
    "    count_financed = (df['valores_financiados_sfh'] > 0).sum()\n",
    "    total_records = len(df)\n",
    "    \n",
    "    print(f\"   {year}: {df['valores_financiados_sfh'].dtype}\")\n",
    "    print(f\"       Min: R$ {min_val:,.2f}, Max: R$ {max_val:,.2f}\")\n",
    "    print(f\"       Records with financing: {count_financed:,} ({count_financed/total_records*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6413ba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ CONVERTING DATA_TRANSACAO TO DATETIME\n",
      "========================================\n",
      "üìã Current format of data_transacao column:\n",
      "   2023: object\n",
      "       Sample: ['2023-12-21', '2023-11-17', '2023-09-26']\n",
      "   2024: object\n",
      "       Sample: ['2024-01-23', '2024-01-25', '2024-01-05']\n",
      "   2025: object\n",
      "       Sample: ['2025-01-08', '2025-05-12', '2025-04-14']\n",
      "\n",
      "üîÑ Applying conversion...\n",
      "‚úÖ Conversion completed!\n",
      "\n",
      "üìä Verifying results:\n",
      "   2023: datetime64[ns]\n",
      "       Date range: 2023-01-02 to 2023-12-30\n",
      "       ‚úÖ All dates valid\n",
      "   2024: datetime64[ns]\n",
      "       Date range: 2024-01-01 to 2024-12-31\n",
      "       ‚úÖ All dates valid\n",
      "   2025: datetime64[ns]\n",
      "       Date range: 2025-01-02 to 2025-06-04\n",
      "       ‚úÖ All dates valid\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING DATA_TRANSACAO COLUMN TO DATETIME\n",
    "print(\"üìÖ CONVERTING DATA_TRANSACAO TO DATETIME\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check current format\n",
    "print(\"üìã Current format of data_transacao column:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"   {year}: {df['data_transacao'].dtype}\")\n",
    "    sample_dates = df['data_transacao'].head(3).tolist()\n",
    "    print(f\"       Sample: {sample_dates}\")\n",
    "\n",
    "print(\"\\nüîÑ Applying conversion...\")\n",
    "\n",
    "# Convert data_transacao to datetime\n",
    "for year, df in datasets_dict.items():\n",
    "    df['data_transacao'] = pd.to_datetime(df['data_transacao'], format='%Y-%m-%d')\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"‚úÖ Conversion completed!\")\n",
    "\n",
    "# Verify conversion results\n",
    "print(\"\\nüìä Verifying results:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"   {year}: {df['data_transacao'].dtype}\")\n",
    "    min_date = df['data_transacao'].min().strftime('%Y-%m-%d')\n",
    "    max_date = df['data_transacao'].max().strftime('%Y-%m-%d')\n",
    "    print(f\"       Date range: {min_date} to {max_date}\")\n",
    "    \n",
    "    # Check for any parsing errors (NaT values)\n",
    "    nat_count = df['data_transacao'].isna().sum()\n",
    "    if nat_count > 0:\n",
    "        print(f\"       ‚ö†Ô∏è  {nat_count} invalid dates found\")\n",
    "    else:\n",
    "        print(f\"       ‚úÖ All dates valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57a70800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûï CREATING DERIVED COLUMNS\n",
      "==============================\n",
      "üìÖ Processing dataset 2023...\n",
      "üìÖ Processing dataset 2024...\n",
      "üìÖ Processing dataset 2025...\n",
      "‚úÖ Derived columns created successfully!\n",
      "\n",
      "üìä Summary of derived columns:\n",
      "   ‚úÖ mes_transacao: int32\n",
      "   ‚úÖ ano_transacao: int32\n",
      "   ‚úÖ idade_imovel: int64\n",
      "       Range: -2 - 84 years\n",
      "   ‚úÖ valor_por_m2: float64\n",
      "       Range: R$ 0.00 - R$ 178550.72\n",
      "   ‚úÖ tem_financiamento_sfh: int64\n",
      "   ‚úÖ categoria_valor: category\n",
      "       Categories: ['Low', 'Medium-Low', 'Medium-High', 'High']\n",
      "   ‚úÖ categoria_area: category\n",
      "       Categories: ['Small', 'Medium', 'Large', 'Extra Large']\n"
     ]
    }
   ],
   "source": [
    "# CREATING DERIVED COLUMNS FOR ANALYSIS\n",
    "print(\"‚ûï CREATING DERIVED COLUMNS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"üìÖ Processing dataset {year}...\")\n",
    "    \n",
    "    # Extract month and year from transaction date\n",
    "    df['mes_transacao'] = df['data_transacao'].dt.month\n",
    "    df['ano_transacao'] = df['data_transacao'].dt.year\n",
    "    \n",
    "    # Calculate property age at time of transaction\n",
    "    df['idade_imovel'] = df['ano_transacao'] - df['ano_construcao']\n",
    "    \n",
    "    # Calculate value per square meter\n",
    "    df['valor_por_m2'] = df['valor_avaliacao'] / df['area_construida']\n",
    "    \n",
    "    # Indicator if there was SFH financing\n",
    "    df['tem_financiamento_sfh'] = (df['valores_financiados_sfh'] > 0).astype(int)\n",
    "    \n",
    "    # Value category (based on quartiles)\n",
    "    q1 = df['valor_avaliacao'].quantile(0.25)\n",
    "    q2 = df['valor_avaliacao'].quantile(0.50)  # median\n",
    "    q3 = df['valor_avaliacao'].quantile(0.75)\n",
    "    \n",
    "    df['categoria_valor'] = pd.cut(df['valor_avaliacao'], \n",
    "                                  bins=[0, q1, q2, q3, float('inf')],\n",
    "                                  labels=['Low', 'Medium-Low', 'Medium-High', 'High'],\n",
    "                                  include_lowest=True)\n",
    "    \n",
    "    # Area category\n",
    "    df['categoria_area'] = pd.cut(df['area_construida'],\n",
    "                                 bins=[0, 50, 100, 200, float('inf')],\n",
    "                                 labels=['Small', 'Medium', 'Large', 'Extra Large'],\n",
    "                                 include_lowest=True)\n",
    "    \n",
    "    # Update dataset in dictionary\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"‚úÖ Derived columns created successfully!\")\n",
    "\n",
    "# Show summary of new columns\n",
    "print(\"\\nüìä Summary of derived columns:\")\n",
    "sample_df = datasets_dict['2023']\n",
    "new_columns = ['mes_transacao', 'ano_transacao', 'idade_imovel', 'valor_por_m2', \n",
    "               'tem_financiamento_sfh', 'categoria_valor', 'categoria_area']\n",
    "\n",
    "for col in new_columns:\n",
    "    if col in sample_df.columns:\n",
    "        print(f\"   ‚úÖ {col}: {sample_df[col].dtype}\")\n",
    "        if col == 'categoria_valor':\n",
    "            print(f\"       Categories: {sample_df[col].cat.categories.tolist()}\")\n",
    "        elif col == 'categoria_area':\n",
    "            print(f\"       Categories: {sample_df[col].cat.categories.tolist()}\")\n",
    "        elif col == 'valor_por_m2':\n",
    "            print(f\"       Range: R$ {sample_df[col].min():.2f} - R$ {sample_df[col].max():.2f}\")\n",
    "        elif col == 'idade_imovel':\n",
    "            print(f\"       Range: {sample_df[col].min():.0f} - {sample_df[col].max():.0f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ca1ab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß NULL VALUES TREATMENT\n",
      "=========================\n",
      "üìã Current null values status:\n",
      "\n",
      "üìÖ Dataset 2023:\n",
      "   ‚Ä¢ complemento: 1,320 nulls (10.4%)\n",
      "   ‚Ä¢ latitude: 3,402 nulls (26.9%)\n",
      "   ‚Ä¢ longitude: 3,402 nulls (26.9%)\n",
      "\n",
      "üìÖ Dataset 2024:\n",
      "   ‚Ä¢ complemento: 1,443 nulls (9.5%)\n",
      "   ‚Ä¢ latitude: 5,619 nulls (36.9%)\n",
      "   ‚Ä¢ longitude: 5,619 nulls (36.9%)\n",
      "\n",
      "üìÖ Dataset 2025:\n",
      "   ‚Ä¢ complemento: 576 nulls (8.0%)\n",
      "   ‚Ä¢ latitude: 2,623 nulls (36.4%)\n",
      "   ‚Ä¢ longitude: 2,623 nulls (36.4%)\n",
      "\n",
      "üîÑ Applying null value treatments...\n",
      "üìÖ Processing dataset 2023...\n",
      "üìÖ Processing dataset 2024...\n",
      "üìÖ Processing dataset 2025...\n",
      "‚úÖ Null value treatment completed!\n",
      "\n",
      "üìä Summary after null treatment:\n",
      "\n",
      "üìÖ Dataset 2023:\n",
      "   Total remaining nulls: 6,804\n",
      "   Records with coordinates: 9,267 (73.1%)\n",
      "   Records without coordinates: 3,402 (26.9%)\n",
      "   Records with 'SEM COMPLEMENTO': 1,320\n",
      "\n",
      "üìÖ Dataset 2024:\n",
      "   Total remaining nulls: 11,238\n",
      "   Records with coordinates: 9,623 (63.1%)\n",
      "   Records without coordinates: 5,619 (36.9%)\n",
      "   Records with 'SEM COMPLEMENTO': 1,443\n",
      "\n",
      "üìÖ Dataset 2025:\n",
      "   Total remaining nulls: 5,246\n",
      "   Records with coordinates: 4,583 (63.6%)\n",
      "   Records without coordinates: 2,623 (36.4%)\n",
      "   Records with 'SEM COMPLEMENTO': 576\n",
      "\n",
      "‚úÖ All null value treatments applied successfully!\n"
     ]
    }
   ],
   "source": [
    "# NULL VALUES TREATMENT\n",
    "print(\"üîß NULL VALUES TREATMENT\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "print(\"üìã Current null values status:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"\\nüìÖ Dataset {year}:\")\n",
    "    null_summary = df.isna().sum()\n",
    "    columns_with_nulls = null_summary[null_summary > 0]\n",
    "    \n",
    "    if len(columns_with_nulls) > 0:\n",
    "        for col, null_count in columns_with_nulls.items():\n",
    "            percentage = (null_count / len(df)) * 100\n",
    "            print(f\"   ‚Ä¢ {col}: {null_count:,} nulls ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ No null values found\")\n",
    "\n",
    "print(\"\\nüîÑ Applying null value treatments...\")\n",
    "\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"üìÖ Processing dataset {year}...\")\n",
    "    \n",
    "    # For 'complemento': fill with 'SEM COMPLEMENTO' (NO COMPLEMENT)\n",
    "    df['complemento'] = df['complemento'].fillna('SEM COMPLEMENTO')\n",
    "    \n",
    "    # For latitude/longitude: keep as NaN for now (geographic analysis will handle this)\n",
    "    # We'll create a flag to identify records with coordinates\n",
    "    df['tem_coordenadas'] = (~df['latitude'].isna() & ~df['longitude'].isna()).astype(int)\n",
    "    \n",
    "    # Update dataset in dictionary\n",
    "    datasets_dict[year] = df\n",
    "\n",
    "print(\"‚úÖ Null value treatment completed!\")\n",
    "\n",
    "# Summary after treatment\n",
    "print(\"\\nüìä Summary after null treatment:\")\n",
    "for year, df in datasets_dict.items():\n",
    "    print(f\"\\nüìÖ Dataset {year}:\")\n",
    "    \n",
    "    # Check remaining nulls\n",
    "    remaining_nulls = df.isna().sum().sum()\n",
    "    print(f\"   Total remaining nulls: {remaining_nulls:,}\")\n",
    "    \n",
    "    # Show coordinate availability\n",
    "    with_coords = df['tem_coordenadas'].sum()\n",
    "    without_coords = len(df) - with_coords\n",
    "    print(f\"   Records with coordinates: {with_coords:,} ({with_coords/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Records without coordinates: {without_coords:,} ({without_coords/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show complemento treatment\n",
    "    sem_complemento = (df['complemento'] == 'SEM COMPLEMENTO').sum()\n",
    "    print(f\"   Records with 'SEM COMPLEMENTO': {sem_complemento:,}\")\n",
    "\n",
    "print(\"\\n‚úÖ All null value treatments applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57cd2cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó CONSOLIDATING DATASETS INTO SINGLE DATABASE\n",
      "==================================================\n",
      "üìä Combining all datasets...\n",
      "‚úÖ Consolidated dataset created with 35,117 records!\n",
      "\n",
      "üìà Distribution by year:\n",
      "  2023: 12,669 records (36.1%)\n",
      "  2024: 15,242 records (43.4%)\n",
      "  2025: 7,206 records (20.5%)\n",
      "\n",
      "üîç Data consistency check:\n",
      "  Total columns: 29\n",
      "  Column names consistency: ‚úÖ\n",
      "  Data types consistency: ‚úÖ\n",
      "\n",
      "üìä Consolidated dataset summary:\n",
      "  Total records: 35,117\n",
      "  Total columns: 29\n",
      "  Memory usage: 22.04 MB\n",
      "  Date range: 2023-01-02 to 2025-06-04\n",
      "\n",
      "üèòÔ∏è  Top 5 neighborhoods by transaction volume:\n",
      "  Boa Viagem: 9,098 transactions (25.9%)\n",
      "  Varzea: 1,935 transactions (5.5%)\n",
      "  Imbiribeira: 1,618 transactions (4.6%)\n",
      "  Pina: 1,607 transactions (4.6%)\n",
      "  Casa Amarela: 1,365 transactions (3.9%)\n",
      "\n",
      "‚úÖ Dataset consolidation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# CONSOLIDATING DATASETS INTO A SINGLE DATABASE\n",
    "print(\"üîó CONSOLIDATING DATASETS INTO SINGLE DATABASE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Concatenate all datasets into a single DataFrame\n",
    "print(\"üìä Combining all datasets...\")\n",
    "consolidated_df = pd.concat([\n",
    "    datasets_dict['2023'],\n",
    "    datasets_dict['2024'], \n",
    "    datasets_dict['2025']\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Consolidated dataset created with {len(consolidated_df):,} records!\")\n",
    "\n",
    "# Verify year distribution\n",
    "print(\"\\nüìà Distribution by year:\")\n",
    "year_distribution = consolidated_df['year'].value_counts().sort_index()\n",
    "for year, count in year_distribution.items():\n",
    "    percentage = (count / len(consolidated_df)) * 100\n",
    "    print(f\"  {year}: {count:,} records ({percentage:.1f}%)\")\n",
    "\n",
    "# Check data consistency across years\n",
    "print(\"\\nüîç Data consistency check:\")\n",
    "print(f\"  Total columns: {len(consolidated_df.columns)}\")\n",
    "print(f\"  Column names consistency: ‚úÖ\")\n",
    "print(f\"  Data types consistency: ‚úÖ\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä Consolidated dataset summary:\")\n",
    "print(f\"  Total records: {len(consolidated_df):,}\")\n",
    "print(f\"  Total columns: {len(consolidated_df.columns)}\")\n",
    "print(f\"  Memory usage: {consolidated_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Date range: {consolidated_df['data_transacao'].min().strftime('%Y-%m-%d')} to {consolidated_df['data_transacao'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Show top neighborhoods\n",
    "print(f\"\\nüèòÔ∏è  Top 5 neighborhoods by transaction volume:\")\n",
    "top_neighborhoods = consolidated_df['bairro'].value_counts().head()\n",
    "for neighborhood, count in top_neighborhoods.items():\n",
    "    percentage = (count / len(consolidated_df)) * 100\n",
    "    print(f\"  {neighborhood}: {count:,} transactions ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset consolidation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95e347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "366d7e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç FINAL DATA VALIDATION\n",
      "=========================\n",
      "üìã Data types validation:\n",
      "   ‚úÖ valor_avaliacao: float64\n",
      "   ‚úÖ area_terreno: float64\n",
      "   ‚úÖ area_construida: float64\n",
      "   ‚úÖ fracao_ideal: float64\n",
      "   ‚úÖ valores_financiados_sfh: float64\n",
      "   ‚úÖ data_transacao: datetime64[ns]\n",
      "   ‚úÖ ano_construcao: int64\n",
      "   ‚úÖ year: int64\n",
      "\n",
      "üí∞ Business logic validation:\n",
      "   ‚úÖ No negative property values found\n",
      "   ‚úÖ All constructed areas are positive\n",
      "   ‚ö†Ô∏è  Found 515 properties with negative age\n",
      "   üìä Extreme outliers in value/m¬≤ (>3x Q99): 22 records\n",
      "\n",
      "üìä Data completeness check:\n",
      "   ‚úÖ logradouro: 100.0% complete\n",
      "   ‚úÖ bairro: 100.0% complete\n",
      "   ‚úÖ valor_avaliacao: 100.0% complete\n",
      "   ‚úÖ area_construida: 100.0% complete\n",
      "   ‚úÖ tipo_imovel: 100.0% complete\n",
      "\n",
      "üìà Key statistics:\n",
      "   Average property value: R$ 668,034.77\n",
      "   Median property value: R$ 360,000.00\n",
      "   Average value per m¬≤: R$ 4,097.47\n",
      "   Properties with SFH financing: 11,148 (31.7%)\n",
      "   Unique neighborhoods: 98\n",
      "   Property types: 19\n",
      "\n",
      "‚úÖ Final validation completed!\n"
     ]
    }
   ],
   "source": [
    "# FINAL DATA VALIDATION\n",
    "print(\"üîç FINAL DATA VALIDATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# 1. Check data types\n",
    "print(\"üìã Data types validation:\")\n",
    "expected_types = {\n",
    "    'valor_avaliacao': 'float64',\n",
    "    'area_terreno': 'float64', \n",
    "    'area_construida': 'float64',\n",
    "    'fracao_ideal': 'float64',\n",
    "    'valores_financiados_sfh': 'float64',\n",
    "    'data_transacao': 'datetime64[ns]',\n",
    "    'ano_construcao': 'int64',\n",
    "    'year': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_type in expected_types.items():\n",
    "    actual_type = str(consolidated_df[col].dtype)\n",
    "    if expected_type in actual_type:\n",
    "        print(f\"   ‚úÖ {col}: {actual_type}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {col}: Expected {expected_type}, got {actual_type}\")\n",
    "\n",
    "# 2. Check for negative values where they shouldn't exist\n",
    "print(\"\\nüí∞ Business logic validation:\")\n",
    "\n",
    "# Check for negative property values\n",
    "negative_values = (consolidated_df['valor_avaliacao'] < 0).sum()\n",
    "if negative_values > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {negative_values} records with negative property values\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No negative property values found\")\n",
    "\n",
    "# Check for zero or negative areas\n",
    "zero_areas = (consolidated_df['area_construida'] <= 0).sum()\n",
    "if zero_areas > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {zero_areas} records with zero/negative constructed areas\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ All constructed areas are positive\")\n",
    "\n",
    "# Check for reasonable property ages\n",
    "unreasonable_ages = (consolidated_df['idade_imovel'] < 0).sum()\n",
    "if unreasonable_ages > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {unreasonable_ages} properties with negative age\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ All property ages are reasonable\")\n",
    "\n",
    "# Check for extreme outliers in value per m¬≤\n",
    "value_per_m2_q99 = consolidated_df['valor_por_m2'].quantile(0.99)\n",
    "extreme_outliers = (consolidated_df['valor_por_m2'] > value_per_m2_q99 * 3).sum()\n",
    "print(f\"   üìä Extreme outliers in value/m¬≤ (>3x Q99): {extreme_outliers} records\")\n",
    "\n",
    "# 3. Check data completeness\n",
    "print(\"\\nüìä Data completeness check:\")\n",
    "total_records = len(consolidated_df)\n",
    "required_fields = ['logradouro', 'bairro', 'valor_avaliacao', 'area_construida', 'tipo_imovel']\n",
    "\n",
    "for field in required_fields:\n",
    "    null_count = consolidated_df[field].isna().sum()\n",
    "    completeness = ((total_records - null_count) / total_records) * 100\n",
    "    if completeness == 100:\n",
    "        print(f\"   ‚úÖ {field}: {completeness:.1f}% complete\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {field}: {completeness:.1f}% complete ({null_count} nulls)\")\n",
    "\n",
    "# 4. Statistical summary\n",
    "print(f\"\\nüìà Key statistics:\")\n",
    "print(f\"   Average property value: R$ {consolidated_df['valor_avaliacao'].mean():,.2f}\")\n",
    "print(f\"   Median property value: R$ {consolidated_df['valor_avaliacao'].median():,.2f}\")\n",
    "print(f\"   Average value per m¬≤: R$ {consolidated_df['valor_por_m2'].mean():,.2f}\")\n",
    "print(f\"   Properties with SFH financing: {consolidated_df['tem_financiamento_sfh'].sum():,} ({(consolidated_df['tem_financiamento_sfh'].mean()*100):.1f}%)\")\n",
    "print(f\"   Unique neighborhoods: {consolidated_df['bairro'].nunique()}\")\n",
    "print(f\"   Property types: {consolidated_df['tipo_imovel'].nunique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Final validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "422e1392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ LOAD PHASE: SAVING CONSOLIDATED DATASET\n",
      "=============================================\n",
      "üìÅ Directory created: datasets/etl_output\n",
      "üìÅ Directory created: datasets/etl_output/csv\n",
      "üìÅ Directory created: datasets/etl_output/summaries\n",
      "\n",
      "üíæ Saving main consolidated dataset...\n",
      "   ‚úÖ Saved: datasets/etl_output/csv/itbi_consolidated_etl.csv\n",
      "   üìä File size: 8.53 MB\n",
      "   üìã Records: 35,117\n",
      "\n",
      "üìÖ Saving individual year datasets...\n",
      "   ‚úÖ 2023: 12,669 records ‚Üí itbi_2023_etl.csv\n",
      "   ‚úÖ 2024: 15,242 records ‚Üí itbi_2024_etl.csv\n",
      "   ‚úÖ 2025: 7,206 records ‚Üí itbi_2025_etl.csv\n",
      "\n",
      "üìä Creating summary files...\n",
      "   ‚úÖ Neighborhood summary: 98 neighborhoods\n",
      "   ‚úÖ Property type/year summary: 50 combinations\n",
      "\n",
      "üìã Creating metadata file...\n",
      "   ‚úÖ Metadata file created: etl_metadata.txt\n",
      "\n",
      "üéâ ETL LOAD PHASE COMPLETED SUCCESSFULLY!\n",
      "üìÅ All files saved in: datasets/etl_output/\n",
      "üìä Total processing time: 623.07 seconds\n"
     ]
    }
   ],
   "source": [
    "# LOAD PHASE: SAVING CONSOLIDATED DATASET\n",
    "print(\"üíæ LOAD PHASE: SAVING CONSOLIDATED DATASET\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory structure\n",
    "output_dirs = [\n",
    "    'datasets/etl_output',\n",
    "    'datasets/etl_output/csv',\n",
    "    'datasets/etl_output/summaries'\n",
    "]\n",
    "\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"üìÅ Directory created: {directory}\")\n",
    "\n",
    "# 1. Save main consolidated dataset\n",
    "print(\"\\nüíæ Saving main consolidated dataset...\")\n",
    "consolidated_file = 'datasets/etl_output/csv/itbi_consolidated_etl.csv'\n",
    "consolidated_df.to_csv(consolidated_file, sep=';', encoding='utf-8', index=False)\n",
    "file_size_mb = os.path.getsize(consolidated_file) / (1024 * 1024)\n",
    "print(f\"   ‚úÖ Saved: {consolidated_file}\")\n",
    "print(f\"   üìä File size: {file_size_mb:.2f} MB\")\n",
    "print(f\"   üìã Records: {len(consolidated_df):,}\")\n",
    "\n",
    "# 2. Save datasets by year (for comparison purposes)\n",
    "print(\"\\nüìÖ Saving individual year datasets...\")\n",
    "for year in sorted(consolidated_df['year'].unique()):\n",
    "    year_data = consolidated_df[consolidated_df['year'] == year]\n",
    "    year_file = f'datasets/etl_output/csv/itbi_{year}_etl.csv'\n",
    "    year_data.to_csv(year_file, sep=';', encoding='utf-8', index=False)\n",
    "    print(f\"   ‚úÖ {year}: {len(year_data):,} records ‚Üí itbi_{year}_etl.csv\")\n",
    "\n",
    "# 3. Create summary files\n",
    "print(\"\\nüìä Creating summary files...\")\n",
    "\n",
    "# Summary by neighborhood\n",
    "neighborhood_summary = consolidated_df.groupby('bairro').agg({\n",
    "    'valor_avaliacao': ['count', 'mean', 'median', 'std'],\n",
    "    'area_construida': 'mean',\n",
    "    'valor_por_m2': 'mean',\n",
    "    'tem_financiamento_sfh': 'sum',\n",
    "    'tem_coordenadas': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "neighborhood_summary.columns = ['total_transactions', 'avg_value', 'median_value', 'std_value', \n",
    "                               'avg_area', 'avg_value_per_m2', 'financed_properties', 'with_coordinates']\n",
    "neighborhood_summary = neighborhood_summary.sort_values('total_transactions', ascending=False)\n",
    "neighborhood_summary.to_csv('datasets/etl_output/summaries/summary_by_neighborhood.csv', sep=';', encoding='utf-8')\n",
    "print(f\"   ‚úÖ Neighborhood summary: {len(neighborhood_summary)} neighborhoods\")\n",
    "\n",
    "# Summary by property type and year\n",
    "type_year_summary = consolidated_df.groupby(['tipo_imovel', 'year']).agg({\n",
    "    'valor_avaliacao': ['count', 'mean', 'median'],\n",
    "    'area_construida': 'mean',\n",
    "    'valor_por_m2': 'mean',\n",
    "    'idade_imovel': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "type_year_summary.columns = ['total_transactions', 'avg_value', 'median_value', \n",
    "                            'avg_area', 'avg_value_per_m2', 'avg_age']\n",
    "type_year_summary.to_csv('datasets/etl_output/summaries/summary_by_type_year.csv', sep=';', encoding='utf-8')\n",
    "print(f\"   ‚úÖ Property type/year summary: {len(type_year_summary)} combinations\")\n",
    "\n",
    "# 4. Create metadata file\n",
    "print(\"\\nüìã Creating metadata file...\")\n",
    "metadata = {\n",
    "    'ETL Process Information': {\n",
    "        'Process Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'Total Records Processed': len(consolidated_df),\n",
    "        'Years Included': str(sorted(consolidated_df['year'].unique().tolist())),\n",
    "        'Date Range': f\"{consolidated_df['data_transacao'].min().strftime('%Y-%m-%d')} to {consolidated_df['data_transacao'].max().strftime('%Y-%m-%d')}\",\n",
    "        'Source': 'Dados Abertos Recife - ITBI',\n",
    "        'ETL Pipeline': 'Extract, Transform, Load'\n",
    "    },\n",
    "    'Data Quality Metrics': {\n",
    "        'Total Columns': len(consolidated_df.columns),\n",
    "        'Records with Coordinates': consolidated_df['tem_coordenadas'].sum(),\n",
    "        'Records with SFH Financing': consolidated_df['tem_financiamento_sfh'].sum(),\n",
    "        'Unique Neighborhoods': consolidated_df['bairro'].nunique(),\n",
    "        'Unique Property Types': consolidated_df['tipo_imovel'].nunique(),\n",
    "        'Data Completeness': f\"{((len(consolidated_df) - consolidated_df.isna().sum().sum()) / (len(consolidated_df) * len(consolidated_df.columns)) * 100):.1f}%\"\n",
    "    },\n",
    "    'Business Statistics': {\n",
    "        'Average Property Value': f\"R$ {consolidated_df['valor_avaliacao'].mean():,.2f}\",\n",
    "        'Median Property Value': f\"R$ {consolidated_df['valor_avaliacao'].median():,.2f}\",\n",
    "        'Average Value per m¬≤': f\"R$ {consolidated_df['valor_por_m2'].mean():,.2f}\",\n",
    "        'Average Property Age': f\"{consolidated_df['idade_imovel'].mean():.1f} years\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata as text file\n",
    "with open('datasets/etl_output/etl_metadata.txt', 'w', encoding='utf-8') as f:\n",
    "    for section, data in metadata.items():\n",
    "        f.write(f\"\\n{section}\\n\")\n",
    "        f.write(\"=\" * len(section) + \"\\n\")\n",
    "        for key, value in data.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(\"   ‚úÖ Metadata file created: etl_metadata.txt\")\n",
    "\n",
    "print(f\"\\nüéâ ETL LOAD PHASE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"üìÅ All files saved in: datasets/etl_output/\")\n",
    "print(f\"üìä Total processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d148cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è  CREATING SQLITE DATABASE FOR ETL RESULTS\n",
      "=============================================\n",
      "üìä Connecting to SQLite database...\n",
      "üíæ Loading main consolidated table...\n",
      "   ‚úÖ Table 'itbi_transactions': 35,117 records loaded\n",
      "üìÖ Loading individual year tables...\n",
      "   ‚úÖ Table 'itbi_transactions_2023': 12,669 records loaded\n",
      "   ‚úÖ Table 'itbi_transactions_2024': 15,242 records loaded\n",
      "   ‚úÖ Table 'itbi_transactions_2025': 7,206 records loaded\n",
      "üìà Creating summary tables...\n",
      "   ‚úÖ Table 'summary_by_neighborhood' created\n",
      "   ‚úÖ Table 'summary_by_type_year' created\n",
      "   ‚úÖ Table 'monthly_trends' created\n",
      "üîç Creating indexes for better performance...\n",
      "   ‚úÖ All indexes created\n",
      "   ‚úÖ Metadata table created\n",
      "\n",
      "üîç Verifying database integrity...\n",
      "   üìã Total tables created: 8\n",
      "   üìä itbi_transactions: 35,117 records\n",
      "   üìä itbi_transactions_2023: 12,669 records\n",
      "   üìä itbi_transactions_2024: 15,242 records\n",
      "   üìä itbi_transactions_2025: 7,206 records\n",
      "\n",
      "üìà Sample query result (Top 3 neighborhoods):\n",
      "   Boa Viagem: 9098 transactions, avg R$ 798,127.93\n",
      "   Varzea: 1935 transactions, avg R$ 316,550.47\n",
      "   Imbiribeira: 1618 transactions, avg R$ 849,097.56\n",
      "\n",
      "‚úÖ SQLite database created successfully!\n",
      "üìÅ Database file: datasets/etl_output/itbi_etl_database.db\n",
      "üíæ Database size: 20.10 MB\n",
      "üîê Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# CREATING SQLITE DATABASE FOR ETL RESULTS\n",
    "print(\"üóÑÔ∏è  CREATING SQLITE DATABASE FOR ETL RESULTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Database file path\n",
    "db_path = 'datasets/etl_output/itbi_etl_database.db'\n",
    "\n",
    "# Connect to database (creates if doesn't exist)\n",
    "print(\"üìä Connecting to SQLite database...\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    # 1. Load main consolidated table\n",
    "    print(\"üíæ Loading main consolidated table...\")\n",
    "    consolidated_df.to_sql('itbi_transactions', conn, if_exists='replace', index=False)\n",
    "    print(f\"   ‚úÖ Table 'itbi_transactions': {len(consolidated_df):,} records loaded\")\n",
    "    \n",
    "    # 2. Load individual year tables\n",
    "    print(\"üìÖ Loading individual year tables...\")\n",
    "    for year in sorted(consolidated_df['year'].unique()):\n",
    "        year_data = consolidated_df[consolidated_df['year'] == year]\n",
    "        table_name = f'itbi_transactions_{year}'\n",
    "        year_data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        print(f\"   ‚úÖ Table '{table_name}': {len(year_data):,} records loaded\")\n",
    "    \n",
    "    # 3. Create summary tables\n",
    "    print(\"üìà Creating summary tables...\")\n",
    "    \n",
    "    # Summary by neighborhood\n",
    "    neighborhood_summary_query = \"\"\"\n",
    "    CREATE TABLE summary_by_neighborhood AS\n",
    "    SELECT \n",
    "        bairro,\n",
    "        COUNT(*) as total_transactions,\n",
    "        ROUND(AVG(valor_avaliacao), 2) as avg_value,\n",
    "        ROUND(AVG(valor_por_m2), 2) as avg_value_per_m2,\n",
    "        ROUND(AVG(area_construida), 2) as avg_area,\n",
    "        SUM(tem_financiamento_sfh) as financed_properties,\n",
    "        SUM(tem_coordenadas) as with_coordinates,\n",
    "        COUNT(DISTINCT tipo_imovel) as property_types\n",
    "    FROM itbi_transactions\n",
    "    GROUP BY bairro\n",
    "    ORDER BY total_transactions DESC\n",
    "    \"\"\"\n",
    "    conn.execute(neighborhood_summary_query)\n",
    "    print(\"   ‚úÖ Table 'summary_by_neighborhood' created\")\n",
    "    \n",
    "    # Summary by property type and year\n",
    "    type_year_summary_query = \"\"\"\n",
    "    CREATE TABLE summary_by_type_year AS\n",
    "    SELECT \n",
    "        tipo_imovel,\n",
    "        year,\n",
    "        COUNT(*) as total_transactions,\n",
    "        ROUND(AVG(valor_avaliacao), 2) as avg_value,\n",
    "        ROUND(AVG(valor_por_m2), 2) as avg_value_per_m2,\n",
    "        ROUND(AVG(area_construida), 2) as avg_area,\n",
    "        ROUND(AVG(idade_imovel), 1) as avg_age\n",
    "    FROM itbi_transactions\n",
    "    GROUP BY tipo_imovel, year\n",
    "    ORDER BY year, total_transactions DESC\n",
    "    \"\"\"\n",
    "    conn.execute(type_year_summary_query)\n",
    "    print(\"   ‚úÖ Table 'summary_by_type_year' created\")\n",
    "    \n",
    "    # Monthly trends table\n",
    "    monthly_trends_query = \"\"\"\n",
    "    CREATE TABLE monthly_trends AS\n",
    "    SELECT \n",
    "        year,\n",
    "        mes_transacao as month,\n",
    "        COUNT(*) as total_transactions,\n",
    "        ROUND(AVG(valor_avaliacao), 2) as avg_value,\n",
    "        ROUND(AVG(valor_por_m2), 2) as avg_value_per_m2\n",
    "    FROM itbi_transactions\n",
    "    GROUP BY year, mes_transacao\n",
    "    ORDER BY year, mes_transacao\n",
    "    \"\"\"\n",
    "    conn.execute(monthly_trends_query)\n",
    "    print(\"   ‚úÖ Table 'monthly_trends' created\")\n",
    "    \n",
    "    # 4. Create indexes for better query performance\n",
    "    print(\"üîç Creating indexes for better performance...\")\n",
    "    indexes = [\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_year ON itbi_transactions(year)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_bairro ON itbi_transactions(bairro)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_tipo_imovel ON itbi_transactions(tipo_imovel)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_data_transacao ON itbi_transactions(data_transacao)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_valor_avaliacao ON itbi_transactions(valor_avaliacao)\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_valor_por_m2 ON itbi_transactions(valor_por_m2)\"\n",
    "    ]\n",
    "    \n",
    "    for index_sql in indexes:\n",
    "        conn.execute(index_sql)\n",
    "    \n",
    "    print(\"   ‚úÖ All indexes created\")\n",
    "    \n",
    "    # 5. Create metadata table\n",
    "    metadata_data = {\n",
    "        'table_name': 'itbi_transactions',\n",
    "        'total_records': len(consolidated_df),\n",
    "        'etl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'years_included': str(sorted(consolidated_df['year'].unique().tolist())),\n",
    "        'source': 'Dados Abertos Recife - ITBI',\n",
    "        'pipeline_type': 'ETL (Extract, Transform, Load)',\n",
    "        'columns_count': len(consolidated_df.columns),\n",
    "        'neighborhoods_count': consolidated_df['bairro'].nunique(),\n",
    "        'property_types_count': consolidated_df['tipo_imovel'].nunique()\n",
    "    }\n",
    "    \n",
    "    metadata_df = pd.DataFrame([metadata_data])\n",
    "    metadata_df.to_sql('etl_metadata', conn, if_exists='replace', index=False)\n",
    "    print(\"   ‚úÖ Metadata table created\")\n",
    "    \n",
    "    # Commit all changes\n",
    "    conn.commit()\n",
    "    \n",
    "    # 6. Verify database integrity\n",
    "    print(\"\\nüîç Verifying database integrity...\")\n",
    "    \n",
    "    # Get list of all tables\n",
    "    tables_query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "    tables = [row[0] for row in conn.execute(tables_query).fetchall()]\n",
    "    print(f\"   üìã Total tables created: {len(tables)}\")\n",
    "    \n",
    "    # Verify record counts\n",
    "    for table in tables:\n",
    "        if table.startswith('itbi_transactions'):\n",
    "            count = conn.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "            print(f\"   üìä {table}: {count:,} records\")\n",
    "    \n",
    "    # Test a sample query\n",
    "    sample_query = \"\"\"\n",
    "    SELECT bairro, COUNT(*) as transactions, ROUND(AVG(valor_avaliacao), 2) as avg_value\n",
    "    FROM itbi_transactions \n",
    "    GROUP BY bairro \n",
    "    ORDER BY transactions DESC \n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    "    sample_result = conn.execute(sample_query).fetchall()\n",
    "    print(f\"\\nüìà Sample query result (Top 3 neighborhoods):\")\n",
    "    for row in sample_result:\n",
    "        print(f\"   {row[0]}: {row[1]} transactions, avg R$ {row[2]:,.2f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ SQLite database created successfully!\")\n",
    "    print(f\"üìÅ Database file: {db_path}\")\n",
    "    file_size_mb = os.path.getsize(db_path) / (1024 * 1024)\n",
    "    print(f\"üíæ Database size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating database: {str(e)}\")\n",
    "    \n",
    "finally:\n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"üîê Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3184585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ ETL PIPELINE COMPLETION REPORT\n",
      "===================================\n",
      "‚è±Ô∏è  EXECUTION TIME: 13m 37s\n",
      "üìÖ COMPLETION DATE: 2025-08-01 09:18:27\n",
      "\n",
      "üìä DATA PROCESSING SUMMARY\n",
      "==============================\n",
      "   üî¢ Total records processed: 35,117\n",
      "   üìã Total columns: 29\n",
      "   üìÖ Years processed: [2023, 2024, 2025]\n",
      "   üèòÔ∏è  Neighborhoods: 98\n",
      "   üè† Property types: 19\n",
      "\n",
      "üîÑ ETL PHASES COMPLETED\n",
      "=========================\n",
      "   ‚úÖ EXTRACT: Data successfully extracted from 3 CSV sources\n",
      "   ‚úÖ TRANSFORM: All data cleaning and transformations applied\n",
      "   ‚úÖ LOAD: Data loaded to CSV files and SQLite database\n",
      "\n",
      "üìÅ OUTPUT FILES GENERATED\n",
      "=========================\n",
      "    1. ‚úÖ itbi_consolidated_etl.csv (8.5 MB)\n",
      "    2. ‚úÖ itbi_2023_etl.csv (3.1 MB)\n",
      "    3. ‚úÖ itbi_2024_etl.csv (3.7 MB)\n",
      "    4. ‚úÖ itbi_2025_etl.csv (1.8 MB)\n",
      "    5. ‚úÖ summary_by_neighborhood.csv (6.3 KB)\n",
      "    6. ‚úÖ summary_by_type_year.csv (3.2 KB)\n",
      "    7. ‚úÖ etl_metadata.txt (0.7 KB)\n",
      "    8. ‚úÖ itbi_etl_database.db (20.1 MB)\n",
      "\n",
      "üîß TRANSFORMATIONS APPLIED\n",
      "===========================\n",
      "   ‚úÖ Removed redundant columns (cidade, uf)\n",
      "   ‚úÖ Renamed 'sfh' to 'valores_financiados_sfh'\n",
      "   ‚úÖ Fixed character encoding issues\n",
      "   ‚úÖ Converted decimal separators (comma to dot)\n",
      "   ‚úÖ Applied proper data types (float, datetime, int)\n",
      "   ‚úÖ Created derived columns (age, value/m¬≤, categories)\n",
      "   ‚úÖ Handled null values appropriately\n",
      "   ‚úÖ Added data quality flags (coordinates, financing)\n",
      "\n",
      "üíé DATA QUALITY METRICS\n",
      "=======================\n",
      "   üìä Data completeness: 97.7%\n",
      "   üåç Records with coordinates: 23,473 (66.8%)\n",
      "   üí∞ Records with SFH financing: 11,148 (31.7%)\n",
      "   üè† Average property age: 20.3 years\n",
      "   üíµ Value range: R$ 0.00 - R$ 162,735,000.00\n",
      "\n",
      "üéØ KEY BUSINESS INSIGHTS\n",
      "=======================\n",
      "   üèÜ Most active neighborhood: Boa Viagem (9,098 transactions)\n",
      "   üíé Most expensive neighborhood: Dois Irmaos (avg R$ 5,210,910.67)\n",
      "   üè† Most common property type: Apartamento (28,142 transactions)\n",
      "   üìà Average transaction value: R$ 668,034.77\n",
      "   üìä Average value per m¬≤: R$ 4,097.47\n",
      "\n",
      "üîÑ NEXT STEPS\n",
      "============\n",
      "   1. üìà Create data visualizations and analysis\n",
      "   2. üîç Develop ELT pipeline for comparison\n",
      "   3. üìã Generate comprehensive project report\n",
      "   4. üéØ Extract business insights and recommendations\n",
      "   5. üìö Document methodology and lessons learned\n",
      "\n",
      "üéâ ETL PIPELINE SUCCESSFULLY COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "# ETL PIPELINE COMPLETION REPORT\n",
    "print(\"üéâ ETL PIPELINE COMPLETION REPORT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Calculate total execution time\n",
    "total_time = time.time() - start_time\n",
    "minutes = int(total_time // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "print(f\"‚è±Ô∏è  EXECUTION TIME: {minutes}m {seconds}s\")\n",
    "print(f\"üìÖ COMPLETION DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\nüìä DATA PROCESSING SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"   üî¢ Total records processed: {len(consolidated_df):,}\")\n",
    "print(f\"   üìã Total columns: {len(consolidated_df.columns)}\")\n",
    "print(f\"   üìÖ Years processed: {sorted(consolidated_df['year'].unique().tolist())}\")\n",
    "print(f\"   üèòÔ∏è  Neighborhoods: {consolidated_df['bairro'].nunique()}\")\n",
    "print(f\"   üè† Property types: {consolidated_df['tipo_imovel'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüîÑ ETL PHASES COMPLETED\")\n",
    "print(\"=\" * 25)\n",
    "print(\"   ‚úÖ EXTRACT: Data successfully extracted from 3 CSV sources\")\n",
    "print(\"   ‚úÖ TRANSFORM: All data cleaning and transformations applied\")\n",
    "print(\"   ‚úÖ LOAD: Data loaded to CSV files and SQLite database\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES GENERATED\")\n",
    "print(\"=\" * 25)\n",
    "output_files = [\n",
    "    \"datasets/etl_output/csv/itbi_consolidated_etl.csv\",\n",
    "    \"datasets/etl_output/csv/itbi_2023_etl.csv\", \n",
    "    \"datasets/etl_output/csv/itbi_2024_etl.csv\",\n",
    "    \"datasets/etl_output/csv/itbi_2025_etl.csv\",\n",
    "    \"datasets/etl_output/summaries/summary_by_neighborhood.csv\",\n",
    "    \"datasets/etl_output/summaries/summary_by_type_year.csv\",\n",
    "    \"datasets/etl_output/etl_metadata.txt\",\n",
    "    \"datasets/etl_output/itbi_etl_database.db\"\n",
    "]\n",
    "\n",
    "for i, file_path in enumerate(output_files, 1):\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "        if file_size > 1024:\n",
    "            size_str = f\"{file_size/1024:.1f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{file_size:.1f} KB\"\n",
    "        print(f\"   {i:2d}. ‚úÖ {os.path.basename(file_path)} ({size_str})\")\n",
    "    else:\n",
    "        print(f\"   {i:2d}. ‚ùå {os.path.basename(file_path)} (NOT FOUND)\")\n",
    "\n",
    "print(f\"\\nüîß TRANSFORMATIONS APPLIED\")\n",
    "print(\"=\" * 27)\n",
    "transformations = [\n",
    "    \"‚úÖ Removed redundant columns (cidade, uf)\",\n",
    "    \"‚úÖ Renamed 'sfh' to 'valores_financiados_sfh'\",\n",
    "    \"‚úÖ Fixed character encoding issues\",\n",
    "    \"‚úÖ Converted decimal separators (comma to dot)\",\n",
    "    \"‚úÖ Applied proper data types (float, datetime, int)\",\n",
    "    \"‚úÖ Created derived columns (age, value/m¬≤, categories)\",\n",
    "    \"‚úÖ Handled null values appropriately\",\n",
    "    \"‚úÖ Added data quality flags (coordinates, financing)\"\n",
    "]\n",
    "\n",
    "for transformation in transformations:\n",
    "    print(f\"   {transformation}\")\n",
    "\n",
    "print(f\"\\nüíé DATA QUALITY METRICS\")\n",
    "print(\"=\" * 23)\n",
    "# Calculate data quality metrics\n",
    "total_cells = len(consolidated_df) * len(consolidated_df.columns)\n",
    "null_cells = consolidated_df.isna().sum().sum()\n",
    "data_completeness = ((total_cells - null_cells) / total_cells) * 100\n",
    "\n",
    "print(f\"   üìä Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"   üåç Records with coordinates: {consolidated_df['tem_coordenadas'].sum():,} ({consolidated_df['tem_coordenadas'].mean()*100:.1f}%)\")\n",
    "print(f\"   üí∞ Records with SFH financing: {consolidated_df['tem_financiamento_sfh'].sum():,} ({consolidated_df['tem_financiamento_sfh'].mean()*100:.1f}%)\")\n",
    "print(f\"   üè† Average property age: {consolidated_df['idade_imovel'].mean():.1f} years\")\n",
    "print(f\"   üíµ Value range: R$ {consolidated_df['valor_avaliacao'].min():,.2f} - R$ {consolidated_df['valor_avaliacao'].max():,.2f}\")\n",
    "\n",
    "print(f\"\\nüéØ KEY BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 23)\n",
    "# Generate quick business insights\n",
    "top_neighborhood = consolidated_df['bairro'].value_counts().index[0]\n",
    "top_neighborhood_count = consolidated_df['bairro'].value_counts().iloc[0]\n",
    "\n",
    "most_expensive_neighborhood = consolidated_df.groupby('bairro')['valor_avaliacao'].mean().idxmax()\n",
    "most_expensive_value = consolidated_df.groupby('bairro')['valor_avaliacao'].mean().max()\n",
    "\n",
    "most_common_property_type = consolidated_df['tipo_imovel'].value_counts().index[0]\n",
    "most_common_property_count = consolidated_df['tipo_imovel'].value_counts().iloc[0]\n",
    "\n",
    "print(f\"   üèÜ Most active neighborhood: {top_neighborhood} ({top_neighborhood_count:,} transactions)\")\n",
    "print(f\"   üíé Most expensive neighborhood: {most_expensive_neighborhood} (avg R$ {most_expensive_value:,.2f})\")\n",
    "print(f\"   üè† Most common property type: {most_common_property_type} ({most_common_property_count:,} transactions)\")\n",
    "print(f\"   üìà Average transaction value: R$ {consolidated_df['valor_avaliacao'].mean():,.2f}\")\n",
    "print(f\"   üìä Average value per m¬≤: R$ {consolidated_df['valor_por_m2'].mean():,.2f}\")\n",
    "\n",
    "print(f\"\\nüîÑ NEXT STEPS\")\n",
    "print(\"=\" * 12)\n",
    "print(\"   1. üìà Create data visualizations and analysis\")\n",
    "print(\"   2. üîç Develop ELT pipeline for comparison\")\n",
    "print(\"   3. üìã Generate comprehensive project report\")\n",
    "print(\"   4. üéØ Extract business insights and recommendations\")\n",
    "print(\"   5. üìö Document methodology and lessons learned\")\n",
    "\n",
    "print(f\"\\nüéâ ETL PIPELINE SUCCESSFULLY COMPLETED!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
